{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Evaluation script for pre-trained reinforcement learning models.\n",
    "\n",
    "This script provides functionality for loading trained RL models, generating trajectories,\n",
    "analyzing rewards, and creating visualizations for research papers/theses.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Third-party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Project-specific imports\n",
    "sys.path.append('../.')\n",
    "\n",
    "# Import support modules as namespaces (avoid wildcard imports)\n",
    "import BayesOpt_Support as bayesopt_support\n",
    "import REINFORCE_Support as reinforce_support\n",
    "import SMCMC_Support as smcmc_support\n",
    "import testing_utils as testing_utils\n",
    "\n",
    "# Environment and utility imports\n",
    "from environments import GeneralEnvironment\n",
    "from mlflow_logger import MLflowLogger\n",
    "from utility_functions import (\n",
    "    calculate_cosine_diversity, \n",
    "    get_policy_dist, \n",
    "    load_initial_state,\n",
    "    load_entire_model, \n",
    "    plot_distributions_per_feature,\n",
    "    setup_logger, \n",
    "    simulate_trajectories,\n",
    "    seed_all\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_and_visualize_results(\n",
    "    agent_name: str,\n",
    "    trained_agent,\n",
    "    training_rewards: list,\n",
    "    env,\n",
    "    feature_names: list,\n",
    "    training_parameters: dict,\n",
    "    output_path: str,\n",
    "    model_identifier: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the results of a trained RL agent.\n",
    "    \n",
    "    Args:\n",
    "        agent_name (str): Name of the algorithm (e.g., 'SAC', 'REINFORCE')\n",
    "        trained_agent: The trained agent object\n",
    "        training_rewards (list): List of rewards from training\n",
    "        env: Training environment\n",
    "        feature_names (list): Names of the features\n",
    "        training_parameters (dict): Training parameters\n",
    "        output_path (str): Path to save outputs\n",
    "        model_identifier (str): Unique identifier for the model\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(f\"{agent_name} Training Progress\")\n",
    "    plt.savefig(f\"{output_path}/{agent_name.lower()}_training_progress_{model_identifier}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Generate high-reward trajectories\n",
    "    high_reward_trajectories = generate_high_reward_trajectories(\n",
    "        env,\n",
    "        trained_agent=trained_agent,\n",
    "        num_trajectories=200,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "    )\n",
    "\n",
    "    # Generate top 10 summary\n",
    "    top10_df, _, _ = create_top10_summary_table(\n",
    "        high_reward_trajectories, ['Timestamp'] + feature_names\n",
    "    )\n",
    "\n",
    "    # Save high reward trajectories\n",
    "    with open(f\"{output_path}/{agent_name.lower()}_high_reward_trajectories_{model_identifier}.pkl\", 'wb') as f:\n",
    "        pickle.dump(high_reward_trajectories, f)\n",
    "    # Save styled top10 table\n",
    "    top10_html_path = f\"{output_path}/{agent_name.lower()}_top10_cases_{model_identifier}.html\"\n",
    "    top10_df.style.background_gradient(cmap='Blues').to_html(top10_html_path)\n",
    "    display(top10_df.style.background_gradient(cmap='Blues'))\n",
    "\n",
    "    # Keep top 50 trajectories for diversity analysis\n",
    "    # MAke sure final reward is numeric\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    top10_df['FinalReward'] = top10_df['FinalReward'].apply(lambda x: x[0])\n",
    "    top50_df = top10_df.nlargest(50, 'FinalReward')\n",
    "    final_states = top50_df[feature_names].values\n",
    "    avg_div, avg_div_normalized, distances = calculate_euclidean_diversity(final_states, top50_df['FinalReward'].values)\n",
    "    print(f\"Average Euclidean Diversity: {avg_div}\")\n",
    "    print(f\"Normalized Euclidean Diversity: {avg_div_normalized}\")\n",
    "\n",
    "    # Plot rewards distribution\n",
    "    last_step_rewards = top10_df['FinalReward'].values\n",
    "    pd.Series(last_step_rewards, name='Final Rewards').to_csv(f\"{output_path}/{agent_name}_Final_Rewards.csv\", index=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data=last_step_rewards, bins=50, kde=True, stat='density')\n",
    "    \n",
    "    plt.axvline(np.mean(last_step_rewards), color='red', linestyle='dashed', linewidth=2, \n",
    "                label=f'Mean: {np.mean(last_step_rewards):.2f}')\n",
    "    plt.axvline(np.median(last_step_rewards), color='green', linestyle='dashed', linewidth=2, \n",
    "                label=f'Median: {np.median(last_step_rewards):.2f}')\n",
    "\n",
    "    stats_text = (f'Statistics:\\n'\n",
    "                 f'Mean: {np.mean(last_step_rewards):.2f}\\n'\n",
    "                 f'Median: {np.median(last_step_rewards):.2f}\\n'\n",
    "                 f'Std: {np.std(last_step_rewards):.2f}\\n'\n",
    "                 f'Max: {np.max(last_step_rewards):.2f}\\n'\n",
    "                 f'Min: {np.min(last_step_rewards):.2f}')\n",
    "\n",
    "    plt.text(0.9, 0.75, stats_text, transform=plt.gca().transAxes,\n",
    "             verticalalignment='center', horizontalalignment='left',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.xlabel('Final Reward Values', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.title(f'Distribution of Final Rewards ({agent_name})', fontsize=14, pad=20)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path}/{agent_name.lower()}_rewards_distribution_{model_identifier}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot diversity heatmap\n",
    "    euclidean_distances = squareform(distances)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(euclidean_distances, \n",
    "                cmap='viridis',\n",
    "                fmt=\".2f\",\n",
    "                vmin=0,\n",
    "                vmax=500,\n",
    "                cbar_kws={'label': 'Normalized Euclidean Distance'})\n",
    "\n",
    "    plt.title(f\"Euclidean Distance Matrix for Top 10 Trajectories\\n\"\n",
    "              f\"Diversity Score: {avg_div:.4f}, Normalized: {avg_div_normalized:.4f}\")\n",
    "    plt.xlabel(\"Trajectory Index\")\n",
    "    plt.ylabel(\"Trajectory Index\")\n",
    "    plt.tight_layout()\n",
    "    # Save as high quality pdf \n",
    "    plt.savefig(f\"{output_path}/{agent_name.lower()}_diversity_heatmap_{model_identifier}.pdf\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Usage example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "from utility_functions import  seed_all\n",
    "import random\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "# MODEL_PATH = \"oracles/oracle_3.joblib\"\n",
    "MODEL_PATH = \"oracles/oracle_3_GradientBoostingRegressor.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O3_REINFORCE_GBR\"\n",
    "results_runs = {}\n",
    "for run in range(30):\n",
    "    print(f\"Run {run+1}/30\")\n",
    "    # Simulate trajectories using the forward and backward models\n",
    "    random_seed = random.randint(0, 10000)    \n",
    "    # with Temperature\n",
    "    trained_agent,reinforce_rewards= train_rl_agent(\n",
    "        env=env,\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        algorithm='reinforce',\n",
    "        training_episodes=500,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "        sac_params = {\n",
    "            \"alpha\": 0.5,  # Temperature parameter for SAC\n",
    "            \"tau\": 0.0005,  # Soft update parameter for SAC\n",
    "            \"gamma\": 0.999,  # Discount factor for SAC\n",
    "        },\n",
    "        seed =  random_seed #33#34\n",
    "    )\n",
    "    # Store the trained agent and rewards\n",
    "    results_runs[run] = {\n",
    "        'trained_agent': trained_agent,\n",
    "        'reinforce_rewards': reinforce_rewards,\n",
    "        'seed': random_seed  # Store the random seed for reproducibility\n",
    "    }\n",
    "\n",
    "# Save the results runs as joblib file\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "from joblib import dump\n",
    "with open(f\"{output_path}/reinforce_results_runs_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\", 'wb') as f:\n",
    "    dump(results_runs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "from utility_functions import  seed_all\n",
    "import random\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "MODEL_PATH = \"oracles/oracle_3_GradientBoostingRegressor.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O3_REINFORCE_B_GBR\"\n",
    "results_runs = {}\n",
    "for run in range(30):\n",
    "    print(f\"Run {run+1}/30\")\n",
    "    # Simulate trajectories using the forward and backward models\n",
    "    random_seed = random.randint(0, 10000)    \n",
    "    # with Temperature\n",
    "    trained_agent,reinforce_rewards= train_rl_agent(\n",
    "        env=env,\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        algorithm='reinforce_baseline',\n",
    "        training_episodes=500,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "        sac_params = {\n",
    "            \"alpha\": 0.5,  # Temperature parameter for SAC\n",
    "            \"tau\": 0.0005,  # Soft update parameter for SAC\n",
    "            \"gamma\": 0.999,  # Discount factor for SAC\n",
    "        },\n",
    "        seed =  random_seed #33#34\n",
    "    )\n",
    "    # Store the trained agent and rewards\n",
    "    results_runs[run] = {\n",
    "        'trained_agent': trained_agent,\n",
    "        'reinforce_rewards': reinforce_rewards,\n",
    "        'seed': random_seed  # Store the random seed for reproducibility\n",
    "    }\n",
    "\n",
    "# Save the results runs as joblib file\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "from joblib import dump\n",
    "with open(f\"{output_path}/reinforce_results_runs_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\", 'wb') as f:\n",
    "    dump(results_runs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "from utility_functions import  seed_all\n",
    "import random\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "MODEL_PATH = \"oracles/oracle_3_MLPRegressor.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O3_SAC_GBR\"\n",
    "results_runs = {}\n",
    "for run in range(30):\n",
    "    print(f\"Run {run+1}/30\")\n",
    "    # Simulate trajectories using the forward and backward models\n",
    "    random_seed = random.randint(0, 10000)    \n",
    "    # with Temperature\n",
    "    trained_agent,reinforce_rewards= train_rl_agent(\n",
    "        env=env,\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        algorithm='sac',\n",
    "        training_episodes=500,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "        sac_params = {\n",
    "            \"alpha\": 0.5,  # Temperature parameter for SAC\n",
    "            \"tau\": 0.0005,  # Soft update parameter for SAC\n",
    "            \"gamma\": 0.999,  # Discount factor for SAC\n",
    "        },\n",
    "        seed =  random_seed #33#34\n",
    "    )\n",
    "    # Store the trained agent and rewards\n",
    "    results_runs[run] = {\n",
    "        'trained_agent': trained_agent,\n",
    "        'reinforce_rewards': reinforce_rewards,\n",
    "        'seed': random_seed  # Store the random seed for reproducibility\n",
    "    }\n",
    "\n",
    "# Save the results runs as joblib file\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "from joblib import dump\n",
    "with open(f\"{output_path}/sac_results_runs_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\", 'wb') as f:\n",
    "    dump(results_runs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "MODEL_PATH = \"oracles/oracle_3_MLPRegressor.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O3_SMCMC_GBR\"\n",
    "results_runs = {}\n",
    "for run in range(30):\n",
    "    print(f\"Run {run+1}/30\")\n",
    "    # Simulate trajectories using the forward and backward models\n",
    "    random_seed = random.randint(0, 10000)\n",
    "    seed_all(random_seed)  # Set random seed for reproducibility\n",
    "\n",
    "    # Generate top MCMC results\n",
    "    trajctories, rewards, actions, feature_names,non_sorted_rewards = generate_top_mcmc_results(\n",
    "        env=env,\n",
    "        num_trajectories=2000,  # Number of trajectories\n",
    "        num_steps=training_parameters[\"trajectory_length\"],  # Trajectory length\n",
    "        proposal_std=10,  # Standard deviation for MCMC proposal\n",
    "        feature_names=['Timestamp']+feature_names,  # Feature names from the environment\n",
    "        temperature=10,  # Temperature for MCMC,\n",
    "        # seed=random_seed  # Random seed for reproducibility\n",
    "    )\n",
    "    rewards = rewards.squeeze()\n",
    "    trajctories = trajctories.squeeze()\n",
    "    actions = actions.squeeze()\n",
    "    last_states = np.array([traj[-1] for traj in trajctories])\n",
    "    model_identifier = MODEL_PATH.split('.')[0][-8:]\n",
    "    # Save MCMC results\n",
    "    # Keep top 200\n",
    "    trajctories = trajctories[:200]\n",
    "    rewards = rewards[:200]\n",
    "    actions = actions[:200]\n",
    "    last_states = last_states[:200]\n",
    "\n",
    "    results_dict = {\n",
    "        'trajectories': trajctories,\n",
    "        'rewards': rewards,\n",
    "        'actions': actions,\n",
    "        'last_states': last_states,\n",
    "        'feature_names': feature_names,\n",
    "        'non_sorted_rewards': non_sorted_rewards,\n",
    "    }\n",
    "\n",
    "    results_runs[run] = {\n",
    "        'results_dict': results_dict,\n",
    "        'seed': random_seed  # Store the random seed for reproducibility\n",
    "    }\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Save as joblib file\n",
    "dump(results_runs, f\"{output_path}/mcmc_results_runs_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "\n",
    "    # Print mean reward for this run\n",
    "print(f\"Mean reward: {np.mean(rewards)}\")\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "# Save as joblib file\n",
    "from joblib import dump\n",
    "dump(results_dict, f\"{output_path}/mcmc_results_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_runs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = 0\n",
    "for key,value in results_runs.items():\n",
    "    print(f\"Run {key}: Seed: {value['seed']}, Final Reward: {value['reinforce_rewards'][-1]}\")\n",
    "    total_rewards += value['reinforce_rewards'][-1]\n",
    "print(f\"Mean Final Reward: {total_rewards/len(results_runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-reward trajectories\n",
    "high_reward_trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "        env,\n",
    "        trained_agent=trained_agent,\n",
    "        num_trajectories=200,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "    )\n",
    "model_identifier = MODEL_PATH.split('.')[0][-8:]\n",
    "\n",
    "# Extract last states from each trajectory\n",
    "last_states = np.array([traj[-1] for traj in high_reward_trajectories])\n",
    "\n",
    "# Create a dict with all data\n",
    "results_dict = {\n",
    "    'trajectories': high_reward_trajectories,\n",
    "    'rewards': rewards,\n",
    "    'actions': all_actions,\n",
    "    'last_states': last_states,\n",
    "    'feature_names': f_names\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Save as joblib file\n",
    "dump(results_dict, f\"{output_path}/reinforce_results_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "# analyze_and_visualize_results(\n",
    "#     agent_name='reinforce',\n",
    "#     trained_agent=trained_agent,\n",
    "#     training_rewards=reinforce_rewards,\n",
    "#     env=env,\n",
    "#     feature_names=feature_names,\n",
    "#     training_parameters=training_parameters,\n",
    "#     output_path=output_path,\n",
    "#     model_identifier=model_identifier\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "MODEL_PATH = \"oracles/oracle_3.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O1_REINFORCE_B\"\n",
    "\n",
    "# with Temperature\n",
    "trained_agent,reinforce_rewards= train_rl_agent(\n",
    "    env=env,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    algorithm='reinforce_baseline',\n",
    "    training_episodes=150,\n",
    "    max_steps=training_parameters[\"trajectory_length\"],\n",
    "    sac_params = {\n",
    "        \"alpha\": 0.5,  # Temperature parameter for SAC\n",
    "        \"tau\": 0.0005,  # Soft update parameter for SAC\n",
    "        \"gamma\": 0.999,  # Discount factor for SAC\n",
    "    },\n",
    "    seed =  34 #33#34\n",
    ")\n",
    " # Generate high-reward trajectories\n",
    "high_reward_trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "        env,\n",
    "        trained_agent=trained_agent,\n",
    "        num_trajectories=200,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "    )\n",
    "model_identifier = MODEL_PATH.split('.')[0][-8:]\n",
    "\n",
    "# Extract last states from each trajectory\n",
    "last_states = np.array([traj[-1] for traj in high_reward_trajectories])\n",
    "\n",
    "# Create a dict with all data\n",
    "results_dict = {\n",
    "    'trajectories': high_reward_trajectories,\n",
    "    'rewards': rewards,\n",
    "    'actions': all_actions,\n",
    "    'last_states': last_states,\n",
    "    'feature_names': f_names\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Save as joblib file\n",
    "dump(results_dict, f\"{output_path}/reinforce_baseline_results_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "# analyze_and_visualize_results(\n",
    "#     agent_name='reinforce',\n",
    "#     trained_agent=trained_agent,\n",
    "#     training_rewards=reinforce_rewards,\n",
    "#     env=env,\n",
    "#     feature_names=feature_names,\n",
    "#     training_parameters=training_parameters,\n",
    "#     output_path=output_path,\n",
    "#     model_identifier=model_identifier\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "MODEL_PATH = \"oracles/oracle_3.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O3_SAC\"\n",
    "\n",
    "# with Temperature\n",
    "trained_agent,reinforce_rewards= train_rl_agent(\n",
    "    env=env,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    algorithm='sac',\n",
    "    training_episodes=550,\n",
    "    max_steps=training_parameters[\"trajectory_length\"],\n",
    "    sac_params = {\n",
    "        \"alpha\": 0.5,  # Temperature parameter for SAC\n",
    "        \"tau\": 0.0005,  # Soft update parameter for SAC\n",
    "        \"gamma\": 0.999,  # Discount factor for SAC\n",
    "    },\n",
    "    seed =  34 #33#34\n",
    ")\n",
    " # Generate high-reward trajectories\n",
    "high_reward_trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "        env,\n",
    "        trained_agent=trained_agent,\n",
    "        num_trajectories=200,\n",
    "        max_steps=training_parameters[\"trajectory_length\"],\n",
    "    )\n",
    "model_identifier = MODEL_PATH.split('.')[0][-8:]\n",
    "\n",
    "# Extract last states from each trajectory\n",
    "last_states = np.array([traj[-1] for traj in high_reward_trajectories])\n",
    "\n",
    "# Create a dict with all data\n",
    "results_dict = {\n",
    "    'trajectories': high_reward_trajectories,\n",
    "    'rewards': rewards,\n",
    "    'actions': all_actions,\n",
    "    'last_states': last_states,\n",
    "    'feature_names': f_names\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Save as joblib file\n",
    "dump(results_dict, f\"{output_path}/sac_results_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "# analyze_and_visualize_results(\n",
    "#     agent_name='reinforce',\n",
    "#     trained_agent=trained_agent,\n",
    "#     training_rewards=reinforce_rewards,\n",
    "#     env=env,\n",
    "#     feature_names=feature_names,\n",
    "#     training_parameters=training_parameters,\n",
    "#     output_path=output_path,\n",
    "#     model_identifier=model_identifier\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "CONFIG_PATH = r'config/run_params.yaml'\n",
    "MODEL_PATH = \"oracles/oracle_2.joblib\"\n",
    "sys.path.append('../.')\n",
    "config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "# Correct state_dim to exclude the time-step part for action selection\n",
    "state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "output_path = r\"results/PaperPlots_O2_SMCMC\"\n",
    "results_runs = {}\n",
    "for run in range(30):\n",
    "    print(f\"Run {run+1}/30\")\n",
    "    # Simulate trajectories using the forward and backward models\n",
    "    random_seed = random.randint(0, 10000)\n",
    "    seed_all(random_seed)  # Set random seed for reproaducibility\n",
    "\n",
    "    # Generate top MCMC results\n",
    "    trajctories, rewards, actions, feature_names,non_sorted_rewards = generate_top_mcmc_results(\n",
    "        env=env,\n",
    "        num_trajectories=2000,  # Number of trajectories\n",
    "        num_steps=training_parameters[\"trajectory_length\"],  # Trajectory length\n",
    "        proposal_std=10,  # Standard deviation for MCMC proposal\n",
    "        feature_names=['Timestamp']+feature_names,  # Feature names from the environment\n",
    "        temperature=10,  # Temperature for MCMC,\n",
    "        # seed=random_seed  # Random seed for reproducibility\n",
    "    )\n",
    "    rewards = rewards.squeeze()\n",
    "    trajctories = trajctories.squeeze()\n",
    "    actions = actions.squeeze()\n",
    "    last_states = np.array([traj[-1] for traj in trajctories])\n",
    "    model_identifier = MODEL_PATH.split('.')[0][-8:]\n",
    "    # Save MCMC results\n",
    "    # Keep top 200\n",
    "    trajctories = trajctories[:200]\n",
    "    rewards = rewards[:200]\n",
    "    actions = actions[:200]\n",
    "    last_states = last_states[:200]\n",
    "\n",
    "    results_dict = {\n",
    "        'trajectories': trajctories,\n",
    "        'rewards': rewards,\n",
    "        'actions': actions,\n",
    "        'last_states': last_states,\n",
    "        'feature_names': feature_names,\n",
    "        'non_sorted_rewards': non_sorted_rewards,\n",
    "    }\n",
    "\n",
    "    results_runs[run] = {\n",
    "        'results_dict': results_dict,\n",
    "        'seed': random_seed  # Store the random seed for reproducibility\n",
    "    }\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Save as joblib file\n",
    "dump(results_runs, f\"{output_path}/mcmc_results_runs_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "\n",
    "    # Print mean reward for this run\n",
    "print(f\"Mean reward: {np.mean(rewards)}\")\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "# Save as joblib file\n",
    "from joblib import dump\n",
    "dump(results_dict, f\"{output_path}/mcmc_results_{MODEL_PATH.split('/')[-1].split('.')[0]}.joblib\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajctories.shape, rewards.shape, actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(last_step_rewards, name='Final Rewards').to_csv(f\"{output_path}/SMCMC_Final_Rewards.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(last_step_reinforce_rewards, name='Final Rewards').to_csv(f\"Reinforce_Final_Rewards.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def calculate_euclidean_diversity(final_states: np.ndarray, rewards):\n",
    "    distances = pdist(final_states, metric='euclidean')\n",
    "    avg_div = distances.mean()\n",
    "    avg_div_normalized = (avg_div / (max(rewards)-min(rewards))) * max(rewards)\n",
    "    distances = squareform(distances)  # Convert to square form for better visualization\n",
    "    # Convert to square form for better visualization\n",
    "    return avg_div, avg_div_normalized,distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avg_diversity, \n",
    "avg_div_normalized,\n",
    "distances)= calculate_euclidean_diversity(top10_mcmc_df[feature_names].values,top10_mcmc_df['FinalReward'])\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(distances, \n",
    "            cmap='viridis', \n",
    "            fmt=\".2f\",\n",
    "            vmin=0,  # Set minimum value to 0\n",
    "            vmax=500,  # Set maximum value to 500\n",
    "            cbar_kws={'label': 'Euclidean Distance'})\n",
    "\n",
    "plt.title(f\"Euclidean Distance Matrix for Top 10 Trajectories\\nDiversity Score: {avg_diversity:.4f}, Normalized: {avg_div_normalized[0]:.4f}\")\n",
    "plt.xlabel(\"Trajectory Index\")\n",
    "plt.ylabel(\"Trajectory Index\")\n",
    "plt.tight_layout()\n",
    "# Save as high quality pdf\n",
    "plt.savefig(f\"{output_path}/smcmc_diversity_heatmap_{MODEL_PATH.split('/')[-1].split('.')[0]}.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "from utility_functions import seed_all\n",
    "\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "import joblib\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from typing import Dict, Any, Callable\n",
    "from utility_functions import seed_all\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "    experiments: Dict[str, Dict[str, Any]],\n",
    "    base_output_dir: str,\n",
    "    simulate_fn: Callable = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Unified experiment runner that infers which algorithm to run based on run name.\n",
    "    Supports RL, SMCMC, and GFlowNet runs with per-run configurations.\n",
    "\n",
    "    Args:\n",
    "        experiments: Dictionary mapping run_name -> parameters. Expected keys:\n",
    "            - oracle_path (for RL/SMCMC)\n",
    "            - config_path (for RL/SMCMC)\n",
    "            - num_runs, training_episodes, etc.\n",
    "            - For SMCMC: n_trajectories, proposal_std, temperature.\n",
    "            - For GFlow: fwd_model_path, bwd_model_path, run_params_path, gflow_runs (list of dicts).\n",
    "        base_output_dir: Root folder where results will be stored.\n",
    "        simulate_fn: Function to simulate GFlow trajectories (required for gflow runs).\n",
    "    \"\"\"\n",
    "    sys.path.append('../.')\n",
    "\n",
    "    for run_name, params in experiments.items():\n",
    "        print(f\"\\n=== Running experiment: {run_name} ===\")\n",
    "        output_path = os.path.join(base_output_dir, run_name)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        run_name_lower = run_name.lower()\n",
    "        # --- Determine algorithm type ---\n",
    "        if \"gflow\" in run_name_lower:\n",
    "            algo_name = \"gflow\"\n",
    "        elif \"smcmc\" in run_name_lower:\n",
    "            algo_name = \"smcmc\"\n",
    "        elif \"reinforce_baseline\" in run_name_lower:\n",
    "            algo_name = \"reinforce_baseline\"\n",
    "        elif \"reinforce\" in run_name_lower:\n",
    "            algo_name = \"reinforce\"\n",
    "        elif \"sac\" in run_name_lower:\n",
    "            algo_name = \"sac\"\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot detect algorithm type from run name: {run_name}\")\n",
    "\n",
    "        # === RL/SMCMC Experiments ===\n",
    "        if algo_name in {\"reinforce\", \"reinforce_baseline\", \"sac\", \"smcmc\"}:\n",
    "            config_path = params.get(\"config_path\")\n",
    "            if not config_path:\n",
    "                raise ValueError(f\"No config_path provided for run: {run_name}\")\n",
    "\n",
    "            config, env, logger, training_parameters, initial_state, feature_names = (\n",
    "                setup_environment_and_logger(config_path, params[\"oracle_path\"])\n",
    "            )\n",
    "\n",
    "            state_dim = len(initial_state) + 1\n",
    "            action_dim = len(initial_state)\n",
    "            results_runs = {}\n",
    "\n",
    "            for run in range(params.get(\"num_runs\", 30)):\n",
    "                print(f\"Run {run+1}/{params.get('num_runs', 30)}\")\n",
    "                random_seed = random.randint(0, 10_000)\n",
    "                seed_all(random_seed)\n",
    "\n",
    "                if algo_name in {\"reinforce\", \"reinforce_baseline\", \"sac\"}:\n",
    "                    trained_agent, rewards = train_rl_agent(\n",
    "                        env=env,\n",
    "                        state_dim=state_dim,\n",
    "                        action_dim=action_dim,\n",
    "                        algorithm=algo_name,\n",
    "                        training_episodes=params.get(\"training_episodes\", 500),\n",
    "                        max_steps=training_parameters[\"trajectory_length\"],\n",
    "                        sac_params={\n",
    "                            \"alpha\": 0.5,\n",
    "                            \"tau\": 0.0005,\n",
    "                            \"gamma\": 0.999,\n",
    "                        },\n",
    "                        seed=random_seed,\n",
    "                    )\n",
    "                    results_runs[run] = {\n",
    "                        \"trained_agent\": trained_agent,\n",
    "                        \"rewards\": rewards,\n",
    "                        \"seed\": random_seed,\n",
    "                    }\n",
    "\n",
    "                elif algo_name == \"smcmc\":\n",
    "                    # === EXACT MATCH to your original saving style ===\n",
    "                    trajectories, rewards, actions, feature_names_out, non_sorted_rewards = (\n",
    "                        generate_top_mcmc_results(\n",
    "                            env=env,\n",
    "                            num_trajectories=params.get(\"n_trajectories\", 2000),\n",
    "                            num_steps=training_parameters[\"trajectory_length\"],\n",
    "                            proposal_std=params.get(\"proposal_std\", 10),\n",
    "                            feature_names=['Timestamp'] + feature_names,\n",
    "                            temperature=params.get(\"temperature\", 10),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    rewards = rewards.squeeze()\n",
    "                    trajectories = trajectories.squeeze()\n",
    "                    actions = actions.squeeze()\n",
    "                    last_states = np.array([traj[-1] for traj in trajectories])\n",
    "\n",
    "                    # Keep top 200 trajectories\n",
    "                    trajectories = trajectories[:200]\n",
    "                    rewards = rewards[:200]\n",
    "                    actions = actions[:200]\n",
    "                    last_states = last_states[:200]\n",
    "\n",
    "                    results_dict = {\n",
    "                        \"trajectories\": trajectories,\n",
    "                        \"rewards\": rewards,\n",
    "                        \"actions\": actions,\n",
    "                        \"last_states\": last_states,\n",
    "                        \"feature_names\": feature_names_out,\n",
    "                        \"non_sorted_rewards\": non_sorted_rewards,\n",
    "                    }\n",
    "\n",
    "                    results_runs[run] = {\n",
    "                        \"results_dict\": results_dict,\n",
    "                        \"seed\": random_seed,\n",
    "                    }\n",
    "\n",
    "            dump(results_runs, os.path.join(output_path, f\"{algo_name}_results.joblib\"))\n",
    "            print(f\"Saved {algo_name} results to {output_path}\")\n",
    "\n",
    "        # === GFlowNet Experiments ===\n",
    "        elif algo_name == \"gflow\":\n",
    "            if not simulate_fn:\n",
    "                raise RuntimeError(\"simulate_fn must be provided for GFlow runs\")\n",
    "\n",
    "            results_runs = {}\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            # Support per-run config path for GFlow\n",
    "            run_params_path = params.get(\"run_params_path\")\n",
    "            if not run_params_path:\n",
    "                raise ValueError(f\"No run_params_path provided for GFlow run: {run_name}\")\n",
    "            config = yaml.safe_load(open(run_params_path, \"r\"))\n",
    "\n",
    "            for i, run_cfg in enumerate(params.get(\"gflow_runs\", [])):\n",
    "                seed = run_cfg[\"seed\"]\n",
    "                seed_all(seed)\n",
    "\n",
    "                print(f\"Simulating GFlow run {i+1}/{len(params['gflow_runs'])} with seed {seed}...\")\n",
    "                forward_model = load_entire_model(params[\"fwd_model_path\"], device)\n",
    "                backward_model = load_entire_model(params[\"bwd_model_path\"], device)\n",
    "                logger, _ = setup_logger(f\"gflow_run_{i}\")\n",
    "                initial_state, feature_names = load_initial_state(config)\n",
    "\n",
    "                trajectories, rewards, all_actions, _ = simulate_fn(\n",
    "                    env_class=GeneralEnvironment,\n",
    "                    forward_model=forward_model,\n",
    "                    backward_model=backward_model,\n",
    "                    initial_state=initial_state,\n",
    "                    config=config,\n",
    "                    trajectory_length=run_cfg.get(\"trajectory_length\", 12),\n",
    "                    n_trajectories=run_cfg.get(\"n_trajectories\", 200),\n",
    "                    device=device,\n",
    "                    logger=logger,\n",
    "                    model_path=config[\"oracle\"][\"model_path\"],\n",
    "                    distribution=\"mixture_beta\",\n",
    "                    extra_parameters=params.get(\"extra_parameters\", {}),\n",
    "                )\n",
    "\n",
    "                results_runs[i] = {\n",
    "                    \"trajectories\": trajectories,\n",
    "                    \"rewards\": np.array(rewards),\n",
    "                    \"all_actions\": all_actions,\n",
    "                    \"seed\": seed,\n",
    "                }\n",
    "\n",
    "            joblib.dump(results_runs, os.path.join(output_path, \"gflow_results_runs.joblib\"))\n",
    "            print(f\"GFlow results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = {\n",
    "    \"reinforce_1_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_1_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_1.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"reinforce_2_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_2_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_2.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"reinforce_3_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_3_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_3.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"reinforce_baseline_1_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_1_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_1.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"reinforce_baseline_2_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_2_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_2.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"reinforce_baseline_3_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_3_MLPRegressor.joblib\",\n",
    "        \"config_path\":\"config/_run_params_3.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"sac_1_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_1_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_1.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"sac_2_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_2_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_2.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"sac_3_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_3_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_3.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"training_episodes\": 500,\n",
    "    },\n",
    "    \"smcmc_1_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_1_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_1.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"n_trajectories\": 2000,\n",
    "        \"proposal_std\": 10,\n",
    "    },\n",
    "    \"smcmc_2_mlp\": {\n",
    "        \"oracle_path\": \"oracles/oracle_2_MLPRegressor.joblib\",\n",
    "        \"config_path\": \"config/_run_params_2.yaml\",\n",
    "        \"num_runs\": 30,\n",
    "        \"n_trajectories\": 2000,\n",
    "        \"proposal_std\": 10,\n",
    "    },\n",
    "    \"smcmc_3_mlp\": {\n",
    "            \"oracle_path\": \"oracles/oracle_3_MLPRegressor.joblib\",\n",
    "            \"config_path\": \"config/_run_params_3.yaml\",\n",
    "            \"num_runs\": 30,\n",
    "            \"n_trajectories\": 2000,\n",
    "            \"proposal_std\": 10,\n",
    "        },\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# run_all_experiments(\n",
    "#     experiments=EXPERIMENTS,\n",
    "#     base_output_dir='results/All_Experiment_Results',\n",
    "#     simulate_fn=None,\n",
    "# )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS[\"gflow_1_mlp\"] = {\n",
    "    \"gflow_results_path\": \"mlruns/2/240c95ed1dfa41de9637a8d9a876e50d/artifacts/forward_model_iteration_500/results_run.joblib\",\n",
    "    \"run_params_path\": \"config/_run_params_1.yaml\",\n",
    "    \"oracle_path\": \"oracles/oracle_1_MLPRegressor.joblib\",\n",
    "    \"trajectory_length\": 12,\n",
    "    \"n_trajectories\": 200,\n",
    "    \"extra_parameters\": {\n",
    "        'mixture_components': 15,\n",
    "        'num_variables': 6,\n",
    "    }\n",
    "}\n",
    "EXPERIMENTS[\"gflow_2_mlp\"] = {\n",
    "    \"gflow_results_path\": \"mlruns/3/385afeb31ff1439b834d885958294961/artifacts/forward_model_iteration_500/results_run.joblib\",\n",
    "    \"run_params_path\": \"config/_run_params_2.yaml\",\n",
    "    \"oracle_path\": \"oracles/oracle_2_MLPRegressor.joblib\",\n",
    "    \"trajectory_length\": 12,\n",
    "    \"n_trajectories\": 200,\n",
    "    \"extra_parameters\": {\n",
    "        'mixture_components': 15,\n",
    "        'num_variables': 6,\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPERIMENTS['gflow_3_mlp'] = {\n",
    "    \"gflow_results_path\": \"mlruns/4/785cadee6b0141ad9de43ea9b1cf365c/artifacts/forward_model_iteration_500/results_run.joblib\",\n",
    "    \"run_params_path\": \"config/_run_params_3.yaml\",\n",
    "    \"oracle_path\": \"oracles/oracle_3_MLPRegressor.joblib\",\n",
    "    \"trajectory_length\": 12,\n",
    "    \"n_trajectories\": 200,\n",
    "    \"extra_parameters\": {\n",
    "        'mixture_components': 15,\n",
    "        'num_variables': 6,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "from typing import Dict, Any, Iterable, Tuple\n",
    "\n",
    "# Map short tag -> sklearn model suffix used in filenames\n",
    "MODEL_SUFFIX_MAP = {\n",
    "    \"mlp\": \"MLPRegressor\",\n",
    "    \"gbr\": \"GradientBoostingRegressor\",\n",
    "    \"rfr\": \"RandomForestRegressor\",\n",
    "    \"elastic\":\"ElasticNet\",\n",
    "    \"ens\":\"EnsembleRegressor\",\n",
    "}\n",
    "\n",
    "\n",
    "def _replace_oracle_suffix(path: str, new_suffix: str) -> str:\n",
    "    \"\"\"Replace the trailing model suffix in an oracle path.\n",
    "\n",
    "    It expects filenames like: .../oracle_3_<ModelSuffix>.joblib\n",
    "    Supported suffixes: MLPRegressor | GradientBoostingRegressor | RandomForestRegressor\n",
    "\n",
    "    Args:\n",
    "        path: Original oracle .joblib path.\n",
    "        new_suffix: New model suffix to inject, e.g., \"GradientBoostingRegressor\".\n",
    "\n",
    "    Returns:\n",
    "        Modified path with the new suffix.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the path does not match the expected pattern.\n",
    "    \"\"\"\n",
    "    new_path = re.sub(\n",
    "        r\"_(MLPRegressor|GradientBoostingRegressor|RandomForestRegressor)\\.joblib$\",\n",
    "        f\"_{new_suffix}.joblib\",\n",
    "        path,\n",
    "    )\n",
    "    if new_path == path:\n",
    "        raise ValueError(\n",
    "            f\"oracle_path does not end with a supported suffix: {path}\"\n",
    "        )\n",
    "    return new_path\n",
    "\n",
    "\n",
    "def augment_experiments_with_variants(\n",
    "    experiments: Dict[str, Dict[str, Any]],\n",
    "    variants: Iterable[str] = (\"gbr\", \"rfr\"),\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Create GBR/RFR copies from existing *_mlp runs by updating only oracle_path.\n",
    "\n",
    "    For each key that ends with `_mlp`, this will create parallel entries with\n",
    "    `_gbr` and `_rfr` (or whatever `variants` you pass). If an entry already\n",
    "    exists, it is left untouched.\n",
    "\n",
    "    Args:\n",
    "        experiments: Base experiments dict (will NOT be mutated).\n",
    "        variants: Iterable of short tags to create (e.g., (\"gbr\", \"rfr\")).\n",
    "\n",
    "    Returns:\n",
    "        A NEW dict containing the original experiments plus the added variants.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, Any]] = copy.deepcopy(experiments)\n",
    "\n",
    "    for run_name, cfg in experiments.items():\n",
    "        # Only clone runs that are *oracle-based* and keyed as *_mlp\n",
    "        if not run_name.endswith(\"_mlp\"):\n",
    "            continue\n",
    "        if \"oracle_path\" not in cfg:\n",
    "            continue  # skip non-oracle entries like gflow, etc.\n",
    "\n",
    "        base_oracle_path = cfg[\"oracle_path\"]\n",
    "\n",
    "        for v in variants:\n",
    "            if v not in MODEL_SUFFIX_MAP:\n",
    "                raise KeyError(f\"Unknown variant '{v}'. \"\n",
    "                               f\"Known: {list(MODEL_SUFFIX_MAP.keys())}\")\n",
    "\n",
    "            new_key = run_name[:-4] + f\"_{v}\"  # replace trailing _mlp with _gbr/_rfr\n",
    "            if new_key in out:\n",
    "                # already present; do not overwrite\n",
    "                continue\n",
    "\n",
    "            new_cfg = copy.deepcopy(cfg)\n",
    "            new_cfg[\"oracle_path\"] = _replace_oracle_suffix(\n",
    "                base_oracle_path, MODEL_SUFFIX_MAP[v]\n",
    "            )\n",
    "            out[new_key] = new_cfg\n",
    "\n",
    "    return out\n",
    "\n",
    "EXPERIMENTS = augment_experiments_with_variants(EXPERIMENTS, variants=(\"elastic\", \"ens\",\"gbr\", \"rfr\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out keys containing 'mlp' or 'gflow' also fil\n",
    "filtered_experiments = {\n",
    "    key: value for key, value in EXPERIMENTS.items() \n",
    "    if 'mlp' not in key.lower() and 'gflow' not in key.lower()\n",
    "}\n",
    "print(f\"Remaining experiments: {list(filtered_experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only keep keys containing 'ens' or 'elastic'\n",
    "filtered_experiments = {\n",
    "    key: value for key, value in EXPERIMENTS.items() \n",
    "    if 'ens' in key.lower() or 'elastic' in key.lower()\n",
    "}\n",
    "print(f\"Remaining experiments: {list(filtered_experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all SMCMC experiments from EXPERIMENTS\n",
    "smcmc_experiments = {\n",
    "    key: value for key, value in EXPERIMENTS.items() \n",
    "    if 'smcmc' in key.lower()\n",
    "}\n",
    "print(f\"SMCMC experiments: {list(smcmc_experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SMCMC experiments to filtered_experiments\n",
    "filtered_experiments.update(smcmc_experiments)\n",
    "print(f\"Updated filtered_experiments with SMCMC. Total experiments: {len(filtered_experiments)}\")\n",
    "print(f\"Keys: {list(filtered_experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "from joblib import dump\n",
    "\n",
    "from utility_functions import seed_all\n",
    "\n",
    "\n",
    "def simulate_all_trajectories(\n",
    "    experiments: dict,\n",
    "    base_output_dir: str,\n",
    "    num_trajectories: int = 200,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    After training, load results and generate/simulate trajectories for each experiment.\n",
    "\n",
    "    Args:\n",
    "        experiments: Same dictionary passed to run_all_experiments().\n",
    "            For GFlow, dict should include:\n",
    "              - fwd_model_path\n",
    "              - bwd_model_path\n",
    "              - run_params_path\n",
    "              - num_runs\n",
    "              - trajectory_length (optional)\n",
    "              - n_trajectories (optional)\n",
    "              - extra_parameters (optional)\n",
    "        base_output_dir: Root folder where training results are stored.\n",
    "        num_trajectories: Number of trajectories to simulate per RL agent (ignored for GFlow if overridden).\n",
    "\n",
    "    Returns:\n",
    "        None. Saves a `trajectories_results.joblib` file per run.\n",
    "    \"\"\"\n",
    "    sys.path.append('../.')\n",
    "\n",
    "    for run_name, params in experiments.items():\n",
    "        print(f\"\\n=== Simulating trajectories for: {run_name} ===\")\n",
    "        run_output_dir = os.path.join(base_output_dir, run_name)\n",
    "        os.makedirs(run_output_dir, exist_ok=True)\n",
    "\n",
    "        run_name_lower = run_name.lower()\n",
    "\n",
    "        if \"gflow\" in run_name_lower:\n",
    "            algo = \"gflow\"\n",
    "        elif \"reinforce\" in run_name_lower or \"sac\" in run_name_lower:\n",
    "            algo = \"rl\"\n",
    "        elif \"smcmc\" in run_name_lower:\n",
    "            algo = \"smcmc\"\n",
    "        else:\n",
    "            print(f\"Skipping {run_name} (unsupported for simulation)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Oracle Used: {params['oracle_path']}\")\n",
    "\n",
    "        if algo == \"rl\":\n",
    "            results_file = os.path.join(run_output_dir, f\"{run_name_lower.split('_')[0]}_results.joblib\")\n",
    "            print(f\"Loading RL results from {results_file}...\")\n",
    "            if not os.path.exists(results_file):\n",
    "                print(f\"Skipping {run_name}, results file not found.\")\n",
    "                continue\n",
    "\n",
    "            results_runs = joblib.load(results_file)\n",
    "            config, env, logger, training_parameters, initial_state, feature_names = (\n",
    "                setup_environment_and_logger(params[\"config_path\"], params[\"oracle_path\"])\n",
    "            )\n",
    "\n",
    "            traj_results = {}\n",
    "            for run_idx, run_data in results_runs.items():\n",
    "                trained_agent = run_data[\"trained_agent\"]\n",
    "                print(f\"  Generating RL trajectories for run {run_idx}...\")\n",
    "                trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "                    env,\n",
    "                    trained_agent=trained_agent,\n",
    "                    num_trajectories=num_trajectories,\n",
    "                    max_steps=training_parameters[\"trajectory_length\"],\n",
    "                )\n",
    "                last_states = np.array([traj[-1] for traj in trajectories])\n",
    "                traj_results[run_idx] = {\n",
    "                    \"trajectories\": trajectories,\n",
    "                    \"rewards\": rewards,\n",
    "                    \"actions\": all_actions,\n",
    "                    \"last_states\": last_states,\n",
    "                    \"feature_names\": f_names,\n",
    "                    \"seed\": run_data.get(\"seed\"),\n",
    "                }\n",
    "\n",
    "            dump(traj_results, os.path.join(run_output_dir, \"trajectories_results.joblib\"))\n",
    "            print(f\"Saved RL trajectory simulations for {run_name}\")\n",
    "\n",
    "        elif algo == \"smcmc\":\n",
    "            results_file = os.path.join(run_output_dir, \"smcmc_results.joblib\")\n",
    "            if not os.path.exists(results_file):\n",
    "                print(f\"Skipping {run_name}, SMCMC results file not found.\")\n",
    "                continue\n",
    "\n",
    "            results_runs = joblib.load(results_file)\n",
    "            summarized_results = {}\n",
    "            for run_idx, run_data in results_runs.items():\n",
    "                results_dict = run_data[\"results_dict\"]\n",
    "                summarized_results[run_idx] = {\n",
    "                    \"trajectories\": results_dict[\"trajectories\"],\n",
    "                    \"rewards\": results_dict[\"rewards\"],\n",
    "                    \"actions\": results_dict[\"actions\"],\n",
    "                    \"last_states\": results_dict[\"last_states\"],\n",
    "                    \"feature_names\": results_dict[\"feature_names\"],\n",
    "                    \"seed\": run_data.get(\"seed\"),\n",
    "                }\n",
    "\n",
    "            dump(summarized_results, os.path.join(run_output_dir, \"trajectories_results.joblib\"))\n",
    "            print(f\"Saved SMCMC summarized trajectories for {run_name}\")\n",
    "\n",
    "        elif algo == \"gflow\":\n",
    "            from utility_functions import load_initial_state, setup_logger, simulate_trajectories\n",
    "\n",
    "            # Load the joblib file containing all trained forward/backward models\n",
    "            models_dict = joblib.load(params[\"gflow_results_path\"])\n",
    "            config = yaml.safe_load(open(params[\"run_params_path\"], \"r\"))\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            results_runs = {}\n",
    "\n",
    "            for run_idx, run_data in models_dict.items():\n",
    "                seed = run_data[\"seed\"]\n",
    "                seed_all(seed)\n",
    "                print(f\"  Generating GFlow trajectories for run {run_idx+1}/{len(models_dict)} (seed={seed})...\")\n",
    "\n",
    "                forward_model = run_data[\"trained_agent\"]\n",
    "                backward_model = run_data[\"trained_backward_agent\"]\n",
    "\n",
    "                logger, _ = setup_logger(f\"gflow_run_{run_idx}\")\n",
    "                initial_state, feature_names = load_initial_state(config)\n",
    "                extra_params = params.get(\"extra_parameters\", {})\n",
    "                print(f\"Oracle Used: {params['oracle_path']}\")\n",
    "                trajectories, rewards, all_actions, _ = simulate_trajectories(\n",
    "                    env_class=GeneralEnvironment,\n",
    "                    forward_model=forward_model,\n",
    "                    backward_model=backward_model,\n",
    "                    initial_state=initial_state,\n",
    "                    config=config,\n",
    "                    trajectory_length=params.get(\"trajectory_length\", 12),\n",
    "                    n_trajectories=params.get(\"n_trajectories\", 200),\n",
    "                    device=device,\n",
    "                    logger=logger,\n",
    "                    model_path=params[\"oracle_path\"],\n",
    "                    distribution=\"mixture_beta\",\n",
    "                    extra_parameters=extra_params,\n",
    "                )\n",
    "\n",
    "                results_runs[run_idx] = {\n",
    "                    \"trajectories\": trajectories,\n",
    "                    \"rewards\": np.array(rewards),\n",
    "                    \"all_actions\": all_actions,\n",
    "                    \"seed\": seed,\n",
    "                }\n",
    "\n",
    "            dump(results_runs, os.path.join(run_output_dir, f\"{run_name}_trajectories_results.joblib\"))\n",
    "            print(f\"Saved GFlow trajectories for {run_name}\")\n",
    "\n",
    "\n",
    "\n",
    "simulate_all_trajectories(\n",
    "    experiments=filtered_experiments,\n",
    "    base_output_dir='results/All_Experiment_Results',\n",
    "    num_trajectories=200,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_all_trajectories(\n",
    "    experiments: dict,\n",
    "    base_output_dir: str,\n",
    "    num_trajectories: int = 200,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Unified simulation runner.\n",
    "    It loads the experiment configuration and then, based on the run name,\n",
    "    simulates trajectories using RL, SMCMC or GFlowNet approaches.\n",
    "\n",
    "    For SMCMC (which has no external model to load), the trajectories are generated\n",
    "    from scratch by calling generate_top_mcmc_results.\n",
    "    \n",
    "    Args:\n",
    "        experiments: Dictionary mapping run name to parameters.\n",
    "        base_output_dir: Root folder where simulation results are stored.\n",
    "        num_trajectories: Number of trajectories to simulate per run (for RL; SMCMC uses its own parameters).\n",
    "    \n",
    "    Returns:\n",
    "        None. Results are saved as joblib files.\n",
    "    \"\"\"\n",
    "    sys.path.append('../.')\n",
    "    for run_name, params in experiments.items():\n",
    "        print(f\"\\n=== Simulating trajectories for: {run_name} ===\")\n",
    "        run_output_dir = os.path.join(base_output_dir, run_name)\n",
    "        os.makedirs(run_output_dir, exist_ok=True)\n",
    "\n",
    "        run_name_lower = run_name.lower()\n",
    "        if \"gflow\" in run_name_lower:\n",
    "            algo = \"gflow\"\n",
    "        elif \"reinforce\" in run_name_lower or \"sac\" in run_name_lower:\n",
    "            algo = \"rl\"\n",
    "        elif \"smcmc\" in run_name_lower:\n",
    "            algo = \"smcmc\"\n",
    "        else:\n",
    "            print(f\"Skipping {run_name} (unsupported for simulation)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Oracle Used: {params['oracle_path']}\")\n",
    "\n",
    "        if algo == \"rl\":\n",
    "            results_file = os.path.join(run_output_dir, f\"{run_name_lower.split('_')[0]}_results.joblib\")\n",
    "            if not os.path.exists(results_file):\n",
    "                print(f\"Skipping {run_name}, results file not found.\")\n",
    "                continue\n",
    "            results_runs = joblib.load(results_file)\n",
    "            config, env, logger, training_parameters, initial_state, feature_names = (\n",
    "                setup_environment_and_logger(params[\"config_path\"], params[\"oracle_path\"])\n",
    "            )\n",
    "            traj_results = {}\n",
    "            for run_idx, run_data in results_runs.items():\n",
    "                trained_agent = run_data[\"trained_agent\"]\n",
    "                print(f\"  Generating RL trajectories for run {run_idx}...\")\n",
    "                # Replace generate_high_reward_trajectories with your RL function if needed.\n",
    "                trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "                    env,\n",
    "                    trained_agent=trained_agent,\n",
    "                    num_trajectories=num_trajectories,\n",
    "                    max_steps=training_parameters[\"trajectory_length\"],\n",
    "                )\n",
    "                last_states = np.array([traj[-1] for traj in trajectories])\n",
    "                traj_results[run_idx] = {\n",
    "                    \"trajectories\": trajectories,\n",
    "                    \"rewards\": rewards,\n",
    "                    \"actions\": all_actions,\n",
    "                    \"last_states\": last_states,\n",
    "                    \"feature_names\": f_names,\n",
    "                    \"seed\": run_data.get(\"seed\"),\n",
    "                }\n",
    "            dump(traj_results, os.path.join(run_output_dir, \"trajectories_results.joblib\"))\n",
    "            print(f\"Saved RL trajectory simulations for {run_name}\")\n",
    "\n",
    "        elif algo == \"smcmc\":\n",
    "            config, env, logger, training_parameters, initial_state, feature_names = (\n",
    "                setup_environment_and_logger(params[\"config_path\"], params[\"oracle_path\"])\n",
    "            )\n",
    "            # For SMCMC, generate trajectories from scratch using your SMCMC function.\n",
    "            results_runs = {}\n",
    "            for run in range(params.get(\"num_runs\", 30)):\n",
    "                print(f\"  SMCMC Run {run+1}/{params.get('num_runs', 30)}\")\n",
    "                random_seed = random.randint(0, 10000)\n",
    "                seed_all(random_seed)\n",
    "                trajs, rewards, actions, fname_out, non_sorted_rewards = generate_top_mcmc_results(\n",
    "                    env=env,\n",
    "                    num_trajectories=params.get(\"n_trajectories\", 2000),\n",
    "                    num_steps=training_parameters[\"trajectory_length\"],\n",
    "                    proposal_std=params.get(\"proposal_std\", 10),\n",
    "                    feature_names=['Timestamp'] + feature_names,\n",
    "                    temperature=params.get(\"temperature\", 10),\n",
    "                )\n",
    "                # Squeeze, if needed\n",
    "                rewards = rewards.squeeze()\n",
    "                trajs = trajs.squeeze()\n",
    "                actions = actions.squeeze()\n",
    "                last_states = np.array([traj[-1] for traj in trajs])\n",
    "                # Keep top 200 trajectories\n",
    "                trajs = trajs[:200]\n",
    "                rewards = rewards[:200]\n",
    "                actions = actions[:200]\n",
    "                last_states = last_states[:200]\n",
    "                results_dict = {\n",
    "                    \"trajectories\": trajs,\n",
    "                    \"rewards\": rewards,\n",
    "                    \"actions\": actions,\n",
    "                    \"last_states\": last_states,\n",
    "                    \"feature_names\": fname_out,\n",
    "                    \"non_sorted_rewards\": non_sorted_rewards,\n",
    "                }\n",
    "                results_runs[run] = {\n",
    "                    \"results_dict\": results_dict,\n",
    "                    \"seed\": random_seed,\n",
    "                }\n",
    "            dump(results_runs, os.path.join(run_output_dir, \"trajectories_results.joblib\"))\n",
    "            print(f\"Saved SMCMC trajectory simulations for {run_name}\")\n",
    "\n",
    "        elif algo == \"gflow\":\n",
    "            # (GFlow branch remains unchanged; include your GFlow simulation code here.)\n",
    "            from utility_functions import load_initial_state, setup_logger, simulate_trajectories\n",
    "            models_dict = joblib.load(params[\"gflow_results_path\"])\n",
    "            config = yaml.safe_load(open(params[\"run_params_path\"], \"r\"))\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            results_runs = {}\n",
    "            for run_idx, run_data in models_dict.items():\n",
    "                seed = run_data[\"seed\"]\n",
    "                seed_all(seed)\n",
    "                print(f\"  Generating GFlow trajectories for run {run_idx+1}/{len(models_dict)} (seed={seed})...\")\n",
    "                forward_model = run_data[\"trained_agent\"]\n",
    "                backward_model = run_data[\"trained_backward_agent\"]\n",
    "                logger, _ = setup_logger(f\"gflow_run_{run_idx}\")\n",
    "                initial_state, feature_names = load_initial_state(config)\n",
    "                extra_params = params.get(\"extra_parameters\", {})\n",
    "                trajectories, rewards, all_actions, _ = simulate_trajectories(\n",
    "                    env_class=GeneralEnvironment,\n",
    "                    forward_model=forward_model,\n",
    "                    backward_model=backward_model,\n",
    "                    initial_state=initial_state,\n",
    "                    config=config,\n",
    "                    trajectory_length=params.get(\"trajectory_length\", 12),\n",
    "                    n_trajectories=params.get(\"n_trajectories\", 200),\n",
    "                    device=device,\n",
    "                    logger=logger,\n",
    "                    model_path=params[\"oracle_path\"],\n",
    "                    distribution=\"mixture_beta\",\n",
    "                    extra_parameters=extra_params,\n",
    "                )\n",
    "                results_runs[run_idx] = {\n",
    "                    \"trajectories\": trajectories,\n",
    "                    \"rewards\": np.array(rewards),\n",
    "                    \"all_actions\": all_actions,\n",
    "                    \"seed\": seed,\n",
    "                }\n",
    "            dump(results_runs, os.path.join(run_output_dir, f\"{run_name}_trajectories_results.joblib\"))\n",
    "            print(f\"Saved GFlow trajectories for {run_name}\")\n",
    "            \n",
    "simulate_all_trajectories(\n",
    "    experiments=filtered_experiments,\n",
    "    base_output_dir='results/All_Experiment_Results',\n",
    "    num_trajectories=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-reward trajectories\n",
    "high_reward_trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "        env,\n",
    "        trained_agent=trained_agent,\n",
    "        num_trajectories=200,\n",
    "        max_steps=12,\n",
    "    )\n",
    "model_identifier = MODEL_PATH.split('.')[0][-8:]\n",
    "\n",
    "# Extract last states from each trajectory\n",
    "last_states = np.array([traj[-1] for traj in high_reward_trajectories])\n",
    "\n",
    "# Create a dict with all data\n",
    "results_dict = {\n",
    "    'trajectories': high_reward_trajectories,\n",
    "    'rewards': rewards,\n",
    "    'actions': all_actions,\n",
    "    'last_states': last_states,\n",
    "    'feature_names': f_names\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectsVenv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
