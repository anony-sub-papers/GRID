{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Evaluation script for pre-trained GFlowNet models.\n",
    "\n",
    "This script allows loading trained GFlowNet models to generate trajectories,\n",
    "analyze rewards, and create visualizations for research papers/theses.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import yaml\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "from utilities.REINFORCE_Support import *\n",
    "from utilities.SMCMC_Support import *\n",
    "from utilities.testing_utils import *\n",
    "from utilities.BayesOpt_Support import * # Import project modules\n",
    "from src.environments import GeneralEnvironment\n",
    "from src.mlflow_logger import MLflowLogger\n",
    "from src.utility_functions import (\n",
    "    calculate_cosine_diversity, get_policy_dist, load_initial_state,\n",
    "    load_entire_model, plot_distributions_per_feature,\n",
    "    setup_logger, simulate_trajectories,seed_all\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "import joblib\n",
    "try:\n",
    "    from fastdtw import fastdtw\n",
    "except ImportError:\n",
    "    warnings.warn(\"fastdtw not installed; falling back to naive DTW (slower).\")\n",
    "    def fastdtw(x, y, dist=None):\n",
    "        # naive DTW for 1-D arrays\n",
    "        T1, T2 = len(x), len(y)\n",
    "        dtw_mat = np.full((T1+1, T2+1), np.inf)\n",
    "        dtw_mat[0, 0] = 0.0\n",
    "        for i in range(1, T1+1):\n",
    "            for j in range(1, T2+1):\n",
    "                cost = abs(x[i-1] - y[j-1])\n",
    "                dtw_mat[i, j] = cost + min(\n",
    "                    dtw_mat[i-1, j], dtw_mat[i, j-1], dtw_mat[i-1, j-1]\n",
    "                )\n",
    "        return dtw_mat[T1, T2], None\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def average_reward(rewards: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean of rewards.\n",
    "    \"\"\"\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def tail_coverage(predictions: np.ndarray, tau: float) -> float:\n",
    "    \"\"\"\n",
    "    Percentage of trajectories with predicted return <= tau.\n",
    "    \"\"\"\n",
    "    return np.mean(predictions <= tau)\n",
    "\n",
    "def expected_shortfall(predictions: np.ndarray, q: float) -> float:\n",
    "    \"\"\"\n",
    "    Expected Shortfall (ES) at quantile q (in percent).\n",
    "    \"\"\"\n",
    "    cutoff = np.percentile(predictions, q)\n",
    "    print(f\"Cutoff for ES: {cutoff}\")\n",
    "    worst = predictions[predictions <= cutoff]\n",
    "    print(f\"Worst predictions for ES: {worst}\")\n",
    "    return worst.mean() if worst.size > 0 else np.nan\n",
    "\n",
    "def calculate_euclidean_diversity(states: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Average and normalized diversity based on pairwise Euclidean distances.\n",
    "    Normalization is by the maximum observed distance.\n",
    "    \"\"\"\n",
    "    distances = pdist(states, metric='euclidean')\n",
    "    avg_div = distances.mean()\n",
    "    max_div = distances.max()\n",
    "    norm_div = avg_div / max_div if max_div > 0 else np.nan\n",
    "    return avg_div, norm_div, distances\n",
    "\n",
    "def coverage_epsilon(states: np.ndarray, eps: float) -> int:\n",
    "    \"\"\"\n",
    "    Greedy epsilon-cover count: number of unique centers such that\n",
    "    all points are within eps of at least one center.\n",
    "    \"\"\"\n",
    "    N = states.shape[0]\n",
    "    assigned = np.zeros(N, dtype=bool)\n",
    "    centers = 0\n",
    "    for i in range(N):\n",
    "        if not assigned[i]:\n",
    "            centers += 1\n",
    "            dists = np.linalg.norm(states - states[i], axis=1)\n",
    "            assigned |= (dists < eps)\n",
    "    return centers\n",
    "\n",
    "def calculate_quality_diversity(states: np.ndarray, rewards: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute diversity metrics adjusted for reward quality.\n",
    "\n",
    "    Args:\n",
    "        states: Array of shape (N, D) representing final states or trajectory summaries.\n",
    "        rewards: Array of shape (N,) of corresponding rewards.\n",
    "\n",
    "    Returns:\n",
    "        A dict with:\n",
    "        - avg_div: mean pairwise Euclidean distance\n",
    "        - norm_div: avg_div normalized by the maximum pairwise distance\n",
    "        - avg_reward: mean reward\n",
    "        - reward_norm: (avg_reward - min_reward) / (max_reward - min_reward)\n",
    "        - quality_diversity: avg_div * reward_norm (high only when diversity and reward quality are high)\n",
    "        - distances: raw pairwise distances\n",
    "    \"\"\"\n",
    "    distances = pdist(states, metric='euclidean')\n",
    "    avg_div = float(distances.mean())\n",
    "    rewards = copy.deepcopy(rewards)\n",
    "\n",
    "    max_r = np.max(rewards)\n",
    "    min_r = np.min(rewards)\n",
    "    reward_range = np.abs(max_r - min_r)\n",
    "    reward_scale = np.abs(max_r) + np.abs(min_r) + 1e-8  # Robust to sign and small values\n",
    "\n",
    "    reward_stability = 1 - (reward_range / reward_scale)\n",
    "    # Calculate normalized distance that takes rewards into account\n",
    "    normalized_score = avg_div * reward_stability    \n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"average_diversity\": avg_div,\n",
    "        \"normalized_score\": normalized_score,\n",
    "        \"distances\": distances\n",
    "    }\n",
    "\n",
    "def cluster_trajectories(\n",
    "    dist_matrix: np.ndarray,\n",
    "    n_clusters: int = 3\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster trajectories using Agglomerative Clustering on a precomputed distance matrix.\n",
    "    \"\"\"\n",
    "    model = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters\n",
    "    )\n",
    "    labels = model.fit_predict(dist_matrix)\n",
    "    return labels\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "def compute_dtw_distance_matrix(trajectories: np.ndarray) -> (np.ndarray, dict):\n",
    "    # Ensure trajectories is a 3D array (N_samples, N_timesteps, N_features)\n",
    "    trajectories = np.array([np.array(t) for t in trajectories])\n",
    "    \n",
    "    N = len(trajectories)\n",
    "    D = trajectories[0].shape[1]  # Number of features\n",
    "    \n",
    "    dist_matrix = np.zeros((N, N), dtype=float)\n",
    "    feature_dist_matrices = {d: np.zeros((N, N), dtype=float) for d in range(D)}\n",
    "    \n",
    "    total_pairs = N * (N - 1) // 2\n",
    "    for i, j in tqdm(combinations(range(N), 2), total=total_pairs, desc=\"Computing DTW distances\"):\n",
    "        dists = []\n",
    "        for d in range(D):\n",
    "            # Extract 1D time series for each feature\n",
    "            seq_i = trajectories[i][:, d]\n",
    "            seq_j = trajectories[j][:, d]\n",
    "            dist, _ = fastdtw(seq_i, seq_j, dist=euclidean_distance)\n",
    "            feature_dist_matrices[d][i, j] = dist\n",
    "            feature_dist_matrices[d][j, i] = dist\n",
    "            dists.append(dist)\n",
    "        agg_dist = np.mean(dists)\n",
    "        dist_matrix[i, j] = agg_dist\n",
    "        dist_matrix[j, i] = agg_dist\n",
    "    \n",
    "    return dist_matrix, feature_dist_matrices\n",
    "\n",
    "def dtw_clustering_analysis(trajectories, n_clusters=3):\n",
    "    # Convert list of trajectories to numpy array if needed\n",
    "    if isinstance(trajectories, list):\n",
    "        trajectories = np.array([np.array(t) for t in trajectories])\n",
    "    \n",
    "    # Ensure we have enough samples for the requested number of clusters\n",
    "    if len(trajectories) < n_clusters:\n",
    "        raise ValueError(f\"Number of trajectories ({len(trajectories)}) must be >= number of clusters ({n_clusters})\")\n",
    "    \n",
    "    dist_matrix, feature_dists = compute_dtw_distance_matrix(trajectories)\n",
    "    agg_labels = cluster_trajectories(dist_matrix, n_clusters=n_clusters)\n",
    "    \n",
    "    df = pd.DataFrame({\"trajectory\": np.arange(len(trajectories)), \n",
    "                      \"agg_cluster\": agg_labels})\n",
    "    \n",
    "    # Cluster each feature separately\n",
    "    feature_labels = {}\n",
    "    for d, fmat in feature_dists.items():\n",
    "        labels = cluster_trajectories(fmat, n_clusters=n_clusters)\n",
    "        df[f\"feature_{d}_cluster\"] = labels\n",
    "        feature_labels[d] = labels\n",
    "    \n",
    "    return df, dist_matrix, feature_labels\n",
    "\n",
    "def plot_trajectories_by_cluster(results_df, trajectories, feature_names,num_clusters=5,save_path=None):    \n",
    "    \"\"\"\n",
    "    Plot trajectories for each feature, colored by feature-specific clusters.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing cluster assignments\n",
    "        trajectories (np.ndarray): Array of trajectories\n",
    "        feature_names (list): List of feature names\n",
    "        save_path (str): Optional path to save the plot\n",
    "    \"\"\"\n",
    "    # Get number of features and clusters\n",
    "    n_features = trajectories.shape[2] - 1  # Excluding timestep\n",
    "    n_clusters = num_clusters  # Based on the cluster assignments in results_df\n",
    "    \n",
    "    # Create color palette\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(40, 24))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Time points for x-axis\n",
    "    time_points = np.arange(trajectories.shape[1])\n",
    "    \n",
    "    # Plot each feature\n",
    "    for feature_idx in range(n_features):\n",
    "        ax = axs[feature_idx]\n",
    "        cluster_column = f'feature_{feature_idx}_cluster'\n",
    "        \n",
    "        # Plot trajectories for each cluster\n",
    "        for cluster in range(n_clusters):\n",
    "            # Get indices for current feature's cluster\n",
    "            cluster_indices = results_df[results_df[cluster_column] == cluster]['trajectory'].values\n",
    "            \n",
    "            # Plot each trajectory in cluster\n",
    "            for idx in cluster_indices:\n",
    "                ax.plot(time_points, \n",
    "                       trajectories[idx, :, feature_idx + 1],  # +1 to skip timestep\n",
    "                       color=colors[cluster], \n",
    "                       alpha=0.3, \n",
    "                       linewidth=1)\n",
    "            \n",
    "            # Plot mean trajectory for cluster\n",
    "            if len(cluster_indices) > 0:\n",
    "                cluster_mean = trajectories[cluster_indices, :, feature_idx + 1].mean(axis=0)\n",
    "                ax.plot(time_points, \n",
    "                       cluster_mean, \n",
    "                       color=colors[cluster], \n",
    "                       linewidth=3, \n",
    "                       label=f'Cluster {cluster} (n={len(cluster_indices)})')\n",
    "        \n",
    "        # Customize subplot\n",
    "        ax.set_title(f'{feature_names[feature_idx+1]}', fontsize=12, pad=10)\n",
    "        ax.set_xlabel('Time Step', fontsize=10)\n",
    "        ax.set_ylabel('Value', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=24)\n",
    "    \n",
    "    # Remove empty subplots if any\n",
    "    for idx in range(n_features, len(axs)):\n",
    "        fig.delaxes(axs[idx])\n",
    "    \n",
    "    # Adjust layout and add main title\n",
    "    plt.suptitle('Trajectory Clusters by Feature', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    else:\n",
    "        plt.savefig('trajectory_clusters_by_feature.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_dtw_distance_matrix(distance_matrix, model_name, rewards, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save DTW distance matrix with additional metrics.\n",
    "    \n",
    "    Args:\n",
    "        distance_matrix: The DTW distance matrix\n",
    "        model_name: Name of the model (e.g., 'GFlowNet', 'REINFORCE')\n",
    "        rewards: Array of rewards for normalization\n",
    "        save_path: Optional path to save the PDF file\n",
    "    \"\"\"\n",
    "    rewards = copy.deepcopy(rewards)\n",
    "\n",
    "    # Reward stability: penalize large drops among top-K\n",
    "    max_r = np.max(rewards)\n",
    "    min_r = np.min(rewards)\n",
    "    reward_range = np.abs(max_r - min_r)\n",
    "    reward_scale = np.abs(max_r) + np.abs(min_r) + 1e-8  # Robust to sign and small values\n",
    "    average_diversity = np.mean(distance_matrix)\n",
    "    reward_stability = 1 - (reward_range / reward_scale)\n",
    "    # Calculate normalized distance that takes rewards into account\n",
    "    normalized_score = average_diversity * reward_stability\n",
    "    \n",
    "    # Create figure with high resolution and professional sizing\n",
    "    plt.figure(figsize=(12, 10), dpi=300)\n",
    "    \n",
    "    # Create heatmap with improved styling\n",
    "    sns.heatmap(distance_matrix,\n",
    "                cmap='viridis',  # Professional colormap\n",
    "                square=True,     # Make cells square\n",
    "                fmt=\".2f\",  # Format for numbers\n",
    "                vmin=0, vmax=200,  # Set limits for color range\n",
    "                cbar_kws={\n",
    "                    'label': 'DTW Distance',\n",
    "                    'orientation': 'vertical',\n",
    "                    \n",
    "                })\n",
    "\n",
    "\n",
    "    # Add title with metrics\n",
    "    plt.title(f'DTW Distance Matrix - {model_name}\\n' + \n",
    "              f'Avg Distance: {average_diversity:.2f}\\n' +\n",
    "              f'Normalized Quality Score: {normalized_score:.4f}',\n",
    "              fontsize=16, \n",
    "              pad=20)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Trajectory Index', fontsize=12)\n",
    "    plt.ylabel('Trajectory Index', fontsize=12)\n",
    "    \n",
    "    # Improve tick labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for potential further use\n",
    "    return {\n",
    "        'average_distance': average_diversity,\n",
    "        'normalized_score': normalized_score,\n",
    "        'reward_normalization': normalized_score\n",
    "    }\n",
    "\n",
    "def find_min_max_distance_pairs(distance_matrix):\n",
    "    \"\"\"\n",
    "    Find pairs of trajectories with minimum and maximum distances.\n",
    "    \n",
    "    Args:\n",
    "        distance_matrix (np.ndarray): Square matrix of pairwise distances\n",
    "        \n",
    "    Returns:\n",
    "        tuple: ((i_min, j_min, min_dist), (i_max, j_max, max_dist))\n",
    "            where i,j are trajectory indices and dist is their distance\n",
    "    \"\"\"\n",
    "    # Create mask for upper triangle (excluding diagonal)\n",
    "    mask = np.triu(np.ones_like(distance_matrix), k=1).astype(bool)\n",
    "    \n",
    "    # Get upper triangle values\n",
    "    distances = distance_matrix[mask]\n",
    "    \n",
    "    # Find min and max distances\n",
    "    min_dist = np.min(distances)\n",
    "    max_dist = np.max(distances)\n",
    "    \n",
    "    # Get indices for min distance\n",
    "    min_idx = np.where(distance_matrix == min_dist)\n",
    "    i_min, j_min = min_idx[0][0], min_idx[1][0]\n",
    "    \n",
    "    # Get indices for max distance\n",
    "    max_idx = np.where(distance_matrix == max_dist)\n",
    "    i_max, j_max = max_idx[0][0], max_idx[1][0]\n",
    "    \n",
    "    # Ensure i < j for consistency\n",
    "    if i_min > j_min:\n",
    "        i_min, j_min = j_min, i_min\n",
    "    if i_max > j_max:\n",
    "        i_max, j_max = j_max, i_max\n",
    "        \n",
    "    return ((i_min, j_min, min_dist), (i_max, j_max, max_dist))\n",
    "\n",
    "def plot_trajectories_over_time(trajectories, rewards, feature_names, n_top=50, alpha_others=0.05,save_path=None):\n",
    "    \"\"\"\n",
    "    Plot trajectories over time for each feature, highlighting top performers.\n",
    "    \n",
    "    Args:\n",
    "        trajectories (np.ndarray): Array of shape (n_trajectories, timesteps, features)\n",
    "        rewards (np.ndarray): Array of shape (n_trajectories, timesteps)\n",
    "        feature_names (list): List of feature names\n",
    "        n_top (int): Number of top trajectories to highlight\n",
    "        alpha_others (float): Alpha value for non-top trajectories\n",
    "        save_path (str): Optional path to save the plot\n",
    "    \"\"\"\n",
    "    # Get indices of top n rewards\n",
    "    last_step_rewards = rewards\n",
    "    top_indices = np.argsort(last_step_rewards)[-n_top:][::-1]\n",
    "    top_colors = sns.color_palette(\"husl\", n_top)\n",
    "    \n",
    "    n_features = trajectories.shape[2]\n",
    "    \n",
    "    # Plot each feature individually\n",
    "    for feature_idx in range(n_features):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot all trajectories\n",
    "        for traj_idx, trajectory in enumerate(trajectories):\n",
    "            if traj_idx in top_indices:\n",
    "                color_idx = np.where(top_indices == traj_idx)[0][0]\n",
    "                plt.plot(trajectory[:, feature_idx], alpha=1,\n",
    "                        color=top_colors[color_idx], linewidth=1, linestyle='dashed')\n",
    "            else:\n",
    "                plt.plot(trajectory[:, feature_idx], alpha=alpha_others, color='blue')\n",
    "        \n",
    "        # Add mean and median\n",
    "        mean_feature = np.mean(trajectories[:, :, feature_idx], axis=0)\n",
    "        median_feature = np.median(trajectories[:, :, feature_idx], axis=0)\n",
    "        plt.plot(mean_feature, color='black', linewidth=2, label='Mean')\n",
    "        plt.plot(median_feature, color='red', linewidth=2, label='Median')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.legend(prop={'size': 10}, loc='upper left')\n",
    "        plt.xlabel(\"Time Step\", fontsize=10)\n",
    "        plt.ylabel(f\"{feature_names[feature_idx]} Value\", fontsize=10)\n",
    "        plt.title(f\"Feature: {feature_names[feature_idx]}\", fontsize=10)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(os.path.join(save_path, f\"feature_{feature_names[feature_idx]}.pdf\"), dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "def compare_trajectories(traj_idx1, traj_idx2, trajectories, feature_names, rewards,save_path=None):\n",
    "    \"\"\"\n",
    "    Compare two trajectories by plotting features against time.\n",
    "    \n",
    "    Args:\n",
    "        traj_idx1 (int): Index of first trajectory\n",
    "        traj_idx2 (int): Index of second trajectory\n",
    "        trajectories (np.ndarray): Array of trajectories\n",
    "        feature_names (list): List of feature names\n",
    "        rewards (np.ndarray): Array of rewards\n",
    "        save_path (str): Optional path to save the plot\n",
    "    \"\"\"\n",
    "    # Get the two trajectories\n",
    "    traj1 = trajectories[traj_idx1]\n",
    "    traj2 = trajectories[traj_idx2]\n",
    "    \n",
    "    # Get number of features (excluding timestamp)\n",
    "    n_features = traj1.shape[1] - 1\n",
    "    \n",
    "    # Create a grid of subplots\n",
    "    fig, axs = plt.subplots(n_features, 1, figsize=(15, 4*n_features))\n",
    "    \n",
    "    # Get timestamps\n",
    "    timestamps = traj1[:, 0]\n",
    "    \n",
    "    # Plot each feature\n",
    "    for i in range(n_features):\n",
    "        # Plot feature against time\n",
    "        axs[i].plot(timestamps, traj1[:, i+1], 'b-', label=f'Traj {traj_idx1}', linewidth=2)\n",
    "        axs[i].plot(timestamps, traj2[:, i+1], 'r--', label=f'Traj {traj_idx2}', linewidth=2)\n",
    "        \n",
    "        # Mark start and end points\n",
    "        axs[i].plot(timestamps[0], traj1[0, i+1], 'bo', label='Start 1')\n",
    "        axs[i].plot(timestamps[-1], traj1[-1, i+1], 'b*', label='End 1', markersize=10)\n",
    "        axs[i].plot(timestamps[0], traj2[0, i+1], 'ro', label='Start 2')\n",
    "        axs[i].plot(timestamps[-1], traj2[-1, i+1], 'r*', label='End 2', markersize=10)\n",
    "        \n",
    "        # Add labels\n",
    "        axs[i].set_xlabel('Time')\n",
    "        axs[i].set_ylabel(feature_names[i+1])\n",
    "        \n",
    "        # Add grid\n",
    "        axs[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Only add legend to first subplot\n",
    "        if i == 0:\n",
    "            axs[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add title with trajectory information\n",
    "    plt.suptitle(f'Comparison of Trajectories {traj_idx1} vs {traj_idx2}\\n' + \n",
    "                 f'Final Rewards: {rewards[traj_idx1]:.2f} vs {rewards[traj_idx2]:.2f}',\n",
    "                 fontsize=16, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_action_distributions_by_timestep(trajectories, all_actions, feature_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the distribution of actions for each feature at each timestep.\n",
    "    \n",
    "    Args:\n",
    "        trajectories (np.ndarray): Array of shape (n_trajectories, timesteps, features)\n",
    "        all_actions (np.ndarray): Array of shape (n_trajectories, timesteps, features)\n",
    "        feature_names (list): List of feature names\n",
    "        save_path (str, optional): Path to save output plots. If None, won't save plots.\n",
    "    \"\"\"\n",
    "    n_features = trajectories.shape[2] - 1\n",
    "    n_timesteps = trajectories.shape[1] - 1\n",
    "\n",
    "    for feature_idx in range(n_features-1):\n",
    "        # Create a figure for the current feature\n",
    "        fig, axs = plt.subplots(3, 4, figsize=(16, 12))  # Assuming 12 timesteps (3x4 grid)\n",
    "        axs = axs.flatten()  # Flatten the 2D grid to iterate easily\n",
    "        \n",
    "        # Iterate over each timestep for this feature\n",
    "        for timestep in range(n_timesteps):\n",
    "            # Get the actions for the current feature at the current timestep\n",
    "            actions_per_timestep = all_actions[:, timestep, feature_idx]\n",
    "            \n",
    "            # Plot the distribution of actions for this timestep\n",
    "            sns.histplot(actions_per_timestep, kde=True, ax=axs[timestep], stat=\"density\", bins=50)\n",
    "            \n",
    "            # Set labels and title for each subplot\n",
    "            axs[timestep].set_title(f\"Timestep {timestep + 1}\", fontsize=12)\n",
    "            axs[timestep].set_xlabel(f\"Action Value\", fontsize=10)\n",
    "            axs[timestep].set_ylabel(f\"Density\", fontsize=10)\n",
    "        \n",
    "        # Adjust layout and set a super title for the current feature\n",
    "        fig.suptitle(f\"Distribution of Sampled Actions - Feature {feature_names[feature_idx+1]}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the layout to fit the title\n",
    "        \n",
    "        # Save the figure if output directory is provided\n",
    "        if save_path:\n",
    "            save_pathz = os.path.join(save_path, f\"distribution_feature_{feature_names[feature_idx+1]}.pdf\")\n",
    "            plt.savefig(save_pathz, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "def plot_action_distributions(all_actions, feature_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Create distribution plots for actions across all features and timesteps.\n",
    "    \n",
    "    Args:\n",
    "        all_actions (np.ndarray): Array of shape (n_trajectories, timesteps, n_features)\n",
    "        feature_names (list): List of feature names\n",
    "        save\n",
    "         save_path (str, optional): Path to save the plot. If None, won't save.\n",
    "    \"\"\"\n",
    "    # Create a subplot for each feature\n",
    "    n_features = all_actions.shape[2]\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Iterate over each feature\n",
    "    for feature_idx in range(n_features):\n",
    "        # Get all actions for this feature across all timesteps\n",
    "        feature_actions = all_actions[:, :, feature_idx].flatten()\n",
    "        \n",
    "        # Create distribution plot\n",
    "        sns.histplot(feature_actions, kde=True, ax=axs[feature_idx], stat=\"density\", bins=50)\n",
    "        \n",
    "        # Add mean and median lines\n",
    "        mean_val = np.mean(feature_actions)\n",
    "        median_val = np.median(feature_actions)\n",
    "        axs[feature_idx].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "        axs[feature_idx].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "        \n",
    "        # Customize subplot\n",
    "        axs[feature_idx].set_title(f\"Distribution of Actions - {feature_names[feature_idx+1]}\", fontsize=12)\n",
    "        axs[feature_idx].set_xlabel(\"Action Value\")\n",
    "        axs[feature_idx].set_ylabel(\"Density\")\n",
    "        axs[feature_idx].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to high quality pdf if save_path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_reward_distribution(last_step_rewards, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the distribution of final rewards with statistics.\n",
    "    \n",
    "    Args:\n",
    "        last_step_rewards (array-like): Array of final reward values\n",
    "        save_path (str, optional): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Create a figure with proper sizing\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create the distribution plot using seaborn\n",
    "    sns.histplot(data=last_step_rewards, bins=50, kde=True, stat='density')\n",
    "\n",
    "    # Add mean and median lines\n",
    "    plt.axvline(np.mean(last_step_rewards), color='red', linestyle='dashed', \n",
    "                linewidth=2, label=f'Mean: {np.mean(last_step_rewards):.2f}')\n",
    "    plt.axvline(np.median(last_step_rewards), color='green', linestyle='dashed', \n",
    "                linewidth=2, label=f'Median: {np.median(last_step_rewards):.2f}')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Final Reward Values', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.title('Distribution of Final Rewards', fontsize=14, pad=20)\n",
    "\n",
    "    # Add statistics text box\n",
    "    stats_text = f'Statistics:\\n' \\\n",
    "                 f'Mean: {np.mean(last_step_rewards):.2f}\\n' \\\n",
    "                 f'Median: {np.median(last_step_rewards):.2f}\\n' \\\n",
    "                 f'Std: {np.std(last_step_rewards):.2f}\\n' \\\n",
    "                 f'Max: {np.max(last_step_rewards):.2f}\\n' \\\n",
    "                 f'Min: {np.min(last_step_rewards):.2f}'\n",
    "\n",
    "    plt.text(0.9, 0.75, stats_text, transform=plt.gca().transAxes, \n",
    "             verticalalignment='center', horizontalalignment='left',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "from scipy.spatial.distance import squareform\n",
    "def plot_diversity_metrics(df, feature_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot diversity metrics and euclidean distances for trajectories.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing trajectory information and rewards\n",
    "        feature_names (list): List of feature names\n",
    "        save_path (str, optional): Path to save the plot\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (avg_diversity, normalized_diversity, distances)\n",
    "    \"\"\"\n",
    "    # Calculate diversity metrics\n",
    "    finalrewards = df['FinalReward']\n",
    "    res_dict = calculate_quality_diversity(\n",
    "        df[feature_names].values,\n",
    "        finalrewards\n",
    "    )\n",
    "\n",
    "    avg_euclidean_diversity = res_dict['average_diversity']\n",
    "    norm_diversity = res_dict['normalized_score']\n",
    "    distances = res_dict['distances']\n",
    "\n",
    "    # Create heatmap with improved visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    euclidean_distances_matrix = squareform(distances)\n",
    "    sns.heatmap(euclidean_distances_matrix, \n",
    "                cmap='viridis', \n",
    "                fmt=\".2f\",\n",
    "                vmin=0,\n",
    "                vmax=200,  # Set maximum scale for better color contrast\n",
    "                cbar_kws={\n",
    "                    'label': 'Euclidean Distance',\n",
    "                    'orientation': 'vertical'\n",
    "                })\n",
    "\n",
    "    # Add title with diversity scores\n",
    "    plt.title(f\"Euclidean Distance Matrix for Trajectories\\n\" + \n",
    "              f\"Diversity Score: {avg_euclidean_diversity:.2f}, Normalized: {norm_diversity:.4f}\",\n",
    "              pad=20)\n",
    "\n",
    "    # Improve axis labels\n",
    "    plt.xlabel(\"Trajectory Index\", fontsize=12)\n",
    "    plt.ylabel(\"Trajectory Index\", fontsize=12)\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return avg_euclidean_diversity, norm_diversity, distances\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def perform_clustering_analysis(final_states_df, feature_names, n_clusters_range=(2, 10)):\n",
    "    \"\"\"\n",
    "    Perform clustering analysis on trajectory data and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        final_states_df (pd.DataFrame): DataFrame containing final states of trajectories\n",
    "        feature_names (list): List of feature names\n",
    "        n_clusters_range (tuple): Range of number of clusters to try (min, max)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (optimal_clusters, cluster_labels, tsne_results)\n",
    "    \"\"\"\n",
    "    # Scale the features before clustering\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(final_states_df)\n",
    "    \n",
    "    # Determine optimal number of clusters using silhouette score\n",
    "    silhouette_scores = []\n",
    "    K = range(*n_clusters_range)\n",
    "    for k in tqdm(K, desc=\"Finding optimal clusters\"):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "        silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Get optimal number of clusters\n",
    "    optimal_clusters = K[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    # Perform K-means clustering with optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(scaled_features)\n",
    "    \n",
    "    return optimal_clusters, cluster_labels, tsne_results\n",
    "\n",
    "def plot_reward_distributions_by_cluster(cluster_labels, rewards, optimal_clusters, \n",
    "                                       save_path=None, figsize=(12, 8), dpi=300):\n",
    "    \"\"\"\n",
    "    Plot reward distributions for each cluster with statistics.\n",
    "    \n",
    "    Args:\n",
    "        cluster_labels (array-like): Cluster assignments for each trajectory\n",
    "        rewards (array-like): Reward values for each trajectory\n",
    "        optimal_clusters (int): Number of clusters\n",
    "        save_path (str, optional): Path to save the plot\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "        dpi (int): Dots per inch for the plot\n",
    "        \n",
    "    Returns:\n",
    "        dict: Statistics for each cluster\n",
    "    \"\"\"\n",
    "    # Create figure with specified resolution and style\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # Get rewards for each cluster\n",
    "    cluster_rewards = []\n",
    "    for i in range(optimal_clusters):\n",
    "        cluster_rewards.append(np.array(rewards)[cluster_labels == i])\n",
    "    \n",
    "    # Use viridis color scheme\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, optimal_clusters))\n",
    "    labels = [f'Cluster {i+1} (n={len(rewards)})' for i, rewards in enumerate(cluster_rewards)]\n",
    "    \n",
    "    # Plot distributions\n",
    "    for i, rewards in enumerate(cluster_rewards):\n",
    "        sns.kdeplot(data=rewards, \n",
    "                   color=colors[i], \n",
    "                   alpha=0.7,\n",
    "                   linewidth=2,\n",
    "                   label=labels[i])\n",
    "    \n",
    "    # Add median lines for each cluster\n",
    "    for i, rewards in enumerate(cluster_rewards):\n",
    "        plt.axvline(np.median(rewards), \n",
    "                   color=colors[i], \n",
    "                   linestyle='--', \n",
    "                   alpha=0.5,\n",
    "                   linewidth=1.5)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Reward Distribution by Cluster', fontsize=14, pad=20)\n",
    "    plt.xlabel('Reward Value', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Calculate and format statistics\n",
    "    cluster_stats = {}\n",
    "    stats_text = 'Cluster Statistics:\\n\\n'\n",
    "    for i, rewards in enumerate(cluster_rewards):\n",
    "        stats = {\n",
    "            'median': np.median(rewards),\n",
    "            'mean': np.mean(rewards),\n",
    "            'std': np.std(rewards),\n",
    "            'size': len(rewards)\n",
    "        }\n",
    "        cluster_stats[f'Cluster_{i+1}'] = stats\n",
    "        stats_text += (f'Cluster {i+1}:\\n'\n",
    "                      f'Median: {stats[\"median\"]:.2f}\\n'\n",
    "                      f'Mean: {stats[\"mean\"]:.2f}\\n'\n",
    "                      f'Std: {stats[\"std\"]:.2f}\\n')\n",
    "    \n",
    "    # Add statistics text box\n",
    "    plt.text(1.35, 0.5, stats_text, \n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'),\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return cluster_stats\n",
    "\n",
    "def analyze_cluster_diversity(trajectories, cluster_labels, optimal_clusters, last_step_rewards, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze diversity within each cluster and create visualizations.\n",
    "    \n",
    "    Args:\n",
    "        trajectories (np.ndarray): Array of shape (n_trajectories, timesteps, features)\n",
    "        cluster_labels (np.ndarray): Cluster assignments for each trajectory\n",
    "        optimal_clusters (int): Number of clusters\n",
    "        last_step_rewards (np.ndarray): Final rewards for each trajectory\n",
    "        feature_names (list): Names of features\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples containing (cluster_id, diversity_score, normalized_diversity)\n",
    "    \"\"\"\n",
    "    cluster_diversity_scores = []\n",
    "    \n",
    "    for i in range(optimal_clusters):\n",
    "        # Get indices for current cluster\n",
    "        cluster_indices = np.where(cluster_labels == i)[0]\n",
    "        \n",
    "        # Select trajectories and rewards for current cluster\n",
    "        cluster_trajectories = trajectories[cluster_indices]\n",
    "        cluster_rewards = last_step_rewards[cluster_indices]\n",
    "        \n",
    "        # Calculate diversity scores\n",
    "        avg_euclidean_diversity, avg_euclidean_diversity_normalized, euclidean_distances = calculate_euclidean_diversity(\n",
    "            cluster_trajectories[:, -1, 1:],  # Take final timestep values, exclude timestep column\n",
    "            cluster_rewards\n",
    "        )\n",
    "        \n",
    "        cluster_diversity_scores.append((i, avg_euclidean_diversity, avg_euclidean_diversity_normalized))\n",
    "        \n",
    "        # Create heatmap for current cluster\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        from scipy.spatial.distance import squareform\n",
    "        sns.heatmap(squareform(euclidean_distances), \n",
    "                   cmap='viridis', \n",
    "                   fmt=\".2f\", \n",
    "                   cbar_kws={'label': 'Euclidean Distance'})\n",
    "        plt.title(f\"Cluster {i+1} - Euclidean Distance Matrix\\n\" + \n",
    "                 f\"Diversity Score: {avg_euclidean_diversity:.4f}, \" + \n",
    "                 f\"Normalized: {avg_euclidean_diversity_normalized:.4f}\")\n",
    "        plt.xlabel(\"Trajectory Index\")\n",
    "        plt.ylabel(\"Trajectory Index\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return cluster_diversity_scores\n",
    "\n",
    "def analyze_cluster_feature_variability(final_states_df, cluster_labels, optimal_clusters, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the feature variability within clusters using multiple visualization methods.\n",
    "    \n",
    "    Args:\n",
    "        final_states_df (pd.DataFrame): DataFrame containing the final states\n",
    "        cluster_labels (np.ndarray): Array of cluster assignments\n",
    "        optimal_clusters (int): Number of clusters\n",
    "        feature_names (list): List of feature names\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing feature standard deviations by cluster\n",
    "    \"\"\"\n",
    "    # Calculate standard deviations per feature per cluster\n",
    "    cluster_feature_stds = []\n",
    "    for i in range(optimal_clusters):\n",
    "        cluster_indices = np.where(cluster_labels == i)[0]\n",
    "        cluster_features = final_states_df.iloc[cluster_indices]\n",
    "        feature_stds = cluster_features.std()\n",
    "        cluster_feature_stds.append(feature_stds)\n",
    "\n",
    "    # Create a DataFrame with the feature STDs for each cluster\n",
    "    feature_std_df = pd.DataFrame([\n",
    "        {**{'Cluster': f'Cluster {i+1}', 'Size': len(np.where(cluster_labels == i)[0])}, \n",
    "         **{f_name: std for f_name, std in zip(feature_names[1:], cluster_feature_stds[i])}}\n",
    "        for i in range(optimal_clusters)\n",
    "    ])\n",
    "\n",
    "    # Create a heatmap of feature STDs per cluster\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(feature_std_df.set_index('Cluster')[feature_names[1:]], \n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cmap='viridis',\n",
    "                cbar_kws={'label': 'Standard Deviation'})\n",
    "    plt.title('Feature-wise Standard Deviations by Cluster')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Clusters')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a radar plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "    # Prepare the data for radar plot\n",
    "    angles = np.linspace(0, 2*np.pi, len(feature_names[1:]), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # complete the circle\n",
    "\n",
    "    for i in range(optimal_clusters):\n",
    "        values = feature_std_df.iloc[i][feature_names[1:]].values\n",
    "        values = np.concatenate((values, [values[0]]))  # complete the circle\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {i+1}')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(feature_names[1:], size=8)\n",
    "    ax.set_title('Feature Standard Deviations by Cluster (Radar Plot)')\n",
    "    plt.legend(bbox_to_anchor=(1.2, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a box plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    feature_std_melted = pd.melt(feature_std_df, \n",
    "                                id_vars=['Cluster', 'Size'], \n",
    "                                value_vars=feature_names[1:],\n",
    "                                var_name='Feature',\n",
    "                                value_name='Standard Deviation')\n",
    "\n",
    "    sns.boxplot(data=feature_std_melted, x='Feature', y='Standard Deviation', hue='Cluster')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Distribution of Feature Standard Deviations by Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return feature_std_df\n",
    "\n",
    "def plot_phase_portraits(trajectories, feature_names, n_trajectories=100, figsize=(30, 30), alpha=0.1):\n",
    "    \"\"\"\n",
    "    Create phase portraits for pairs of features from the trajectories.\n",
    "    \n",
    "    Args:\n",
    "        trajectories (np.ndarray): Array of shape (n_trajectories, timesteps, features)\n",
    "        feature_names (list): List of feature names\n",
    "        n_trajectories (int): Number of trajectories to plot\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "        alpha (float): Transparency of trajectory lines\n",
    "    \"\"\"\n",
    "    important_features = range(len(feature_names))\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for i, f1 in enumerate(important_features):\n",
    "        for j, f2 in enumerate(important_features):\n",
    "            if i >= j:\n",
    "                continue\n",
    "                \n",
    "            plt.subplot(len(important_features), len(important_features), i*len(important_features)+j+1)\n",
    "            for traj in trajectories[:n_trajectories]:\n",
    "                plt.plot(traj[:, f1+1], traj[:, f2+1], 'b-', alpha=alpha)  # +1 to skip timestamp\n",
    "                plt.plot(traj[0, f1+1], traj[0, f2+1], 'go', markersize=2)  # Start\n",
    "                plt.plot(traj[-1, f1+1], traj[-1, f2+1], 'ro', markersize=2)  # End\n",
    "                \n",
    "            plt.xlabel(f\"{feature_names[f1+1]}\")  # +1 to skip timestamp\n",
    "            plt.ylabel(f\"{feature_names[f2+1]}\")  # +1 to skip timestamp\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_multi_model_reward_distributions_with_ci_corrected(model_results_dict, save_path=None, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Plot reward distributions for multiple models with proper confidence intervals.\n",
    "    Shows mean KDE per method with confidence intervals for the MEAN VALUES, not density.\n",
    "    \n",
    "    Args:\n",
    "        model_results_dict (dict): Dictionary where keys are model names and values are \n",
    "                                 dictionaries with run results containing 'rewards' arrays\n",
    "        save_path (str, optional): Path to save the plot\n",
    "        confidence_level (float): Confidence level for intervals (default 0.95 for 95% CI)\n",
    "    \"\"\"\n",
    "    from scipy.stats import gaussian_kde, t\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create professional distribution plot\n",
    "    plt.figure(figsize=(14, 10), dpi=300)\n",
    "    \n",
    "    # Define color palette for different models\n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # First pass: collect data and determine global x range\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "                    # Update global range\n",
    "                    global_x_min = min(global_x_min, np.min(rewards))\n",
    "                    global_x_max = max(global_x_max, np.max(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means)\n",
    "            }\n",
    "    \n",
    "    # Add padding to global range\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    # Common x range for all KDE calculations\n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Second pass: create plots for each model\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        color = colors.get(model_name, 'gray')\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        overall_mean = np.mean(run_means)\n",
    "        overall_std = np.std(run_means, ddof=1)  # Sample standard deviation\n",
    "        n_runs = len(run_means)\n",
    "        \n",
    "        # Calculate confidence interval for the MEAN (not the density)\n",
    "        if n_runs > 1:\n",
    "            # Use t-distribution for small samples\n",
    "            t_critical = t.ppf((1 + confidence_level) / 2, df=n_runs-1)\n",
    "            margin_error = t_critical * (overall_std / np.sqrt(n_runs))\n",
    "            ci_lower = overall_mean - margin_error\n",
    "            ci_upper = overall_mean + margin_error\n",
    "        else:\n",
    "            ci_lower = ci_upper = overall_mean\n",
    "        \n",
    "        # Calculate mean KDE curve across all runs\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                mean_kde_curve = np.mean(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot mean KDE curve\n",
    "                plt.plot(common_x, mean_kde_curve, color=color, linewidth=3, \n",
    "                        label=f'{model_name.upper()}: ={overall_mean:.2f}{overall_std:.2f}')\n",
    "                \n",
    "                # Add confidence band around the KDE (this represents variability between runs)\n",
    "                std_kde_curve = np.std(kde_curves, axis=0)\n",
    "                plt.fill_between(common_x, \n",
    "                               np.maximum(mean_kde_curve - 0.5*std_kde_curve, 0), \n",
    "                               mean_kde_curve + 0.5*std_kde_curve,\n",
    "                               color=color, alpha=0.2, \n",
    "                               label=f'{model_name.upper()} KDE variability')\n",
    "        \n",
    "        # Add vertical lines for mean confidence interval\n",
    "        plt.axvline(overall_mean, color=color, linestyle='-', linewidth=2, alpha=0.8)\n",
    "        plt.axvline(ci_lower, color=color, linestyle=':', alpha=0.6, linewidth=1.5)\n",
    "        plt.axvline(ci_upper, color=color, linestyle=':', alpha=0.6, linewidth=1.5)\n",
    "        \n",
    "        # Add shaded area for mean confidence interval\n",
    "        plt.axvspan(ci_lower, ci_upper, color=color, alpha=0.1)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean,\n",
    "            'std': overall_std,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'n_runs': n_runs\n",
    "        }\n",
    "    \n",
    "    # Styling\n",
    "    plt.title(f'Model Comparison: Mean Distributions with {int(confidence_level*100)}% Confidence Intervals\\n'\n",
    "              f'Solid lines = Mean KDE, Dotted lines = CI bounds for means, Shaded = KDE variability', \n",
    "              fontsize=16, fontweight='bold', pad=25)\n",
    "    plt.xlabel('Final Reward Values', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Probability Density', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Professional styling\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.legend(fontsize=10, frameon=True, framealpha=0.9, loc='upper left')\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    stats_text = f\"{int(confidence_level*100)}% Confidence Intervals for Means:\\n\"\n",
    "    for model_name, stats in model_stats.items():\n",
    "        stats_text += f\"{model_name.upper()}: [{stats['ci_lower']:.2f}, {stats['ci_upper']:.2f}]\\n\"\n",
    "    \n",
    "    plt.text(0.98, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'),\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Much cleaner version - just show the mean KDE without confusing \"confidence bounds\"\n",
    "def plot_multi_model_clean_kde(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Clean version: just show mean KDE per method with proper statistical annotations.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8), dpi=300)\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # Collect data and determine global range\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        run_maxs = []\n",
    "        run_mins = []\n",
    "        run_stds = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    run_maxs.append(np.max(rewards))\n",
    "                    run_mins.append(np.min(rewards))\n",
    "                    run_stds.append(np.std(rewards, ddof=1))\n",
    "                    global_x_min = min(global_x_min, np.min(rewards))\n",
    "                    global_x_max = max(global_x_max, np.max(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means),\n",
    "                'run_maxs': np.array(run_maxs),\n",
    "                'run_mins': np.array(run_mins),\n",
    "                'run_stds': np.array(run_stds)\n",
    "            }\n",
    "    \n",
    "    # Add padding\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Plot each model\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        run_maxs = data['run_maxs']\n",
    "        run_mins = data['run_mins']\n",
    "        run_stds = data['run_stds']\n",
    "        \n",
    "        color = colors.get(model_name, 'gray')\n",
    "        overall_mean = np.mean(run_means)\n",
    "        overall_std = np.std(run_means, ddof=1)\n",
    "        \n",
    "        # Calculate mean KDE\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                mean_kde_curve = np.mean(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot clean KDE line\n",
    "                plt.plot(common_x, mean_kde_curve, color=color, linewidth=3,\n",
    "                    label=f'{model_name.upper()}: ={overall_mean:.2f}{overall_std:.2f}')\n",
    "                \n",
    "                \n",
    "                # Add mean line\n",
    "                plt.axvline(overall_mean, color=color, linestyle='--', \n",
    "                          linewidth=2, alpha=0.7)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean,\n",
    "            'std': overall_std,\n",
    "            'n_runs': len(run_means)\n",
    "        }\n",
    "    \n",
    "    plt.title('Model Performance Comparison: Average Reward Distributions',\n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Final Reward', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Probability Density', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12, frameon=True, framealpha=0.9)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, wilcoxon, kruskal, friedmanchisquare\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_run_means(model_results_dict):\n",
    "    \"\"\"\n",
    "    Extract mean rewards from each run for all models.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with model names as keys and arrays of run means as values\n",
    "    \"\"\"\n",
    "    run_means = {}\n",
    "    \n",
    "    for model_name, runs_dict in model_results_dict.items():\n",
    "        means = []\n",
    "        for run_idx in range(30):\n",
    "            rewards = runs_dict[run_idx]['rewards']\n",
    "            \n",
    "            # Handle different reward formats\n",
    "            if isinstance(rewards, (list, tuple)):\n",
    "                rewards = np.array(rewards, dtype=float)\n",
    "            elif isinstance(rewards, np.ndarray):\n",
    "                rewards = rewards.astype(float)\n",
    "            \n",
    "            # Extract final rewards\n",
    "            if rewards.ndim == 1:\n",
    "                final_rewards = rewards\n",
    "            elif rewards.ndim == 2:\n",
    "                final_rewards = rewards[:, -1]\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            # Remove invalid values\n",
    "            final_rewards = final_rewards[np.isfinite(final_rewards)]\n",
    "            if len(final_rewards) > 0:\n",
    "                means.append(np.mean(final_rewards))\n",
    "        \n",
    "        run_means[model_name] = np.array(means)\n",
    "    \n",
    "    return run_means\n",
    "\n",
    "def perform_pairwise_tests(run_means, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform pairwise statistical tests between all method pairs.\n",
    "    \n",
    "    Args:\n",
    "        run_means (dict): Dictionary with model names and their run means\n",
    "        alpha (float): Significance level\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results of all pairwise comparisons\n",
    "    \"\"\"\n",
    "    models = list(run_means.keys())\n",
    "    n_models = len(models)\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'mann_whitney': {},\n",
    "        'welch_t_test': {},\n",
    "        'effect_sizes': {},\n",
    "        'summary': []\n",
    "    }\n",
    "    \n",
    "    print(\"PAIRWISE STATISTICAL TESTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, model1 in enumerate(models):\n",
    "        for j, model2 in enumerate(models):\n",
    "            if i >= j:  # Avoid duplicate comparisons\n",
    "                continue\n",
    "                \n",
    "            data1 = run_means[model1]\n",
    "            data2 = run_means[model2]\n",
    "            \n",
    "            # Mann-Whitney U test (non-parametric)\n",
    "            mw_stat, mw_p = mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "            \n",
    "            # Welch's t-test (assumes unequal variances)\n",
    "            welch_stat, welch_p = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt((np.var(data1, ddof=1) + np.var(data2, ddof=1)) / 2)\n",
    "            cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std\n",
    "            \n",
    "            # Cliff's delta (non-parametric effect size)\n",
    "            def cliffs_delta(x, y):\n",
    "                n1, n2 = len(x), len(y)\n",
    "                delta = 0\n",
    "                for i in x:\n",
    "                    for j in y:\n",
    "                        if i > j:\n",
    "                            delta += 1\n",
    "                        elif i < j:\n",
    "                            delta -= 1\n",
    "                return delta / (n1 * n2)\n",
    "            \n",
    "            cliff_delta = cliffs_delta(data1, data2)\n",
    "            \n",
    "            # Store results\n",
    "            pair_key = f\"{model1}_vs_{model2}\"\n",
    "            results['mann_whitney'][pair_key] = {'statistic': mw_stat, 'p_value': mw_p}\n",
    "            results['welch_t_test'][pair_key] = {'statistic': welch_stat, 'p_value': welch_p}\n",
    "            results['effect_sizes'][pair_key] = {'cohens_d': cohens_d, 'cliffs_delta': cliff_delta}\n",
    "            \n",
    "            # Summary\n",
    "            summary_row = {\n",
    "                'Model_1': model1,\n",
    "                'Model_2': model2,\n",
    "                'Mean_1': np.mean(data1),\n",
    "                'Mean_2': np.mean(data2),\n",
    "                'Std_1': np.std(data1, ddof=1),\n",
    "                'Std_2': np.std(data2, ddof=1),\n",
    "                'Mann_Whitney_p': mw_p,\n",
    "                'Welch_t_p': welch_p,\n",
    "                'Cohens_d': cohens_d,\n",
    "                'Cliffs_delta': cliff_delta,\n",
    "                'MW_significant': mw_p < alpha,\n",
    "                'Welch_significant': welch_p < alpha\n",
    "            }\n",
    "            results['summary'].append(summary_row)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\n{model1.upper()} vs {model2.upper()}:\")\n",
    "            print(f\"  Means: {np.mean(data1):.3f}  {np.std(data1, ddof=1):.3f} vs {np.mean(data2):.3f}  {np.std(data2, ddof=1):.3f}\")\n",
    "            print(f\"  Mann-Whitney U: p = {mw_p:.6f} {'***' if mw_p < 0.001 else '**' if mw_p < 0.01 else '*' if mw_p < 0.05 else 'ns'}\")\n",
    "            print(f\"  Welch's t-test: p = {welch_p:.6f} {'***' if welch_p < 0.001 else '**' if welch_p < 0.01 else '*' if welch_p < 0.05 else 'ns'}\")\n",
    "            print(f\"  Effect sizes: Cohen's d = {cohens_d:.3f}, Cliff's  = {cliff_delta:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def perform_omnibus_tests(run_means):\n",
    "    \"\"\"\n",
    "    Perform omnibus tests to check if there are any significant differences between groups.\n",
    "    \"\"\"\n",
    "    models = list(run_means.keys())\n",
    "    data_arrays = [run_means[model] for model in models]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OMNIBUS TESTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Kruskal-Wallis test (non-parametric ANOVA)\n",
    "    kw_stat, kw_p = kruskal(*data_arrays)\n",
    "    print(f\"Kruskal-Wallis test: H = {kw_stat:.3f}, p = {kw_p:.6f}\")\n",
    "    print(f\"Interpretation: {'Significant differences exist between groups' if kw_p < 0.05 else 'No significant differences between groups'}\")\n",
    "    \n",
    "    # One-way ANOVA (parametric)\n",
    "    f_stat, f_p = stats.f_oneway(*data_arrays)\n",
    "    print(f\"One-way ANOVA: F = {f_stat:.3f}, p = {f_p:.6f}\")\n",
    "    print(f\"Interpretation: {'Significant differences exist between groups' if f_p < 0.05 else 'No significant differences between groups'}\")\n",
    "    \n",
    "    return {'kruskal_wallis': (kw_stat, kw_p), 'anova': (f_stat, f_p)}\n",
    "\n",
    "def bonferroni_correction(results, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction for multiple comparisons.\n",
    "    \"\"\"\n",
    "    summary_df = pd.DataFrame(results['summary'])\n",
    "    n_comparisons = len(summary_df)\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"BONFERRONI CORRECTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Original  = {alpha}\")\n",
    "    print(f\"Number of comparisons = {n_comparisons}\")\n",
    "    print(f\"Bonferroni-corrected  = {corrected_alpha:.6f}\")\n",
    "    \n",
    "    # Apply correction\n",
    "    summary_df['MW_significant_bonferroni'] = summary_df['Mann_Whitney_p'] < corrected_alpha\n",
    "    summary_df['Welch_significant_bonferroni'] = summary_df['Welch_t_p'] < corrected_alpha\n",
    "    \n",
    "    print(f\"\\nSignificant comparisons after Bonferroni correction:\")\n",
    "    significant_mw = summary_df[summary_df['MW_significant_bonferroni']]\n",
    "    significant_welch = summary_df[summary_df['Welch_significant_bonferroni']]\n",
    "    \n",
    "    print(f\"Mann-Whitney U test: {len(significant_mw)}/{n_comparisons} significant\")\n",
    "    print(f\"Welch's t-test: {len(significant_welch)}/{n_comparisons} significant\")\n",
    "    \n",
    "    if len(significant_mw) > 0:\n",
    "        print(\"\\nSignificant Mann-Whitney comparisons (Bonferroni-corrected):\")\n",
    "        for _, row in significant_mw.iterrows():\n",
    "            print(f\"  {row['Model_1']} vs {row['Model_2']}: p = {row['Mann_Whitney_p']:.6f}\")\n",
    "    \n",
    "    return summary_df, corrected_alpha\n",
    "\n",
    "def create_statistical_plots(run_means, results, save_path=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive statistical visualization plots.\n",
    "    \"\"\"\n",
    "    summary_df = pd.DataFrame(results['summary'])\n",
    "    \n",
    "    # Create a 2x2 subplot figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=300)\n",
    "    \n",
    "    # Plot 1: Box plot comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    data_for_box = []\n",
    "    labels_for_box = []\n",
    "    colors = ['#2ecc71', '#3498db', '#f39c12', '#9b59b6', '#e74c3c']\n",
    "    \n",
    "    for i, (model, means) in enumerate(run_means.items()):\n",
    "        data_for_box.append(means)\n",
    "        labels_for_box.append(model.upper())\n",
    "    \n",
    "    bp = ax1.boxplot(data_for_box, labels=labels_for_box, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax1.set_title('Distribution of Run Means by Method', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Mean Final Reward', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: P-value heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    models = list(run_means.keys())\n",
    "    n_models = len(models)\n",
    "    p_matrix = np.ones((n_models, n_models))\n",
    "    \n",
    "    for i, model1 in enumerate(models):\n",
    "        for j, model2 in enumerate(models):\n",
    "            if i != j:\n",
    "                pair_key = f\"{model1}_vs_{model2}\" if i < j else f\"{model2}_vs_{model1}\"\n",
    "                if pair_key in results['mann_whitney']:\n",
    "                    p_val = results['mann_whitney'][pair_key]['p_value']\n",
    "                    p_matrix[i, j] = p_val\n",
    "    \n",
    "    # Use -log10(p) for better visualization\n",
    "    log_p_matrix = -np.log10(p_matrix + 1e-10)  # Add small value to avoid log(0)\n",
    "    \n",
    "    im = ax2.imshow(log_p_matrix, cmap='Reds', aspect='auto')\n",
    "    ax2.set_xticks(range(n_models))\n",
    "    ax2.set_yticks(range(n_models))\n",
    "    ax2.set_xticklabels([m.upper() for m in models], rotation=45)\n",
    "    ax2.set_yticklabels([m.upper() for m in models])\n",
    "    ax2.set_title('-log(p-value) Heatmap\\n(Mann-Whitney U Test)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2)\n",
    "    cbar.set_label('-log(p-value)', fontsize=10)\n",
    "    \n",
    "    # Add significance thresholds\n",
    "    ax2.axhline(y=-0.5, color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=-0.5, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 3: Effect sizes\n",
    "    ax3 = axes[1, 0]\n",
    "    cohens_d_values = [results['effect_sizes'][pair]['cohens_d'] for pair in results['effect_sizes']]\n",
    "    pair_labels = [pair.replace('_vs_', ' vs ').replace('_', ' ').title() for pair in results['effect_sizes']]\n",
    "    \n",
    "    colors_effect = ['red' if abs(d) > 0.8 else 'orange' if abs(d) > 0.5 else 'green' for d in cohens_d_values]\n",
    "    \n",
    "    bars = ax3.barh(range(len(cohens_d_values)), cohens_d_values, color=colors_effect, alpha=0.7)\n",
    "    ax3.set_yticks(range(len(pair_labels)))\n",
    "    ax3.set_yticklabels(pair_labels, fontsize=10)\n",
    "    ax3.set_xlabel(\"Cohen's d\", fontsize=12)\n",
    "    ax3.set_title(\"Effect Sizes (Cohen's d)\", fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='x')\n",
    "    ax3.axvline(x=0, color='black', linestyle='-', alpha=0.8)\n",
    "    ax3.axvline(x=0.2, color='gray', linestyle='--', alpha=0.5, label='Small effect')\n",
    "    ax3.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Medium effect')\n",
    "    ax3.axvline(x=0.8, color='gray', linestyle='--', alpha=0.5, label='Large effect')\n",
    "    ax3.axvline(x=-0.2, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax3.axvline(x=-0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax3.axvline(x=-0.8, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Plot 4: Statistical power analysis\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create significance summary\n",
    "    sig_summary = {}\n",
    "    for model in models:\n",
    "        sig_count = sum(1 for row in results['summary'] \n",
    "                       if (row['Model_1'] == model or row['Model_2'] == model) \n",
    "                       and row['MW_significant'])\n",
    "        sig_summary[model] = sig_count\n",
    "    \n",
    "    model_names = list(sig_summary.keys())\n",
    "    sig_counts = list(sig_summary.values())\n",
    "    \n",
    "    bars = ax4.bar(model_names, sig_counts, color=colors, alpha=0.7)\n",
    "    ax4.set_title('Number of Significant Pairwise Comparisons\\n(Mann-Whitney U, p < 0.05)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Count of Significant Comparisons', fontsize=12)\n",
    "    ax4.set_xlabel('Methods', fontsize=12)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, sig_counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_ylim(0, max(sig_counts) * 1.2)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Statistical Analysis: Method Comparison Results', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_statistical_report(run_means, results, corrected_alpha, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive statistical report.\n",
    "    \"\"\"\n",
    "    summary_df = pd.DataFrame(results['summary'])\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"COMPREHENSIVE STATISTICAL ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Number of methods compared: {len(run_means)}\")\n",
    "    report.append(f\"Number of independent runs per method: 30\")\n",
    "    report.append(f\"Total pairwise comparisons: {len(summary_df)}\")\n",
    "    report.append(f\"Significance level (): 0.05\")\n",
    "    report.append(f\"Bonferroni-corrected : {corrected_alpha:.6f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Descriptive statistics\n",
    "    report.append(\"DESCRIPTIVE STATISTICS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    for model, means in run_means.items():\n",
    "        report.append(f\"{model.upper()}:\")\n",
    "        report.append(f\"  Mean  SD: {np.mean(means):.3f}  {np.std(means, ddof=1):.3f}\")\n",
    "        report.append(f\"  Median [IQR]: {np.median(means):.3f} [{np.percentile(means, 25):.3f}, {np.percentile(means, 75):.3f}]\")\n",
    "        report.append(f\"  Range: [{np.min(means):.3f}, {np.max(means):.3f}]\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Omnibus test results\n",
    "    omnibus_results = perform_omnibus_tests(run_means)\n",
    "    report.append(\"OMNIBUS TEST RESULTS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(f\"Kruskal-Wallis H = {omnibus_results['kruskal_wallis'][0]:.3f}, p = {omnibus_results['kruskal_wallis'][1]:.6f}\")\n",
    "    report.append(f\"One-way ANOVA F = {omnibus_results['anova'][0]:.3f}, p = {omnibus_results['anova'][1]:.6f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Significant comparisons\n",
    "    significant_mw = summary_df[summary_df['MW_significant']]\n",
    "    significant_bonf = summary_df[summary_df['MW_significant_bonferroni']]\n",
    "    \n",
    "    report.append(\"SIGNIFICANT PAIRWISE COMPARISONS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(f\"Without correction: {len(significant_mw)}/{len(summary_df)} significant\")\n",
    "    report.append(f\"With Bonferroni correction: {len(significant_bonf)}/{len(summary_df)} significant\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    if len(significant_bonf) > 0:\n",
    "        report.append(\"Bonferroni-corrected significant comparisons:\")\n",
    "        for _, row in significant_bonf.iterrows():\n",
    "            report.append(f\"  {row['Model_1']} vs {row['Model_2']}: p = {row['Mann_Whitney_p']:.6f}, d = {row['Cohens_d']:.3f}\")\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    report.append(\"\")\n",
    "    report.append(\"EFFECT SIZE INTERPRETATION\")\n",
    "    report.append(\"-\" * 40)\n",
    "    report.append(\"Cohen's d: 0.2 (small), 0.5 (medium), 0.8 (large)\")\n",
    "    report.append(\"Cliff's : 0.11 (small), 0.28 (medium), 0.43 (large)\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    large_effects = summary_df[abs(summary_df['Cohens_d']) > 0.8]\n",
    "    medium_effects = summary_df[(abs(summary_df['Cohens_d']) > 0.5) & (abs(summary_df['Cohens_d']) <= 0.8)]\n",
    "    \n",
    "    report.append(f\"Large effect sizes (|d| > 0.8): {len(large_effects)}\")\n",
    "    report.append(f\"Medium effect sizes (0.5 < |d|  0.8): {len(medium_effects)}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"\")\n",
    "    report.append(\"STATISTICAL CONCLUSIONS\")\n",
    "    report.append(\"-\" * 40)\n",
    "    \n",
    "    if omnibus_results['kruskal_wallis'][1] < 0.05:\n",
    "        report.append(\" Omnibus tests confirm significant differences exist between methods\")\n",
    "    else:\n",
    "        report.append(\" Omnibus tests do not detect significant differences between methods\")\n",
    "    \n",
    "    if len(significant_bonf) > 0:\n",
    "        report.append(f\" {len(significant_bonf)} pairwise comparisons remain significant after Bonferroni correction\")\n",
    "        best_method = significant_bonf.loc[significant_bonf['Mean_1'].idxmax(), 'Model_1'] if len(significant_bonf) > 0 else \"N/A\"\n",
    "        report.append(f\" Recommended method based on statistical analysis: {best_method}\")\n",
    "    else:\n",
    "        report.append(\" No pairwise comparisons remain significant after Bonferroni correction\")\n",
    "    \n",
    "    # Write report to file if path provided\n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write('\\n'.join(report))\n",
    "        print(f\"Statistical report saved to: {save_path}\")\n",
    "    \n",
    "    # Print report\n",
    "    print('\\n'.join(report))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Main execution\n",
    "def perform_comprehensive_statistical_analysis(model_results_dict, save_dir=None):\n",
    "    \"\"\"\n",
    "    Perform comprehensive statistical analysis on model comparison results.\n",
    "    \"\"\"\n",
    "    print(\"COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Extracting run means from model results...\")\n",
    "    \n",
    "    # Extract run means\n",
    "    run_means = extract_run_means(model_results_dict)\n",
    "    \n",
    "    # Perform pairwise tests\n",
    "    results = perform_pairwise_tests(run_means)\n",
    "    \n",
    "    # Apply Bonferroni correction\n",
    "    summary_df, corrected_alpha = bonferroni_correction(results)\n",
    "    \n",
    "    # Create plots\n",
    "    if save_dir:\n",
    "        plot_path = f\"{save_dir}/statistical_analysis_plots.pdf\"\n",
    "        create_statistical_plots(run_means, results, save_path=plot_path)\n",
    "    else:\n",
    "        create_statistical_plots(run_means, results)\n",
    "    \n",
    "    # Generate report\n",
    "    if save_dir:\n",
    "        report_path = f\"{save_dir}/statistical_analysis_report.txt\"\n",
    "        final_summary = generate_statistical_report(run_means, results, corrected_alpha, save_path=report_path)\n",
    "    else:\n",
    "        final_summary = generate_statistical_report(run_means, results, corrected_alpha)\n",
    "    \n",
    "    # Save summary DataFrame\n",
    "    if save_dir:\n",
    "        csv_path = f\"{save_dir}/pairwise_comparison_results.csv\"\n",
    "        summary_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Detailed results saved to: {csv_path}\")\n",
    "    \n",
    "    return summary_df, results, run_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_1 = joblib.load(r\"results/reinforce_results_runs_oracle_1.joblib\")\n",
    "gflow_1 = joblib.load(r\"results/results_run_oracle_1.joblib\")\n",
    "reinforce_baseline_1 = joblib.load(r\"results/reinforce_results_runs_oracle_1.joblib\")\n",
    "sac_1 = joblib.load(r\"results/sac_results_runs_oracle_1.joblib\")\n",
    "reinforce_2 = joblib.load(r\"results/reinforce_results_runs_oracle_2.joblib\")\n",
    "gflow_2 = joblib.load(r\"results/results_run_oracle_2.joblib\")\n",
    "sac_2 = joblib.load(r\"results/sac_results_runs_oracle_2.joblib\")\n",
    "reinforce_baseline_2 = joblib.load(r\"results/reinforce_results_runs_oracle_2.joblib\")\n",
    "reinforce_3 = joblib.load(r\"results/reinforce_results_runs_oracle_3.joblib\")\n",
    "gflow_3 = joblib.load(r\"results/results_run_oracle_3.joblib\")\n",
    "sac_3 = joblib.load(r\"results/sac_results_runs_oracle_3.joblib\")\n",
    "reinforce_baseline_3 = joblib.load(r\"results/reinforce_results_runs_oracle_3.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-reward trajectories\n",
    "# Define the environment\n",
    "results_dict = {}\n",
    "\n",
    "for run in tqdm(range(30)):\n",
    "    CONFIG_PATH = r'configs/run_params.yaml'\n",
    "    MODEL_PATH = r\"oracles/oracle_3.joblib\"\n",
    "    sys.path.append('../.')\n",
    "    config, env, logger, training_parameters, initial_state, feature_names = setup_environment_and_logger(CONFIG_PATH, MODEL_PATH)\n",
    "    # Correct state_dim to exclude the time-step part for action selection\n",
    "    state_dim = len(initial_state) + 1   # Include time-step part for the environment\n",
    "    action_dim = len(initial_state)     # Exclude time-step part for actions\n",
    "    trained_agent = sac_3[run]['trained_agent']\n",
    "    seed = sac_3[run]['seed']\n",
    "    seed_all(seed)\n",
    "\n",
    "    high_reward_trajectories, rewards, all_actions, f_names = generate_high_reward_trajectories(\n",
    "            env,\n",
    "            trained_agent=trained_agent,\n",
    "            num_trajectories=200,\n",
    "            max_steps=training_parameters[\"trajectory_length\"],\n",
    "        )\n",
    "    \n",
    "    results_dict[run] = {\n",
    "        'trajectories': high_reward_trajectories,\n",
    "        'rewards': rewards,\n",
    "        'all_actions': all_actions,\n",
    "        'seed': seed\n",
    "    }\n",
    "    print(f\"Run {run+1} completed with seed {seed} - mean reward: {np.mean(rewards)}.\")\n",
    "    \n",
    "joblib.dump(results_dict, r'oracles/sac_results_runs_oracle_3.joblib')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import seed_all\n",
    "results_dict = {}\n",
    "for i in range(30):\n",
    "    fwd_model_path, bwd_model_path = gflow_3[0]['fwd_model_path'], gflow_3[0]['bwd_model_path']\n",
    "    seed = gflow_3[i]['seed']\n",
    "    print(f\"Running simulation for seed {seed}...\")\n",
    "    seed_all(seed)\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load config\n",
    "    config = yaml.safe_load(open(r'config/run_params.yaml', 'r'))\n",
    "    trajectory_length = 12\n",
    "    n_trajectories = 200\n",
    "\n",
    "    # Load the trained models\n",
    "    forward_model = load_entire_model(fwd_model_path, device)\n",
    "    backward_model = load_entire_model(bwd_model_path, device)\n",
    "    logger, log_file_name = setup_logger('experiment')\n",
    "    top_50_cases_colors = sns.color_palette(\"husl\", 50)\n",
    "    initial_state,feature_names = load_initial_state(config)\n",
    "    input_dim = 7\n",
    "    mixture_components = 15  # Number of mixture components for the beta distribution\n",
    "    extra_parameters={\n",
    "                'mixture_components': mixture_components,\n",
    "                'num_variables': input_dim - 1,\n",
    "                    }\n",
    "    model_path = fr\"{config['oracle']['model_path']}\"\n",
    "\n",
    "    trajectories, rewards,all_actions,_ = simulate_trajectories(\n",
    "        env_class=GeneralEnvironment,\n",
    "        forward_model=forward_model,\n",
    "        backward_model=backward_model,\n",
    "        initial_state=initial_state,\n",
    "        config=config,\n",
    "        trajectory_length=trajectory_length,\n",
    "        n_trajectories=n_trajectories,\n",
    "        device=device,\n",
    "        logger = logger,\n",
    "        model_path = model_path,\n",
    "        distribution='mixture_beta',\n",
    "        extra_parameters=extra_parameters\n",
    "\n",
    "    )\n",
    "    results_dict[i] = {\n",
    "        'trajectories': trajectories,\n",
    "        'rewards': np.array(rewards),\n",
    "        'all_actions': all_actions,\n",
    "        'seed': seed\n",
    "    }\n",
    "joblib.dump(results_dict, r'results/gflow_results_runs_oracle_3.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smcmc_1[0]['results_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle 1\n",
    "res_g1 = joblib.load(r\"results/gflow_results_runs_oracle_1.joblib\")\n",
    "res_reinforce_1 = joblib.load(r\"results/reinforce_results_runs_oracle_1.joblib\")\n",
    "res_reinforce_baseline_1 = joblib.load(r\"results/reinforce_baseline_results_runs_oracle_1.joblib\")\n",
    "res_sac_1 = joblib.load(r\"results/sac_results_runs_oracle_1.joblib\")\n",
    "smcmc_1 = joblib.load(r\"results/PaperPlots_O1_SMCMC/mcmc_results_runs_oracle_1.joblib\")\n",
    "\n",
    "new_smcmc_1 = {}\n",
    "for key,val in smcmc_1.items():\n",
    "    new_smcmc_1[key] = {\n",
    "        'rewards': val['results_dict']['rewards'],\n",
    "        'trajectories': val['results_dict']['trajectories'],\n",
    "        'all_actions': val['results_dict']['actions'],\n",
    "        'seed': val['seed']\n",
    "    }\n",
    "\n",
    "model_results_dict_1 = {\n",
    "    'gflow': res_g1,\n",
    "    'reinforce': res_reinforce_1,\n",
    "    'reinforce_baseline': res_reinforce_baseline_1,\n",
    "    'sac': res_sac_1,\n",
    "    'smcmc': new_smcmc_1\n",
    "}\n",
    "\n",
    "# Oracle 2\n",
    "res_g2 = joblib.load(r\"results/gflow_results_runs_oracle_2.joblib\")\n",
    "res_reinforce_2 = joblib.load(r\"results/reinforce_results_runs_oracle_2.joblib\")\n",
    "res_reinforce_baseline_2 = joblib.load(r\"results/reinforce_baseline_results_runs_oracle_2.joblib\")\n",
    "res_sac_2 = joblib.load(r\"results/sac_results_runs_oracle_2.joblib\")\n",
    "smcmc_2 = joblib.load(r\"results/PaperPlots_O2_SMCMC/mcmc_results_runs_oracle_2.joblib\")\n",
    "\n",
    "new_smcmc_2 = {}\n",
    "for key,val in smcmc_2.items():\n",
    "    new_smcmc_2[key] = {\n",
    "        'rewards': val['results_dict']['rewards'],\n",
    "        'trajectories': val['results_dict']['trajectories'],\n",
    "        'all_actions': val['results_dict']['actions'],\n",
    "        'seed': val['seed']\n",
    "    }\n",
    "model_results_dict_2 = {\n",
    "    'gflow': res_g2,\n",
    "    'reinforce': res_reinforce_2,\n",
    "    'reinforce_baseline': res_reinforce_baseline_2,\n",
    "    'sac': res_sac_2,\n",
    "    'smcmc': new_smcmc_2\n",
    "}\n",
    "\n",
    "# Oracle 3\n",
    "res_g3 = joblib.load(r\"results/gflow_results_runs_oracle_3.joblib\")\n",
    "res_reinforce_3 = joblib.load(r\"results/reinforce_results_runs_oracle_3.joblib\")\n",
    "res_reinforce_baseline_3 = joblib.load(r\"results/reinforce_baseline_results_runs_oracle_3.joblib\")\n",
    "res_sac_3 = joblib.load(r\"results/sac_results_runs_oracle_3.joblib\")\n",
    "smcmc_3 = joblib.load(r\"results/PaperPlots_O3_SMCMC/mcmc_results_runs_oracle_3.joblib\")\n",
    "\n",
    "new_smcmc_3 = {}\n",
    "for key,val in smcmc_3.items():\n",
    "    new_smcmc_3[key] = {\n",
    "        'rewards': val['results_dict']['rewards'],\n",
    "        'trajectories': val['results_dict']['trajectories'],\n",
    "        'all_actions': val['results_dict']['actions'],\n",
    "        'seed': val['seed']\n",
    "    }\n",
    "\n",
    "model_results_dict_3 = {\n",
    "    'gflow': res_g3,\n",
    "    'reinforce': res_reinforce_3,\n",
    "    'reinforce_baseline': res_reinforce_baseline_3,\n",
    "    'sac': res_sac_3,\n",
    "    'smcmc': new_smcmc_3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_g1[0]['rewards'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_reward_distributions(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot reward distributions for multiple models across multiple runs.\n",
    "    \n",
    "    Args:\n",
    "        model_results_dict (dict): Dictionary where keys are model names and values are \n",
    "                                 dictionaries with run results containing 'rewards' arrays\n",
    "        save_path (str, optional): Path to save the plot\n",
    "    \"\"\"\n",
    "    from scipy.stats import gaussian_kde\n",
    "    \n",
    "    # Create professional distribution plot\n",
    "    plt.figure(figsize=(14, 7), dpi=300)\n",
    "    \n",
    "    # Define color palette for different models\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(model_results_dict)))\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    for model_idx, (model_name, results_dict) in enumerate(model_results_dict.items()):\n",
    "        # Collect all rewards from all runs for this model\n",
    "        all_rewards = []\n",
    "        for run_idx in range(len(results_dict)):\n",
    "            if results_dict[run_idx]['rewards'].ndim == 1:\n",
    "                # If rewards are 1D, reshape to 2D with one column\n",
    "                rewards = results_dict[run_idx]['rewards']\n",
    "            else:\n",
    "                # If rewards are already 2D, take the last column\n",
    "                rewards = results_dict[run_idx]['rewards'][:, -1]\n",
    "            all_rewards.append(rewards)\n",
    "        \n",
    "        # Plot each run's KDE distribution\n",
    "        for run_idx, rewards in enumerate(all_rewards):\n",
    "            sns.kdeplot(data=rewards, \n",
    "                        color=colors[model_idx], \n",
    "                        alpha=0.5, \n",
    "                        linewidth=2)\n",
    "        \n",
    "        # Calculate overall statistics for this model\n",
    "        all_values = np.concatenate(all_rewards)\n",
    "        overall_mean = np.mean(all_values)\n",
    "        overall_std = np.std(all_values)\n",
    "        \n",
    "        # Store stats for legend\n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean,\n",
    "            'std': overall_std,\n",
    "            'color': colors[model_idx],\n",
    "            'total_trajectories': sum(len(rewards) for rewards in all_rewards)\n",
    "        }\n",
    "        \n",
    "        # Add mean line for this model\n",
    "        plt.axvline(overall_mean, color=colors[model_idx], linestyle='--', \n",
    "                   linewidth=3, alpha=0.8)\n",
    "    \n",
    "    # Create custom legend\n",
    "    legend_elements = []\n",
    "    for model_name, stats in model_stats.items():\n",
    "        legend_elements.append(\n",
    "            plt.Line2D([0], [0], color=stats['color'], lw=3, alpha=0.8,\n",
    "                      label=f'{model_name}: ={stats[\"mean\"]:.2f}{stats[\"std\"]:.2f}')\n",
    "        )\n",
    "    \n",
    "    # Styling\n",
    "    plt.title('Model Comparison: Reward Distributions Across 30 Independent Runs', \n",
    "              fontsize=20, fontweight='bold', pad=25)\n",
    "    plt.xlabel('Final Reward Values', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Density', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Professional styling\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.legend(handles=legend_elements, fontsize=14, frameon=True, \n",
    "              framealpha=0.9, loc='upper right', fancybox=True, shadow=True)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Increase tick label size\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON SUMMARY (30 independent runs each)\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, stats in model_stats.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.3f}  {stats['std']:.3f}\")\n",
    "        print(f\"  Total Trajectories: {stats['total_trajectories']}\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "\n",
    "stats = plot_multi_model_reward_distributions(\n",
    "    model_results_dict_1, \n",
    "    save_path='multi_model_reward_distributions.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_small_multiples(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Small multiples showing each method's run variability with mean distributions and confidence intervals.\n",
    "    All subplots share the same scale for easy comparison.\n",
    "    \"\"\"\n",
    "    n_models = len(model_results_dict)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(4*n_models, 6), dpi=300, \n",
    "                            sharey=True)  # Share y-axis for comparison\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # First pass: collect all data to determine global x and y ranges\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    global_y_max = 0\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            # Update global ranges\n",
    "            model_x_min = min(np.min(rewards) for rewards in all_rewards)\n",
    "            model_x_max = max(np.max(rewards) for rewards in all_rewards)\n",
    "            global_x_min = min(global_x_min, model_x_min)\n",
    "            global_x_max = max(global_x_max, model_x_max)\n",
    "            \n",
    "            # Store processed data for second pass\n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means)\n",
    "            }\n",
    "    \n",
    "    # Add some padding to the global range\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    # Common x range for all KDE calculations\n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Second pass: create plots with consistent scales\n",
    "    for idx, (model_name, data) in enumerate(all_model_data.items()):\n",
    "        ax = axes[idx] if n_models > 1 else axes\n",
    "        \n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        consistency = 1 / (1 + np.std(run_means) / np.mean(run_means))\n",
    "        \n",
    "        # Plot styling based on consistency\n",
    "        if consistency > 0.8:\n",
    "            alpha_individual = 0.3\n",
    "            linewidth_individual = 1.5\n",
    "            consistency_label = \"HIGH\"\n",
    "            border_color = 'green'\n",
    "        elif consistency > 0.6:\n",
    "            alpha_individual = 0.25\n",
    "            linewidth_individual = 1.2\n",
    "            consistency_label = \"MEDIUM\"\n",
    "            border_color = 'orange'\n",
    "        else:\n",
    "            alpha_individual = 0.2\n",
    "            linewidth_individual = 1.0\n",
    "            consistency_label = \"LOW\"\n",
    "            border_color = 'red'\n",
    "        \n",
    "        # Plot individual runs using common x range\n",
    "        for rewards in all_rewards:\n",
    "            if len(rewards) > 1:\n",
    "                try:\n",
    "                    from scipy.stats import gaussian_kde\n",
    "                    kde = gaussian_kde(rewards)\n",
    "                    density = kde(common_x)\n",
    "                    ax.plot(common_x, density, color=colors.get(model_name, 'gray'), \n",
    "                           alpha=alpha_individual, linewidth=linewidth_individual)\n",
    "                    global_y_max = max(global_y_max, np.max(density))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Calculate and plot mean distribution with confidence intervals\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                \n",
    "                # Calculate mean and confidence bounds\n",
    "                mean_curve = np.mean(kde_curves, axis=0)\n",
    "                std_curve = np.std(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot mean distribution (thicker line)\n",
    "                ax.plot(common_x, mean_curve, \n",
    "                       color=colors.get(model_name, 'gray'), \n",
    "                       linewidth=4, alpha=0.9)\n",
    "                \n",
    "                # Add confidence intervals\n",
    "                ax.fill_between(common_x, \n",
    "                               mean_curve - std_curve, \n",
    "                               mean_curve + std_curve,\n",
    "                               color=colors.get(model_name, 'gray'), \n",
    "                               alpha=0.15)\n",
    "                \n",
    "                ax.fill_between(common_x, \n",
    "                               mean_curve - 0.5*std_curve, \n",
    "                               mean_curve + 0.5*std_curve,\n",
    "                               color=colors.get(model_name, 'gray'), \n",
    "                               alpha=0.25)\n",
    "        \n",
    "        # Add overall mean line\n",
    "        overall_mean = np.mean(np.concatenate(all_rewards))\n",
    "        ax.axvline(overall_mean, color='black', linestyle='--', linewidth=3, alpha=0.8)\n",
    "        \n",
    "        # Set consistent x-axis limits for all subplots\n",
    "        ax.set_xlim(global_x_min, global_x_max)\n",
    "        \n",
    "        # Styling for each subplot\n",
    "        ax.set_title(f'{model_name.upper()}\\nConsistency: {consistency_label}', \n",
    "                    fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Final Reward', fontsize=12)\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Density', fontsize=12)\n",
    "        \n",
    "        # Add colored border to indicate consistency\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(border_color)\n",
    "            spine.set_linewidth(3)\n",
    "        \n",
    "        # Add stats text\n",
    "        stats_text = f'={overall_mean:.1f}\\n_runs={np.std(run_means):.2f}\\nCV={np.std(run_means)/np.mean(run_means):.3f}'\n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                fontsize=10)\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean, \n",
    "            'consistency': consistency,\n",
    "            'run_std': np.std(run_means),\n",
    "            'cv': np.std(run_means)/np.mean(run_means),\n",
    "            'n_runs': len(run_means)\n",
    "        }\n",
    "    \n",
    "    # Set consistent y-axis limits for all subplots\n",
    "    y_padding = global_y_max * 0.05\n",
    "    for ax in (axes if n_models > 1 else [axes]):\n",
    "        ax.set_ylim(0, global_y_max + y_padding)\n",
    "    \n",
    "    plt.suptitle('Model Consistency Analysis: Individual Runs + Mean Distribution with Confidence Intervals\\n'\n",
    "                 'Border Color: Green=Consistent, Orange=Moderate, Red=Variable', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL CONSISTENCY SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    sorted_models = sorted(model_stats.items(), key=lambda x: x[1]['consistency'], reverse=True)\n",
    "    for rank, (model_name, stats) in enumerate(sorted_models, 1):\n",
    "        print(f\"\\n{rank}. {model_name.upper()}:\")\n",
    "        print(f\"   Consistency Score: {stats['consistency']:.3f}\")\n",
    "        print(f\"   Mean Reward: {stats['mean']:.2f}\")\n",
    "        print(f\"   Run Std Dev: {stats['run_std']:.2f}\")\n",
    "        print(f\"   Coefficient of Variation: {stats['cv']:.3f}\")\n",
    "        print(f\"   Number of Runs: {stats['n_runs']}\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Enhanced version with shared scales\n",
    "def plot_multi_model_small_multiples_enhanced(model_results_dict, save_path=None, show_percentiles=True):\n",
    "    \"\"\"\n",
    "    Enhanced version with percentile bands and shared scales for easy comparison.\n",
    "    \"\"\"\n",
    "    n_models = len(model_results_dict)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=(4*n_models, 10), dpi=300)\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # Collect all data first to determine global ranges\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    global_y_max = 0\n",
    "    global_run_means_min = float('inf')\n",
    "    global_run_means_max = float('-inf')\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            run_means = np.array(run_means)\n",
    "            \n",
    "            # Update global ranges\n",
    "            model_x_min = min(np.min(rewards) for rewards in all_rewards)\n",
    "            model_x_max = max(np.max(rewards) for rewards in all_rewards)\n",
    "            global_x_min = min(global_x_min, model_x_min)\n",
    "            global_x_max = max(global_x_max, model_x_max)\n",
    "            global_run_means_min = min(global_run_means_min, np.min(run_means))\n",
    "            global_run_means_max = max(global_run_means_max, np.max(run_means))\n",
    "            \n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': run_means\n",
    "            }\n",
    "    \n",
    "    # Add padding to ranges\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    y_padding = (global_run_means_max - global_run_means_min) * 0.1\n",
    "    global_run_means_min -= y_padding\n",
    "    global_run_means_max += y_padding\n",
    "    \n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Create plots with shared scales\n",
    "    for idx, (model_name, data) in enumerate(all_model_data.items()):\n",
    "        ax_top = axes[0, idx] if n_models > 1 else axes[0]\n",
    "        ax_bottom = axes[1, idx] if n_models > 1 else axes[1]\n",
    "        \n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        consistency = 1 / (1 + np.std(run_means) / np.mean(run_means))\n",
    "        color = colors.get(model_name, 'gray')\n",
    "        \n",
    "        # Plot individual runs on top subplot\n",
    "        for rewards in all_rewards:\n",
    "            if len(rewards) > 1:\n",
    "                try:\n",
    "                    from scipy.stats import gaussian_kde\n",
    "                    kde = gaussian_kde(rewards)\n",
    "                    density = kde(common_x)\n",
    "                    ax_top.plot(common_x, density, color=color, alpha=0.2, linewidth=1)\n",
    "                    global_y_max = max(global_y_max, np.max(density))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Mean distribution with confidence intervals\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                mean_curve = np.mean(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot percentile bands if requested\n",
    "                if show_percentiles:\n",
    "                    p25_curve = np.percentile(kde_curves, 25, axis=0)\n",
    "                    p75_curve = np.percentile(kde_curves, 75, axis=0)\n",
    "                    p10_curve = np.percentile(kde_curves, 10, axis=0)\n",
    "                    p90_curve = np.percentile(kde_curves, 90, axis=0)\n",
    "                    \n",
    "                    ax_top.fill_between(common_x, p10_curve, p90_curve,\n",
    "                                       color=color, alpha=0.1)\n",
    "                    ax_top.fill_between(common_x, p25_curve, p75_curve,\n",
    "                                       color=color, alpha=0.2)\n",
    "                \n",
    "                # Mean line\n",
    "                ax_top.plot(common_x, mean_curve, color=color, linewidth=4, alpha=0.9)\n",
    "        \n",
    "        # Set consistent limits for top subplot\n",
    "        ax_top.set_xlim(global_x_min, global_x_max)\n",
    "        ax_top.set_title(f'{model_name.upper()}\\nConsistency: {consistency:.3f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "        ax_top.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bottom subplot: Box plot with consistent scale\n",
    "        ax_bottom.boxplot([run_means], positions=[0], widths=0.6, patch_artist=True,\n",
    "                         boxprops=dict(facecolor=color, alpha=0.7))\n",
    "        ax_bottom.set_xlim(-0.5, 0.5)\n",
    "        ax_bottom.set_ylim(global_run_means_min, global_run_means_max)\n",
    "        ax_bottom.set_ylabel('Run Means', fontsize=10)\n",
    "        ax_bottom.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = f'Mean: {np.mean(run_means):.1f}\\nStd: {np.std(run_means):.2f}\\nCV: {np.std(run_means)/np.mean(run_means):.3f}'\n",
    "        ax_bottom.text(0.02, 0.98, stats_text, transform=ax_bottom.transAxes,\n",
    "                      verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                      fontsize=9)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': np.mean(run_means), \n",
    "            'consistency': consistency,\n",
    "            'run_std': np.std(run_means),\n",
    "            'cv': np.std(run_means)/np.mean(run_means),\n",
    "            'n_runs': len(run_means)\n",
    "        }\n",
    "    \n",
    "    # Set consistent y-limits for all top subplots\n",
    "    y_max_padding = global_y_max * 0.05\n",
    "    for idx in range(n_models):\n",
    "        ax_top = axes[0, idx] if n_models > 1 else axes[0]\n",
    "        ax_top.set_ylim(0, global_y_max + y_max_padding)\n",
    "        \n",
    "        # Only add y-label to leftmost subplot\n",
    "        if idx == 0:\n",
    "            ax_top.set_ylabel('Density', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Enhanced Model Consistency Analysis with Shared Scales for Easy Comparison', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.pdf', '_enhanced.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Call the functions with shared scales\n",
    "stats_small_multiples = plot_multi_model_small_multiples(\n",
    "    model_results_dict_1, \n",
    "    save_path='multi_model_small_multiples_shared_scale.pdf'\n",
    ")\n",
    "\n",
    "# Enhanced version with shared scales\n",
    "stats_enhanced = plot_multi_model_small_multiples_enhanced(\n",
    "    model_results_dict_1,\n",
    "    save_path='multi_model_small_multiples_enhanced_shared_scale.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_combined_comparison(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Single plot showing all models together with mean distributions and confidence intervals.\n",
    "    All models share the same scale for direct comparison.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 10), dpi=300)\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # First pass: collect all data to determine global x range\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            # Update global ranges\n",
    "            model_x_min = min(np.min(rewards) for rewards in all_rewards)\n",
    "            model_x_max = max(np.max(rewards) for rewards in all_rewards)\n",
    "            global_x_min = min(global_x_min, model_x_min)\n",
    "            global_x_max = max(global_x_max, model_x_max)\n",
    "            \n",
    "            # Store processed data for second pass\n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means)\n",
    "            }\n",
    "    \n",
    "    # Add some padding to the global range\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    # Common x range for all KDE calculations\n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Second pass: create plots for each model\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        consistency = 1 / (1 + np.std(run_means) / np.mean(run_means))\n",
    "        \n",
    "        # Plot styling based on consistency\n",
    "        if consistency > 0.8:\n",
    "            alpha_individual = 0.6\n",
    "            linewidth_individual = 1.0\n",
    "            consistency_label = \"HIGH\"\n",
    "        elif consistency > 0.6:\n",
    "            alpha_individual = 0.6\n",
    "            linewidth_individual = 0.8\n",
    "            consistency_label = \"MEDIUM\"\n",
    "        else:\n",
    "            alpha_individual = 0.6\n",
    "            linewidth_individual = 0.6\n",
    "            consistency_label = \"LOW\"\n",
    "        \n",
    "        color = colors.get(model_name, 'gray')\n",
    "        \n",
    "        # Plot individual runs with low alpha\n",
    "        for rewards in all_rewards:\n",
    "            if len(rewards) > 1:\n",
    "                try:\n",
    "                    from scipy.stats import gaussian_kde\n",
    "                    kde = gaussian_kde(rewards)\n",
    "                    density = kde(common_x)\n",
    "                    plt.plot(common_x, density, color=color, \n",
    "                           alpha=alpha_individual, linewidth=linewidth_individual)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Calculate and plot mean distribution with confidence intervals\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                \n",
    "                # Calculate mean and confidence bounds\n",
    "                mean_curve = np.mean(kde_curves, axis=0)\n",
    "                std_curve = np.std(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot confidence intervals first (so they appear behind the line)\n",
    "                plt.fill_between(common_x, \n",
    "                               mean_curve, \n",
    "                               mean_curve + std_curve,\n",
    "                               color=color, alpha=0.2, \n",
    "                               label=f'{model_name.upper()} 1')\n",
    "                \n",
    "                plt.fill_between(common_x, \n",
    "                               mean_curve, \n",
    "                               mean_curve + 0.5*std_curve,\n",
    "                               color=color, alpha=0.3)\n",
    "                \n",
    "                # Plot mean distribution (thicker line)\n",
    "                overall_mean = np.mean(np.concatenate(all_rewards))\n",
    "                plt.plot(common_x, mean_curve, \n",
    "                       color=color, linewidth=4, alpha=0.9,\n",
    "                       label=f'{model_name.upper()}: ={overall_mean:.1f}, {consistency_label} consistency')\n",
    "                \n",
    "                # Add overall mean line\n",
    "                plt.axvline(overall_mean, color=color, linestyle='--', \n",
    "                          linewidth=2, alpha=0.7)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean, \n",
    "            'consistency': consistency,\n",
    "            'run_std': np.std(run_means),\n",
    "            'cv': np.std(run_means)/np.mean(run_means),\n",
    "            'n_runs': len(run_means)\n",
    "        }\n",
    "    \n",
    "    # Styling\n",
    "    plt.title('Model Comparison: Individual Runs + Mean Distributions with Confidence Intervals\\n'\n",
    "              'Thick lines = Mean distribution, Dashed lines = Overall means, Shaded areas = 1 confidence', \n",
    "              fontsize=18, fontweight='bold', pad=25)\n",
    "    plt.xlabel('Final Reward', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Density', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Professional styling\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.legend(fontsize=12, frameon=True, framealpha=0.9, loc='upper left')\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Increase tick label size\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    stats_text = \"Consistency Ranking:\\n\"\n",
    "    sorted_models = sorted(model_stats.items(), key=lambda x: x[1]['consistency'], reverse=True)\n",
    "    for rank, (model_name, stats) in enumerate(sorted_models, 1):\n",
    "        stats_text += f\"{rank}. {model_name.upper()}: {stats['consistency']:.3f}\\n\"\n",
    "    \n",
    "    plt.text(0.98, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'),\n",
    "             fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL CONSISTENCY SUMMARY - COMBINED COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    for rank, (model_name, stats) in enumerate(sorted_models, 1):\n",
    "        print(f\"\\n{rank}. {model_name.upper()}:\")\n",
    "        print(f\"   Consistency Score: {stats['consistency']:.3f}\")\n",
    "        print(f\"   Mean Reward: {stats['mean']:.2f}\")\n",
    "        print(f\"   Run Std Dev: {stats['run_std']:.2f}\")\n",
    "        print(f\"   Coefficient of Variation: {stats['cv']:.3f}\")\n",
    "        print(f\"   Number of Runs: {stats['n_runs']}\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Alternative version with even cleaner styling\n",
    "def plot_multi_model_combined_clean(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Clean version focusing just on mean distributions and overall means.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8), dpi=300)\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # Collect data and calculate global range\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            model_x_min = min(np.min(rewards) for rewards in all_rewards)\n",
    "            model_x_max = max(np.max(rewards) for rewards in all_rewards)\n",
    "            global_x_min = min(global_x_min, model_x_min)\n",
    "            global_x_max = max(global_x_max, model_x_max)\n",
    "            \n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means)\n",
    "            }\n",
    "    \n",
    "    # Add padding\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Plot each model\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        color = colors.get(model_name, 'gray')\n",
    "        overall_mean = np.mean(np.concatenate(all_rewards))\n",
    "        consistency = 1 / (1 + np.std(run_means) / np.mean(run_means))\n",
    "        \n",
    "        # Calculate mean KDE curve\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                mean_curve = np.mean(kde_curves, axis=0)\n",
    "                std_curve = np.std(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot mean distribution\n",
    "                plt.plot(common_x, mean_curve, \n",
    "                       color=color, linewidth=3, alpha=0.9,\n",
    "                       label=f'{model_name.upper()}: ={overall_mean:.1f} (CV={np.std(run_means)/np.mean(run_means):.3f})')\n",
    "                \n",
    "                # Add light confidence band\n",
    "                plt.fill_between(common_x, \n",
    "                               mean_curve, \n",
    "                               mean_curve + 0.5*std_curve,\n",
    "                               color=color, alpha=0.2)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean, \n",
    "            'consistency': consistency,\n",
    "            'cv': np.std(run_means)/np.mean(run_means)\n",
    "        }\n",
    "    \n",
    "    plt.title('Model Performance Comparison: Mean Reward Distributions\\n'\n",
    "              'Lines show average distribution across 30 independent runs', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Final Reward', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Density', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12, frameon=True, framealpha=0.9)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.pdf', '_clean.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Call the functions\n",
    "stats_combined = plot_multi_model_combined_comparison(\n",
    "    model_results_dict_1, \n",
    "    save_path='multi_model_combined_comparison.pdf'\n",
    ")\n",
    "\n",
    "# Clean version\n",
    "stats_clean = plot_multi_model_combined_clean(\n",
    "    model_results_dict_1,\n",
    "    save_path='multi_model_combined_clean.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_combined_boxplot_distribution(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Two-panel plot: boxplots on top, distributions on bottom with shared x-axis.\n",
    "    \"\"\"\n",
    "    fig, (ax_top, ax_bottom) = plt.subplots(2, 1, figsize=(16, 12), dpi=300, \n",
    "                                           sharex=True, gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    # First pass: collect all data to determine global x range\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    all_model_data = {}\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            # Update global ranges\n",
    "            model_x_min = min(np.min(rewards) for rewards in all_rewards)\n",
    "            model_x_max = max(np.max(rewards) for rewards in all_rewards)\n",
    "            global_x_min = min(global_x_min, model_x_min)\n",
    "            global_x_max = max(global_x_max, model_x_max)\n",
    "            \n",
    "            # Store processed data\n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means)\n",
    "            }\n",
    "    \n",
    "    # Add padding to the global range\n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    \n",
    "    # Common x range for KDE calculations\n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # Prepare data for boxplots\n",
    "    boxplot_data = []\n",
    "    boxplot_labels = []\n",
    "    boxplot_colors = []\n",
    "    positions = []\n",
    "    \n",
    "    # TOP PANEL: Horizontal boxplots\n",
    "    y_pos = 0\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        # Create boxplot for run means\n",
    "        bp = ax_top.boxplot([run_means], positions=[y_pos], vert=False, widths=0.6, \n",
    "                           patch_artist=True, showfliers=True,\n",
    "                           boxprops=dict(facecolor=colors.get(model_name, 'gray'), alpha=0.7),\n",
    "                           medianprops=dict(color='black', linewidth=2),\n",
    "                           flierprops=dict(marker='o', markerfacecolor=colors.get(model_name, 'gray'), \n",
    "                                         markersize=4, alpha=0.6))\n",
    "        \n",
    "        # Add individual run means as scatter points\n",
    "        ax_top.scatter(run_means, [y_pos] * len(run_means), \n",
    "                      color=colors.get(model_name, 'gray'), alpha=0.4, s=20, zorder=10)\n",
    "        \n",
    "        consistency = 1 / (1 + np.std(run_means) / np.mean(run_means))\n",
    "        overall_mean = np.mean(np.concatenate(all_rewards))\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean, \n",
    "            'consistency': consistency,\n",
    "            'run_std': np.std(run_means),\n",
    "            'cv': np.std(run_means)/np.mean(run_means),\n",
    "            'n_runs': len(run_means),\n",
    "            'run_means': run_means\n",
    "        }\n",
    "        \n",
    "        y_pos += 1\n",
    "    \n",
    "    # Style top panel\n",
    "    ax_top.set_yticks(range(len(all_model_data)))\n",
    "    ax_top.set_yticklabels([name.upper() for name in all_model_data.keys()], fontsize=12)\n",
    "    ax_top.set_ylabel('Methods', fontsize=14, fontweight='bold')\n",
    "    ax_top.set_title('Run-Level Performance Distribution (Boxplots)\\n'\n",
    "                    'Each box shows distribution of mean rewards across 30 runs', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_top.grid(True, alpha=0.3, axis='x')\n",
    "    ax_top.set_xlim(global_x_min, global_x_max)\n",
    "    \n",
    "    # Add statistics text for top panel\n",
    "    stats_text = \"Run Statistics:\\n\"\n",
    "    sorted_models = sorted(model_stats.items(), key=lambda x: x[1]['consistency'], reverse=True)\n",
    "    for rank, (model_name, stats) in enumerate(sorted_models, 1):\n",
    "        stats_text += f\"{rank}. {model_name.upper()}: CV={stats['cv']:.3f}\\n\"\n",
    "    \n",
    "    ax_top.text(0.98, 0.98, stats_text, transform=ax_top.transAxes,\n",
    "               verticalalignment='top', horizontalalignment='right',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'),\n",
    "               fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # BOTTOM PANEL: Distribution plots\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        color = colors.get(model_name, 'gray')\n",
    "        consistency = model_stats[model_name]['consistency']\n",
    "        overall_mean = model_stats[model_name]['mean']\n",
    "        \n",
    "        # Calculate mean KDE curve\n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                mean_curve = np.mean(kde_curves, axis=0)\n",
    "                std_curve = np.std(kde_curves, axis=0)\n",
    "                \n",
    "                # Plot confidence intervals\n",
    "                ax_bottom.fill_between(common_x, \n",
    "                                     mean_curve - std_curve, \n",
    "                                     mean_curve + std_curve,\n",
    "                                     color=color, alpha=0.15)\n",
    "                \n",
    "                ax_bottom.fill_between(common_x, \n",
    "                                     mean_curve - 0.5*std_curve, \n",
    "                                     mean_curve + 0.5*std_curve,\n",
    "                                     color=color, alpha=0.25)\n",
    "                \n",
    "                # Plot mean distribution (thick line)\n",
    "                ax_bottom.plot(common_x, mean_curve, \n",
    "                             color=color, linewidth=4, alpha=0.9,\n",
    "                             label=f'{model_name.upper()}: ={overall_mean:.1f}')\n",
    "                \n",
    "                # Add overall mean line\n",
    "                ax_bottom.axvline(overall_mean, color=color, linestyle='--', \n",
    "                                linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Style bottom panel\n",
    "    ax_bottom.set_xlabel('Final Reward', fontsize=14, fontweight='bold')\n",
    "    ax_bottom.set_ylabel('Density', fontsize=14, fontweight='bold')\n",
    "    ax_bottom.set_title('Aggregate Reward Distributions\\n'\n",
    "                       'Mean distribution across all runs with confidence intervals', \n",
    "                       fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_bottom.grid(True, alpha=0.3)\n",
    "    ax_bottom.legend(fontsize=12, frameon=True, framealpha=0.9, loc='upper left')\n",
    "    ax_bottom.set_xlim(global_x_min, global_x_max)\n",
    "    \n",
    "    # Overall styling\n",
    "    plt.suptitle('Model Performance Analysis: Run-Level Variability vs Aggregate Distributions\\n'\n",
    "                'Top: Individual run performance | Bottom: Overall reward distributions', \n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Remove spines\n",
    "    for ax in [ax_top, ax_bottom]:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)  # Make room for suptitle\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY - BOXPLOT + DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    for rank, (model_name, stats) in enumerate(sorted_models, 1):\n",
    "        print(f\"\\n{rank}. {model_name.upper()}:\")\n",
    "        print(f\"   Mean Reward: {stats['mean']:.2f}\")\n",
    "        print(f\"   Run-to-Run CV: {stats['cv']:.3f}\")\n",
    "        print(f\"   Consistency Score: {stats['consistency']:.3f}\")\n",
    "        print(f\"   Run Std Dev: {stats['run_std']:.2f}\")\n",
    "        print(f\"   Number of Runs: {stats['n_runs']}\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Alternative version with violin plots instead of boxplots\n",
    "def plot_multi_model_combined_violin_distribution(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Two-panel plot: violin plots on top, distributions on bottom with shared x-axis.\n",
    "    \"\"\"\n",
    "    fig, (ax_top, ax_bottom) = plt.subplots(2, 1, figsize=(16, 12), dpi=300, \n",
    "                                           sharex=True, gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    # [Same data collection logic as above]\n",
    "    all_model_data = {}\n",
    "    global_x_min = float('inf')\n",
    "    global_x_max = float('-inf')\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        all_rewards = []\n",
    "        run_means = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if rewards.ndim == 2:\n",
    "                    rewards = rewards[:, -1]\n",
    "                elif rewards.ndim > 2:\n",
    "                    continue\n",
    "                \n",
    "                rewards = rewards[np.isfinite(rewards)]\n",
    "                \n",
    "                if len(rewards) > 0:\n",
    "                    all_rewards.append(rewards)\n",
    "                    run_means.append(np.mean(rewards))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if len(all_rewards) > 0:\n",
    "            model_x_min = min(np.min(rewards) for rewards in all_rewards)\n",
    "            model_x_max = max(np.max(rewards) for rewards in all_rewards)\n",
    "            global_x_min = min(global_x_min, model_x_min)\n",
    "            global_x_max = max(global_x_max, model_x_max)\n",
    "            \n",
    "            all_model_data[model_name] = {\n",
    "                'all_rewards': all_rewards,\n",
    "                'run_means': np.array(run_means)\n",
    "            }\n",
    "    \n",
    "    x_padding = (global_x_max - global_x_min) * 0.05\n",
    "    global_x_min -= x_padding\n",
    "    global_x_max += x_padding\n",
    "    common_x = np.linspace(global_x_min, global_x_max, 300)\n",
    "    \n",
    "    # TOP PANEL: Horizontal violin plots\n",
    "    violin_data = []\n",
    "    y_positions = []\n",
    "    violin_colors = []\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(all_model_data.items()):\n",
    "        run_means = data['run_means']\n",
    "        violin_data.append(run_means)\n",
    "        y_positions.append(i)\n",
    "        violin_colors.append(colors.get(model_name, 'gray'))\n",
    "    \n",
    "    # Create horizontal violin plots\n",
    "    parts = ax_top.violinplot(violin_data, positions=y_positions, vert=False, \n",
    "                             showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Color the violins\n",
    "    for pc, color in zip(parts['bodies'], violin_colors):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Style top panel\n",
    "    ax_top.set_yticks(range(len(all_model_data)))\n",
    "    ax_top.set_yticklabels([name.upper() for name in all_model_data.keys()], fontsize=12)\n",
    "    ax_top.set_ylabel('Methods', fontsize=14, fontweight='bold')\n",
    "    ax_top.set_title('Run-Level Performance Distribution (Violin Plots)', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_top.grid(True, alpha=0.3, axis='x')\n",
    "    ax_top.set_xlim(global_x_min, global_x_max)\n",
    "    \n",
    "    # BOTTOM PANEL: Same as before\n",
    "    model_stats = {}\n",
    "    for model_name, data in all_model_data.items():\n",
    "        all_rewards = data['all_rewards']\n",
    "        run_means = data['run_means']\n",
    "        \n",
    "        color = colors.get(model_name, 'gray')\n",
    "        overall_mean = np.mean(np.concatenate(all_rewards))\n",
    "        consistency = 1 / (1 + np.std(run_means) / np.mean(run_means))\n",
    "        \n",
    "        if len(all_rewards) > 1:\n",
    "            kde_curves = []\n",
    "            for rewards in all_rewards:\n",
    "                if len(rewards) > 1:\n",
    "                    try:\n",
    "                        from scipy.stats import gaussian_kde\n",
    "                        kde = gaussian_kde(rewards)\n",
    "                        kde_curve = kde(common_x)\n",
    "                        kde_curves.append(kde_curve)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            if len(kde_curves) > 0:\n",
    "                kde_curves = np.array(kde_curves)\n",
    "                mean_curve = np.mean(kde_curves, axis=0)\n",
    "                std_curve = np.std(kde_curves, axis=0)\n",
    "                \n",
    "                ax_bottom.fill_between(common_x, \n",
    "                                     mean_curve - 0.5*std_curve, \n",
    "                                     mean_curve + 0.5*std_curve,\n",
    "                                     color=color, alpha=0.25)\n",
    "                \n",
    "                ax_bottom.plot(common_x, mean_curve, \n",
    "                             color=color, linewidth=4, alpha=0.9,\n",
    "                             label=f'{model_name.upper()}: ={overall_mean:.1f}')\n",
    "                \n",
    "                ax_bottom.axvline(overall_mean, color=color, linestyle='--', \n",
    "                                linewidth=2, alpha=0.7)\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean, \n",
    "            'consistency': consistency\n",
    "        }\n",
    "    \n",
    "    ax_bottom.set_xlabel('Final Reward', fontsize=14, fontweight='bold')\n",
    "    ax_bottom.set_ylabel('Density', fontsize=14, fontweight='bold')\n",
    "    ax_bottom.set_title('Aggregate Reward Distributions', \n",
    "                       fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_bottom.grid(True, alpha=0.3)\n",
    "    ax_bottom.legend(fontsize=12, frameon=True, framealpha=0.9)\n",
    "    ax_bottom.set_xlim(global_x_min, global_x_max)\n",
    "    \n",
    "    plt.suptitle('Model Performance: Violin + Distribution Analysis', \n",
    "                fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.pdf', '_violin.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# # Call the functions\n",
    "# stats_boxplot = plot_multi_model_combined_boxplot_distribution(\n",
    "#     model_results_dict_1, \n",
    "#     save_path='multi_model_boxplot_distribution.pdf'\n",
    "# )\n",
    "\n",
    "# # Violin version\n",
    "# stats_violin = plot_multi_model_combined_violin_distribution(\n",
    "#     model_results_dict_1,\n",
    "#     save_path='multi_model_violin_distribution.pdf'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Call the corrected functions\n",
    "# stats_corrected = plot_multi_model_reward_distributions_with_ci_corrected(\n",
    "#     model_results_dict_1, \n",
    "#     save_path='multi_model_distributions_corrected.pdf'\n",
    "# )\n",
    "\n",
    "# Or use the clean version\n",
    "stats_clean = plot_multi_model_clean_kde(\n",
    "    model_results_dict_1,\n",
    "    save_path='multi_model_distributions_clean.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_reward_boxviolin(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Create box + violin plot showing distribution shapes and statistics.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8), dpi=300)\n",
    "    \n",
    "    # Prepare data\n",
    "    all_data = []\n",
    "    method_names = []\n",
    "    \n",
    "    for model_name, results_dict in model_results_dict.items():\n",
    "        method_data = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            if isinstance(rewards, (list, tuple)):\n",
    "                rewards = np.array(rewards, dtype=float)\n",
    "            elif isinstance(rewards, np.ndarray):\n",
    "                rewards = rewards.astype(float)\n",
    "            \n",
    "            if rewards.ndim == 1:\n",
    "                final_rewards = rewards\n",
    "            elif rewards.ndim == 2:\n",
    "                final_rewards = rewards[:, -1]\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            final_rewards = final_rewards[np.isfinite(final_rewards)]\n",
    "            if len(final_rewards) > 0:\n",
    "                method_data.extend(final_rewards)\n",
    "        \n",
    "        if method_data:\n",
    "            all_data.append(method_data)\n",
    "            method_names.append(model_name)\n",
    "    \n",
    "    # Create violin plot\n",
    "    parts = plt.violinplot(all_data, positions=range(len(method_names)), \n",
    "                          showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Customize violin colors\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(method_names)))\n",
    "    for pc, color in zip(parts['bodies'], colors):\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Add box plot overlay\n",
    "    bp = plt.boxplot(all_data, positions=range(len(method_names)), \n",
    "                    widths=0.3, patch_artist=True, \n",
    "                    boxprops=dict(alpha=0.3),\n",
    "                    showfliers=False)  # Hide outliers for cleaner look\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # Styling\n",
    "    plt.title('Model Comparison: Distribution Shapes and Statistics', \n",
    "              fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Methods', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Final Reward Values', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.xticks(range(len(method_names)), method_names, fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add statistics annotation\n",
    "    stats_text = \"Statistics Summary:\\n\"\n",
    "    for i, (method, data) in enumerate(zip(method_names, all_data)):\n",
    "        mean_val = np.mean(data)\n",
    "        std_val = np.std(data)\n",
    "        stats_text += f\"{method}: ={mean_val:.1f}{std_val:.1f}\\n\"\n",
    "    \n",
    "    plt.text(0.02, 0.12, stats_text, transform=plt.gca().transAxes,\n",
    "             verticalalignment='top', horizontalalignment='left',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9),\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_multi_model_reward_boxviolin(\n",
    "    model_results_dict_1,\n",
    "    save_path='multi_model_reward_boxviolin.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_ridgeline(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Create ridgeline plot with stacked distributions showing mean  std.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(model_results_dict), 1, \n",
    "                            figsize=(12, 2*len(model_results_dict)), \n",
    "                            dpi=300, sharex=True)\n",
    "    \n",
    "    if len(model_results_dict) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Use the same color scheme as your other plots\n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    for idx, (model_name, results_dict) in enumerate(model_results_dict.items()):\n",
    "        # Collect all rewards\n",
    "        all_rewards = []\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            try:\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Extract final rewards based on dimensionality\n",
    "                if rewards.ndim == 1:\n",
    "                    final_rewards = rewards\n",
    "                elif rewards.ndim == 2:\n",
    "                    final_rewards = rewards[:, -1]  # Take last column\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Remove any NaN or infinite values\n",
    "                final_rewards = final_rewards[np.isfinite(final_rewards)]\n",
    "                if len(final_rewards) > 0:\n",
    "                    all_rewards.extend(final_rewards)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if all_rewards:\n",
    "            all_rewards = np.array(all_rewards)\n",
    "            color = colors.get(model_name, 'gray')\n",
    "            \n",
    "            # Plot distribution\n",
    "            sns.kdeplot(data=all_rewards, ax=axes[idx], color=color, fill=True, alpha=0.7)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            mean_val = np.mean(all_rewards)\n",
    "            std_val = np.std(all_rewards)\n",
    "            \n",
    "            # Add mean line\n",
    "            axes[idx].axvline(mean_val, color='black', linestyle='--', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            # Add 1 std shaded region\n",
    "            axes[idx].axvspan(mean_val - std_val, mean_val + std_val, \n",
    "                             color=color, alpha=0.2)\n",
    "            \n",
    "            # Styling for each subplot with mean  std\n",
    "            axes[idx].set_ylabel(f'{model_name.upper()}\\n(={mean_val:.2f}{std_val:.2f})', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            axes[idx].set_yticks([])  # Remove y-ticks for cleaner look\n",
    "            \n",
    "            # Add text box with additional stats\n",
    "            stats_text = f'n={len(all_rewards)}'\n",
    "            axes[idx].text(0.02, 0.98, stats_text, transform=axes[idx].transAxes,\n",
    "                          verticalalignment='top', horizontalalignment='left',\n",
    "                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                          fontsize=10)\n",
    "    \n",
    "    # Overall styling\n",
    "    axes[-1].set_xlabel('Final Reward Values', fontsize=12, fontweight='bold')\n",
    "    plt.suptitle('Ridgeline Plot: Model Performance Distributions\\n'\n",
    "                 'Each distribution represents 6000 trajectories (30 models  200 simulations each)',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_multi_model_ridgeline(\n",
    "    model_results_dict_1,\n",
    "    save_path='multi_model_ridgeline.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Run the analysis\n",
    "# save_dir = 'analysis_outputs/statistical_analysis_oracle_1'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# summary_df, test_results, run_means = perform_comprehensive_statistical_analysis(\n",
    "#     model_results_dict_1, \n",
    "#     save_dir=save_dir\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Collect all seeds from each model\n",
    "all_seeds = {}\n",
    "\n",
    "# Model results dict 1 (Oracle 1)\n",
    "for model_name, runs in model_results_dict_1.items():\n",
    "    all_seeds[f\"{model_name}_oracle_1\"] = {}\n",
    "    for run_idx, run_data in runs.items():\n",
    "        all_seeds[f\"{model_name}_oracle_1\"][f\"run_{run_idx}\"] = run_data['seed']\n",
    "\n",
    "# Model results dict 2 (Oracle 2) \n",
    "for model_name, runs in model_results_dict_2.items():\n",
    "    all_seeds[f\"{model_name}_oracle_2\"] = {}\n",
    "    for run_idx, run_data in runs.items():\n",
    "        all_seeds[f\"{model_name}_oracle_2\"][f\"run_{run_idx}\"] = run_data['seed']\n",
    "\n",
    "# Model results dict 3 (Oracle 3)\n",
    "for model_name, runs in model_results_dict_3.items():\n",
    "    all_seeds[f\"{model_name}_oracle_3\"] = {}\n",
    "    for run_idx, run_data in runs.items():\n",
    "        all_seeds[f\"{model_name}_oracle_3\"][f\"run_{run_idx}\"] = run_data['seed']\n",
    "\n",
    "# Save to JSON file\n",
    "with open('seeds.json', 'w') as f:\n",
    "    json.dump(all_seeds, f, indent=2)\n",
    "\n",
    "print(\"Seeds saved to seeds.json\")\n",
    "print(f\"Total models across all oracles: {len(all_seeds)}\")\n",
    "for model_key in all_seeds.keys():\n",
    "    print(f\"{model_key}: {len(all_seeds[model_key])} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_model_reward_distributions_with_ci(model_results_dict, save_path=None, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Plot reward distributions for multiple models with confidence intervals.\n",
    "    Shows single mean line per method with shaded confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        model_results_dict (dict): Dictionary where keys are model names and values are \n",
    "                                 dictionaries with run results containing 'rewards' arrays\n",
    "        save_path (str, optional): Path to save the plot\n",
    "        confidence_level (float): Confidence level for intervals (default 0.95 for 95% CI)\n",
    "    \"\"\"\n",
    "    from scipy.stats import gaussian_kde\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create professional distribution plot\n",
    "    plt.figure(figsize=(14, 10), dpi=300)\n",
    "    \n",
    "    # Define color palette for different models\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(model_results_dict)))\n",
    "    \n",
    "    model_stats = {}\n",
    "    \n",
    "    for model_idx, (model_name, results_dict) in enumerate(model_results_dict.items()):\n",
    "        # Collect all rewards from all runs for this model\n",
    "        all_rewards = []\n",
    "        run_means = []  # Store mean of each run for CI calculation\n",
    "        \n",
    "        for run_idx in range(len(results_dict)):\n",
    "            rewards = results_dict[run_idx]['rewards']\n",
    "            \n",
    "            # Handle different reward shapes and ensure numeric type\n",
    "            if isinstance(rewards, (list, tuple)):\n",
    "                rewards = np.array(rewards, dtype=float)\n",
    "            elif isinstance(rewards, np.ndarray):\n",
    "                rewards = rewards.astype(float)\n",
    "            \n",
    "            # Extract final rewards based on dimensionality\n",
    "            if rewards.ndim == 1:\n",
    "                final_rewards = rewards\n",
    "            elif rewards.ndim == 2:\n",
    "                final_rewards = rewards[:, -1]  # Take last column\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected rewards shape: {rewards.shape}\")\n",
    "            \n",
    "            # Remove any NaN or infinite values\n",
    "            final_rewards = final_rewards[np.isfinite(final_rewards)]\n",
    "            \n",
    "            if len(final_rewards) == 0:\n",
    "                print(f\"Warning: No valid rewards found for {model_name}, run {run_idx}\")\n",
    "                continue\n",
    "                \n",
    "            all_rewards.append(final_rewards)\n",
    "            run_means.append(np.mean(final_rewards))\n",
    "        \n",
    "        if len(all_rewards) == 0:\n",
    "            print(f\"Warning: No valid data for {model_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        all_values = np.concatenate(all_rewards)\n",
    "        \n",
    "        # Ensure all_values is numeric and finite\n",
    "        all_values = all_values[np.isfinite(all_values)]\n",
    "        \n",
    "        if len(all_values) == 0:\n",
    "            print(f\"Warning: No finite values for {model_name}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        overall_mean = np.mean(all_values)\n",
    "        overall_std = np.std(all_values)\n",
    "        \n",
    "        # Calculate confidence intervals from run means\n",
    "        run_means = np.array(run_means)\n",
    "        run_means = run_means[np.isfinite(run_means)]  # Remove any NaN values\n",
    "        \n",
    "        if len(run_means) == 0:\n",
    "            print(f\"Warning: No valid run means for {model_name}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        run_mean_avg = np.mean(run_means)\n",
    "        run_std_error = np.std(run_means) / np.sqrt(len(run_means))\n",
    "        \n",
    "        # Calculate confidence interval (95% by default)\n",
    "        z_score = 1.96 if confidence_level == 0.95 else 2.576 if confidence_level == 0.99 else 1.645\n",
    "        ci_lower = run_mean_avg - z_score * run_std_error\n",
    "        ci_upper = run_mean_avg + z_score * run_std_error\n",
    "        \n",
    "        # Create KDE for the overall distribution\n",
    "        try:\n",
    "            kde = gaussian_kde(all_values)\n",
    "            x_range = np.linspace(all_values.min(), all_values.max(), 1000)\n",
    "            density = kde(x_range)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: KDE failed for {model_name}: {e}\")\n",
    "            print(f\"Data type: {all_values.dtype}, shape: {all_values.shape}\")\n",
    "            print(f\"Sample values: {all_values[:5] if len(all_values) >= 5 else all_values}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot main distribution line\n",
    "        plt.plot(x_range, density, color=colors[model_idx], linewidth=3, \n",
    "                label=f'{model_name}: ={overall_mean:.2f}{overall_std:.2f}')\n",
    "        \n",
    "        # Create confidence interval bounds for the density\n",
    "        density_std = np.std(density)\n",
    "        upper_bound = density + 0.3 * density_std\n",
    "        lower_bound = np.maximum(density - 0.3 * density_std, 0)\n",
    "        \n",
    "        # Fill between for confidence interval visualization\n",
    "        plt.fill_between(x_range, lower_bound, upper_bound, \n",
    "                        color=colors[model_idx], alpha=0.2)\n",
    "        \n",
    "        # Add vertical lines for confidence interval of means\n",
    "        plt.axvline(ci_lower, color=colors[model_idx], linestyle=':', alpha=0.7, linewidth=2)\n",
    "        plt.axvline(ci_upper, color=colors[model_idx], linestyle=':', alpha=0.7, linewidth=2)\n",
    "        plt.axvline(overall_mean, color=colors[model_idx], linestyle='--', \n",
    "                   linewidth=3, alpha=0.8)\n",
    "        \n",
    "        # Store stats for summary\n",
    "        model_stats[model_name] = {\n",
    "            'mean': overall_mean,\n",
    "            'std': overall_std,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'color': colors[model_idx],\n",
    "            'total_trajectories': sum(len(rewards) for rewards in all_rewards),\n",
    "            'n_runs': len(run_means)\n",
    "        }\n",
    "    \n",
    "    # Only proceed with plotting if we have valid data\n",
    "    if not model_stats:\n",
    "        print(\"Error: No valid data found for any model!\")\n",
    "        return {}\n",
    "    \n",
    "    # Styling\n",
    "    plt.title(f'Model Comparison: Mean Distributions with {int(confidence_level*100)}% Confidence Intervals\\n'\n",
    "              f'(Based on Independent Runs)', \n",
    "              fontsize=18, fontweight='bold', pad=25)\n",
    "    plt.xlabel('Final Reward Values', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Density', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Professional styling\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.legend(fontsize=12, frameon=True, framealpha=0.9, loc='upper right')\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Increase tick label size\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics with confidence intervals\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"MODEL COMPARISON SUMMARY ({int(confidence_level*100)}% confidence intervals)\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, stats in model_stats.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.3f}  {stats['std']:.3f}\")\n",
    "        print(f\"  {int(confidence_level*100)}% CI: [{stats['ci_lower']:.3f}, {stats['ci_upper']:.3f}]\")\n",
    "        print(f\"  Runs: {stats['n_runs']}, Total Trajectories: {stats['total_trajectories']}\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "# Alternative version with error bars instead of shaded regions\n",
    "def plot_multi_model_reward_distributions_errorbar(model_results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Alternative approach using error bars on mean values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8), dpi=300)\n",
    "    \n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(model_results_dict)))\n",
    "    \n",
    "    positions = range(len(model_results_dict))\n",
    "    means = []\n",
    "    errors = []\n",
    "    labels = []\n",
    "    \n",
    "    for model_idx, (model_name, results_dict) in enumerate(model_results_dict.items()):\n",
    "        # Calculate mean reward per run\n",
    "        run_means = []\n",
    "        for run_idx in range(len(results_dict)):\n",
    "            if results_dict[run_idx]['rewards'].ndim == 1:\n",
    "                rewards = results_dict[run_idx]['rewards']\n",
    "            else:\n",
    "                rewards = results_dict[run_idx]['rewards'][:, -1]\n",
    "            run_means.append(np.mean(rewards))\n",
    "        \n",
    "        run_means = np.array(run_means)\n",
    "        mean_of_means = np.mean(run_means)\n",
    "        std_error = np.std(run_means) / np.sqrt(len(run_means))\n",
    "        ci_95 = 1.96 * std_error\n",
    "        \n",
    "        means.append(mean_of_means)\n",
    "        errors.append(ci_95)\n",
    "        labels.append(f'{model_name}\\n(={mean_of_means:.2f}{ci_95:.2f})')\n",
    "    \n",
    "    # Create bar plot with error bars\n",
    "    bars = plt.bar(positions, means, yerr=errors, capsize=10, \n",
    "                   color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Model Comparison: Mean Final Rewards with 95% Confidence Intervals', \n",
    "              fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Mean Final Reward', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Methods', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.xticks(positions, [name for name, _ in model_results_dict.items()], \n",
    "               rotation=45, ha='right', fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path.replace('.pdf', '_errorbar.pdf'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Usage examples:\n",
    "# Version 1: Density plots with shaded confidence intervals\n",
    "stats_ci = plot_multi_model_reward_distributions_with_ci(\n",
    "    model_results_dict_1, \n",
    "    save_path='multi_model_distributions_with_ci.pdf',\n",
    "    confidence_level=0.95\n",
    ")\n",
    "\n",
    "# Version 2: Bar chart with error bars\n",
    "plot_multi_model_reward_distributions_errorbar(\n",
    "    model_results_dict_1,\n",
    "    save_path='multi_model_distributions_errorbar.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_smcmc = joblib.load('results/PaperPlots_O3_SAC/sac_results_runs_oracle_3.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_smcmc[0]['trained_agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = joblib.load(r\"mlruns/4/1fefba2df9fe4438bac2dfada24da7f4/artifacts/forward_model_iteration_500/results_run.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Collect all rewards from all runs\n",
    "all_rewards = []\n",
    "for run_idx in range(30):\n",
    "    rewards = results_smcmc[run_idx]['results_dict']['rewards'][:, -1]\n",
    "    all_rewards.append(rewards)\n",
    "\n",
    "# Create professional distribution plot\n",
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "# Plot each run's KDE distribution\n",
    "for run_idx, rewards in enumerate(all_rewards):\n",
    "    sns.kdeplot(data=rewards, \n",
    "                color='steelblue', \n",
    "                alpha=0.1, \n",
    "                linewidth=0.1)\n",
    "\n",
    "# Calculate overall statistics for the plot\n",
    "all_values = np.concatenate(all_rewards)\n",
    "overall_mean = np.mean(all_values)\n",
    "overall_std = np.std(all_values)\n",
    "\n",
    "# Add mean line\n",
    "plt.axvline(overall_mean, color='red', linestyle='--', \n",
    "           linewidth=3, label=f'Overall Mean: {overall_mean:.2f}')\n",
    "\n",
    "# Styling\n",
    "plt.title('SMCMC Reward Distributions - 30 Independent Runs', \n",
    "          fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Final Reward Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Density', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add statistics text box\n",
    "total_trajectories = sum(len(rewards) for rewards in all_rewards)\n",
    "stats_text = f'30 Independent Runs\\n' \\\n",
    "             f'Mean: {overall_mean:.2f}  {overall_std:.2f}\\n' \\\n",
    "             f'Total Trajectories: {total_trajectories}'\n",
    "\n",
    "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, \n",
    "         verticalalignment='top', horizontalalignment='left',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, \n",
    "                  edgecolor='black', linewidth=1),\n",
    "         fontsize=12)\n",
    "\n",
    "# Professional styling\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.legend(fontsize=12, frameon=True, framealpha=0.9)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('smcmc_reward_distributions_overlay.pdf', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"SMCMC Results Summary (30 independent runs):\")\n",
    "print(f\"Overall Mean: {overall_mean:.3f}\")\n",
    "print(f\"Overall Std: {overall_std:.3f}\")\n",
    "print(f\"Min: {np.min(all_values):.3f}\")\n",
    "print(f\"Max: {np.max(all_values):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utility_functions import seed_all\n",
    "# Load forward and backward models\n",
    "root = r'mlruns/2'\n",
    "# run_id = '78263dd8c45e435d8579682b76a22ef9' # Current Best\n",
    "# run_id = '94e853e9e6014908b56273897438ef32' # Gaussian \n",
    "# run_id = '1b62a3f850e64697b4191238b3220b24' # Betak = 1 and Current Best\n",
    "# run_id = '940d0167c8784fe7a342794d0f36a2f9' # Gaussian k = 5\n",
    "# run_id = 'b31630e9983a4c79af6d2136fa9b3c0d' # Gaussian k = 10\n",
    "# run_id='78263dd8c45e435d8579682b76a22ef9' # gaussian k = 15\n",
    "# run_id = '17ed5f6c45274eb493c3085ab1b6524f' # beta k = 5\n",
    "# run_id = 'c0bcf8b0df2b48ad8a338c8565ab426d' # beta k = 10\n",
    "# run_id = '5d6a6a7413c442a0a892224b73aa4210' # beta k = 15\n",
    "# run_id = 'd6f0a6ce09274afa92685eba82a94b6f' # O2 beta k = 15\n",
    "# run_id = 'b3309f6144b646559887504bf74dcb3f' # O1 beta k = 15\n",
    "run_id = '5db11b78fc7d49dc9fd975ddf93c7e55' # Testing bootstrapping results\n",
    "iterations = 500\n",
    "path_fwd = fr'{root}/{run_id}/artifacts/forward_model_iteration_{iterations}/data/model.pth'\n",
    "path_bwd = fr'{root}/{run_id}/artifacts/backward_model_iteration_{iterations}/data/model.pth'\n",
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Load the forward and backward models\n",
    "forward_model = load_entire_model(path_fwd, device=device)\n",
    "backward_model = load_entire_model(path_bwd, device=device)\n",
    "# Load used config\n",
    "config_path = fr'{root}/{run_id}/artifacts/run_params_config.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "initial_state,feature_names = load_initial_state(config)\n",
    "trajectory_length = config['model']['training_parameters']['trajectory_length']\n",
    "n_trajectories = 200\n",
    "n_runs = 30\n",
    "input_dim = forward_model.input_layer.in_features\n",
    "model_path = config['oracle']['model_path']\n",
    "distribution = config['model']['training_parameters']['distribution_type']\n",
    "extra_parameters = {\n",
    "    'mixture_components': config['model']['training_parameters']['mixture_components'],\n",
    "    'num_variables': input_dim -1 ,\n",
    "}\n",
    "environment = GeneralEnvironment(\n",
    "    initial_state=initial_state,\n",
    "    config = config,\n",
    "    model = forward_model,\n",
    "    input_dim = forward_model.input_layer.in_features ,\n",
    "    max_steps=config['model']['training_parameters']['trajectory_length'],\n",
    "    model_path=config['oracle']['model_path'])\n",
    "\n",
    "logger,log_file_name = setup_logger('evaluation_logger')\n",
    "results_runs = {}\n",
    "for run in range(n_runs):\n",
    "    print(f\"Run {run+1}/{n_runs}\")\n",
    "    # Simulate trajectories using the forward and backward models\n",
    "     # Simulate trajectories using the forward and backward models\n",
    "    random_seed = random.randint(0, 10000)\n",
    "    seed_all(random_seed)\n",
    "    trajectories, rewards,all_actions,_ = simulate_trajectories(\n",
    "            env_class=GeneralEnvironment,\n",
    "            forward_model=forward_model,\n",
    "            backward_model=backward_model,\n",
    "            initial_state=initial_state,\n",
    "            config=config,\n",
    "            trajectory_length=trajectory_length,\n",
    "            n_trajectories=n_trajectories,\n",
    "            device=device,\n",
    "            logger = logger,\n",
    "            model_path = model_path,\n",
    "            distribution=distribution,\n",
    "            extra_parameters=extra_parameters\n",
    "\n",
    "        )\n",
    "    results_runs[run] = {'trajectories': trajectories, \n",
    "                          'rewards': rewards,\n",
    "                          'all_actions': all_actions,\n",
    "                          '_': _,\n",
    "                          'seed': random_seed}  # Store the feature names as well\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect rewards from all runs\n",
    "all_run_rewards = []\n",
    "run_statistics = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"Run {run+1}/{n_runs}\")\n",
    "    rewards = np.array(results_runs[run]['rewards'])[:,-1]  # Get final rewards\n",
    "    all_run_rewards.append(rewards)\n",
    "    \n",
    "    # Calculate statistics for this run\n",
    "    run_stats = {\n",
    "        'run': run,\n",
    "        'mean': np.mean(rewards),\n",
    "        'std': np.std(rewards),\n",
    "        'median': np.median(rewards),\n",
    "        'min': np.min(rewards),\n",
    "        'max': np.max(rewards),\n",
    "        'q25': np.percentile(rewards, 25),\n",
    "        'q75': np.percentile(rewards, 75)\n",
    "    }\n",
    "    run_statistics.append(run_stats)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "stats_df = pd.DataFrame(run_statistics)\n",
    "\n",
    "# Create professional reward distribution plot\n",
    "plt.figure(figsize=(14, 8), dpi=300)\n",
    "\n",
    "# Use a professional color palette\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_runs))\n",
    "\n",
    "# Plot each run's distribution\n",
    "for i, (rewards, color) in enumerate(zip(all_run_rewards, colors)):\n",
    "    sns.kdeplot(data=rewards, \n",
    "                color=color, \n",
    "                alpha=0.6, \n",
    "                linewidth=1.5,\n",
    "                label=f'Run {i+1}' if i < 5 else \"\")  # Only label first 5 to avoid clutter\n",
    "\n",
    "# Calculate overall statistics\n",
    "all_rewards_combined = np.concatenate(all_run_rewards)\n",
    "overall_mean = np.mean(all_rewards_combined)\n",
    "overall_std = np.std(all_rewards_combined)\n",
    "\n",
    "# Add overall mean line\n",
    "plt.axvline(overall_mean, color='red', linestyle='--', linewidth=3, \n",
    "           label=f'Overall Mean: {overall_mean:.2f}')\n",
    "\n",
    "# Styling\n",
    "plt.title('Distribution of Final Rewards Across All Runs', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Final Reward Values', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Density', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add statistics text box\n",
    "stats_text = f'Overall Statistics:\\n' \\\n",
    "             f'Mean: {overall_mean:.2f}  {overall_std:.2f}\\n' \\\n",
    "             f'Runs: {n_runs}\\n' \\\n",
    "             f'Total Trajectories: {n_runs * n_trajectories}'\n",
    "\n",
    "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, \n",
    "         verticalalignment='top', horizontalalignment='left',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'),\n",
    "         fontsize=12)\n",
    "\n",
    "# Improve legend\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reward_distributions_all_runs.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create statistics plot with error bars\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=300)\n",
    "fig.suptitle('Statistics Across Runs with Error Bars', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Plot 1: Mean with std as error bars\n",
    "axes[0,0].errorbar(range(1, n_runs+1), stats_df['mean'], \n",
    "                   yerr=stats_df['std'], fmt='o-', capsize=5, \n",
    "                   color='blue', ecolor='lightblue', linewidth=2, markersize=6)\n",
    "axes[0,0].set_title('Mean Final Reward by Run', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Run Number', fontsize=12)\n",
    "axes[0,0].set_ylabel('Mean Final Reward', fontsize=12)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Median with IQR as error bars\n",
    "iqr_lower = stats_df['median'] - stats_df['q25']\n",
    "iqr_upper = stats_df['q75'] - stats_df['median']\n",
    "axes[0,1].errorbar(range(1, n_runs+1), stats_df['median'], \n",
    "                   yerr=[iqr_lower, iqr_upper], fmt='s-', capsize=5,\n",
    "                   color='green', ecolor='lightgreen', linewidth=2, markersize=6)\n",
    "axes[0,1].set_title('Median Final Reward by Run', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Run Number', fontsize=12)\n",
    "axes[0,1].set_ylabel('Median Final Reward', fontsize=12)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Min and Max\n",
    "axes[1,0].plot(range(1, n_runs+1), stats_df['min'], 'v-', \n",
    "               color='red', label='Minimum', linewidth=2, markersize=6)\n",
    "axes[1,0].plot(range(1, n_runs+1), stats_df['max'], '^-', \n",
    "               color='orange', label='Maximum', linewidth=2, markersize=6)\n",
    "axes[1,0].fill_between(range(1, n_runs+1), stats_df['min'], stats_df['max'], \n",
    "                       alpha=0.2, color='gray')\n",
    "axes[1,0].set_title('Min/Max Final Reward by Run', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Run Number', fontsize=12)\n",
    "axes[1,0].set_ylabel('Final Reward', fontsize=12)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Standard deviation\n",
    "axes[1,1].bar(range(1, n_runs+1), stats_df['std'], \n",
    "              color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Standard Deviation by Run', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Run Number', fontsize=12)\n",
    "axes[1,1].set_ylabel('Standard Deviation', fontsize=12)\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add overall statistics as text\n",
    "overall_stats_text = f'Overall Statistics Across {n_runs} Runs:\\n' \\\n",
    "                    f'Mean of Means: {stats_df[\"mean\"].mean():.2f}  {stats_df[\"mean\"].std():.2f}\\n' \\\n",
    "                    f'Mean of Medians: {stats_df[\"median\"].mean():.2f}  {stats_df[\"median\"].std():.2f}\\n' \\\n",
    "                    f'Mean Std Dev: {stats_df[\"std\"].mean():.2f}  {stats_df[\"std\"].std():.2f}'\n",
    "\n",
    "plt.figtext(0.02, 0.02, overall_stats_text, \n",
    "           bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8),\n",
    "           fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistics_across_runs.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS ACROSS ALL RUNS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of runs: {n_runs}\")\n",
    "print(f\"Trajectories per run: {n_trajectories}\")\n",
    "print(f\"Total trajectories: {n_runs * n_trajectories}\")\n",
    "print(\"\\nMean Final Rewards:\")\n",
    "print(f\"  Overall mean: {stats_df['mean'].mean():.3f}  {stats_df['mean'].std():.3f}\")\n",
    "print(f\"  Min mean: {stats_df['mean'].min():.3f}\")\n",
    "print(f\"  Max mean: {stats_df['mean'].max():.3f}\")\n",
    "print(\"\\nMedian Final Rewards:\")\n",
    "print(f\"  Overall median: {stats_df['median'].mean():.3f}  {stats_df['median'].std():.3f}\")\n",
    "print(f\"  Min median: {stats_df['median'].min():.3f}\")\n",
    "print(f\"  Max median: {stats_df['median'].max():.3f}\")\n",
    "print(\"\\nStandard Deviations:\")\n",
    "print(f\"  Mean std: {stats_df['std'].mean():.3f}  {stats_df['std'].std():.3f}\")\n",
    "print(f\"  Min std: {stats_df['std'].min():.3f}\")\n",
    "print(f\"  Max std: {stats_df['std'].max():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to joblib like the others \n",
    "import joblib\n",
    "import os\n",
    "# make ndarray of trajectories\n",
    "trajectories = np.array(trajectories)\n",
    "rewards = np.array(rewards)\n",
    "all_actions = np.array(all_actions)\n",
    "# make ndarray of rewards\n",
    "results_dict = {\n",
    "    'trajectories': trajectories,\n",
    "    'rewards': rewards,\n",
    "    'actions': all_actions,\n",
    "    'last_states': np.array([traj[-1] for traj in trajectories]),\n",
    "    'feature_names': _\n",
    "}\n",
    "\n",
    "# Save the results\n",
    "output_path = r\"results/Ablation\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "output_file = os.path.join(output_path, 'gflownet_results_oracle_1.joblib')\n",
    "joblib.dump(results_dict, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import *\n",
    "# trajectories, rewards,all_actions,_ = simulate_trajectories(\n",
    "#         env_class=GeneralEnvironment,\n",
    "#         forward_model=forward_model,\n",
    "#         backward_model=backward_model,\n",
    "#         initial_state=initial_state,\n",
    "#         config=config,\n",
    "#         trajectory_length=trajectory_length,\n",
    "#         n_trajectories=n_trajectories,\n",
    "#         device=device,\n",
    "#         logger = logger,\n",
    "#         model_path = model_path,\n",
    "#         distribution=distribution,\n",
    "#         extra_parameters=extra_parameters\n",
    "\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_dict_1['gflow'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "config_path = fr'{root}/{run_id}/artifacts/run_params_config.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "initial_state,feature_names = load_initial_state(config)\n",
    "trajectory_length = config['model']['training_parameters']['trajectory_length']\n",
    "n_trajectories = 200\n",
    "input_dim = forward_model.input_layer.in_features\n",
    "model_path = config['oracle']['model_path']\n",
    "distribution = config['model']['training_parameters']['distribution_type']\n",
    "extra_parameters = {\n",
    "    'mixture_components': config['model']['training_parameters']['mixture_components'],\n",
    "    'num_variables': input_dim -1 ,\n",
    "}\n",
    "environment = GeneralEnvironment(\n",
    "    initial_state=initial_state,\n",
    "    config = config,\n",
    "    model = forward_model,\n",
    "    input_dim = forward_model.input_layer.in_features ,\n",
    "    max_steps=config['model']['training_parameters']['trajectory_length'],\n",
    "    model_path=config['oracle']['model_path'])\n",
    "\n",
    "# Create main output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_type = \"GFlowNet\"  # or \"REINFORCE\", \"SMCMC\", etc. GFlowNet REINFORCE_B\n",
    "use_case = 'O1'\n",
    "output_path = r\"results/PaperPlots_O1\"\n",
    "base_output_dir = fr\"results/Comparison/PaperPlots_{use_case}/analysis_{model_type}_{timestamp}\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories for different types of outputs\n",
    "plots_dir = os.path.join(base_output_dir, 'plots')\n",
    "metrics_dir = os.path.join(base_output_dir, 'metrics')\n",
    "action_plots_dir = os.path.join(base_output_dir, 'action_distributions')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "os.makedirs(action_plots_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "reinforce_sample = joblib.load(fr\"results/Ablation/gflownet_results_oracle_1.joblib\")\n",
    "# reinforce_sample = joblib.load(fr'results/reinforce_results_oracle_1.joblib')\n",
    "# reinforce_sample = joblib.load(fr'results/reinforce_baseline_results_oracle_1.joblib')\n",
    "# reinforce_sample = joblib.load(fr'results/sac_results_oracle_1.joblib')\n",
    "# reinforce_sample = joblib.load(fr'results/mcmc_results_oracle_{use_case[-1]}.joblib')\n",
    "trajectories = reinforce_sample['trajectories']\n",
    "rewards = reinforce_sample['rewards']\n",
    "# if model_type == 'SMCMC':\n",
    "#     all_actions = reinforce_sample['actions'][:,:,1:]\n",
    "# else:\n",
    "#     all_actions = reinforce_sample['actions']\n",
    "\n",
    "# Process rewards and predictions\n",
    "rewards = np.array(rewards) if isinstance(rewards, list) else rewards\n",
    "final_rewards = np.array([r[-1] for r in rewards])\n",
    "final_predictions = np.array([-r for r in final_rewards])\n",
    "\n",
    "# Get top 100 trajectories\n",
    "top_100_trajectories_indices = np.argsort(final_predictions)[:100]\n",
    "trajectories_top_100 = np.array([trajectories[i] for i in top_100_trajectories_indices])\n",
    "final_rewards_top_100 = np.array([final_rewards[i] for i in top_100_trajectories_indices])\n",
    "final_predictions_top_100 = np.array([final_predictions[i] for i in top_100_trajectories_indices])\n",
    "\n",
    "# Calculate and save metrics\n",
    "avg_reward = average_reward(final_rewards_top_100)\n",
    "tc_20 = tail_coverage(final_predictions_top_100, tau=-20)\n",
    "tc_40 = tail_coverage(final_predictions_top_100, tau=-40)\n",
    "tc_60 = tail_coverage(final_predictions_top_100, tau=-60)\n",
    "es_5= expected_shortfall(final_predictions_top_100, q=5)\n",
    "es_10= expected_shortfall(final_predictions_top_100, q=10)\n",
    "es_20= expected_shortfall(final_predictions_top_100, q=20)\n",
    "\n",
    "metrics_dict = {\n",
    "    'average_reward': avg_reward,\n",
    "    'tail_coverage_20': tc_20,\n",
    "    'tail_coverage_40': tc_40,\n",
    "    'tail_coverage_60': tc_60,\n",
    "    'expected_shortfall': es_5,\n",
    "    'expected_shortfall_10': es_10,\n",
    "    'expected_shortfall_20': es_20\n",
    "}\n",
    "\n",
    "# Save metrics to file\n",
    "with open(os.path.join(metrics_dir, 'performance_metrics.txt'), 'w') as f:\n",
    "    for metric, value in metrics_dict.items():\n",
    "        f.write(f\"{metric}: {value:.2f}\\n\")\n",
    "\n",
    "# Print out a summary of the trajectories and rewards\n",
    "rewards_dict = {\n",
    "    'min': min(final_rewards_top_100),\n",
    "    'max': max(final_rewards_top_100),\n",
    "    'mean': np.mean(final_rewards_top_100),\n",
    "    'std': np.std(final_rewards_top_100),\n",
    "    'median': np.median(final_rewards_top_100)\n",
    "}\n",
    "with open(os.path.join(metrics_dir, 'rewards_summary.txt'), 'w') as f:\n",
    "    for metric, value in rewards_dict.items():\n",
    "        f.write(f\"{metric}: {value:.2f}\\n\")\n",
    "  \n",
    "results_df, distance_matrix, feature_clusters = dtw_clustering_analysis(\n",
    "    trajectories=trajectories_top_100,\n",
    "    n_clusters=3\n",
    ")\n",
    "\n",
    "# Save clustering plots\n",
    "plot_trajectories_by_cluster(results_df, trajectories, ['Timestamp']+feature_names,\n",
    "                           num_clusters=3,\n",
    "                           save_path=os.path.join(plots_dir, 'cluster_trajectories.pdf'))\n",
    "\n",
    "\n",
    "res_dict_dtw = plot_dtw_distance_matrix(distance_matrix, model_name=f'{model_type}', \n",
    "                        rewards=final_rewards_top_100,\n",
    "                        save_path=os.path.join(plots_dir, 'dtw_distance_matrix.pdf'))\n",
    "\n",
    "avg_trajectory_distance = res_dict_dtw['average_distance']  \n",
    "normalized_score = res_dict_dtw['normalized_score']\n",
    "reward_norm = res_dict_dtw['reward_normalization']\n",
    "# Write metrics to file\n",
    "with open(os.path.join(metrics_dir, 'dtw_metrics.txt'), 'w') as f:\n",
    "    f.write(f\"Average Distance: {avg_trajectory_distance:.2f}\\n\")\n",
    "    f.write(f\"Normalized Score: {normalized_score:.4f}\\n\")\n",
    "    f.write(f\"Reward Normalization: {reward_norm:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Min-max trajectory comparisons\n",
    "min_pair, max_pair = find_min_max_distance_pairs(distance_matrix)\n",
    "compare_trajectories(min_pair[0], min_pair[1], trajectories_top_100, \n",
    "                    ['Timestamp']+feature_names, final_rewards_top_100,\n",
    "                    save_path=os.path.join(plots_dir, 'min_distance_pair.pdf'))\n",
    "compare_trajectories(max_pair[0], max_pair[1], trajectories_top_100, \n",
    "                    ['Timestamp']+feature_names, final_rewards_top_100,\n",
    "                    save_path=os.path.join(plots_dir, 'max_distance_pair.pdf'))\n",
    "\n",
    "# Trajectory and action distribution plots\n",
    "plot_trajectories_over_time(\n",
    "    trajectories=trajectories_top_100,\n",
    "    rewards=final_rewards_top_100,\n",
    "    feature_names=['Timestamp'] + feature_names,\n",
    "    n_top=50,\n",
    "    alpha_others=0.05,\n",
    "    save_path=plots_dir\n",
    ")\n",
    "\n",
    "plot_action_distributions_by_timestep(\n",
    "    trajectories=trajectories,\n",
    "    all_actions=all_actions,\n",
    "    feature_names=['Timestamp'] + feature_names, # Remove Timestamps for SMCMC\n",
    "    save_path=action_plots_dir\n",
    ")       \n",
    "\n",
    "plot_action_distributions(\n",
    "    all_actions=all_actions,\n",
    "    feature_names=['Timestamp'] + feature_names,\n",
    "    save_path=os.path.join(plots_dir, 'distribution_actions.pdf')\n",
    ")\n",
    "\n",
    "# Create and save summary DataFrame\n",
    "summary_rows = []\n",
    "for rank, idx in enumerate(top_100_trajectories_indices):\n",
    "    row_dict = {\n",
    "        \"Rank\": rank + 1,\n",
    "        \"CaseIndex\": idx,\n",
    "        \"FinalReward\": final_rewards[idx]\n",
    "    }\n",
    "    final_values = trajectories[idx, -1, :]\n",
    "    for f_i, f_name in enumerate(feature_names):\n",
    "        row_dict[f_name] = final_values[f_i]\n",
    "    summary_rows.append(row_dict)\n",
    "\n",
    "top100_df = pd.DataFrame(summary_rows).sort_values(by=\"FinalReward\", ascending=False)\n",
    "top100_df.to_csv(os.path.join(metrics_dir, 'top_100_summary.csv'), index=False)\n",
    "\n",
    "# Diversity metrics and plots\n",
    "avg_div, norm_div, distances = plot_diversity_metrics(\n",
    "    df=top100_df,\n",
    "    feature_names=feature_names,\n",
    "    save_path=os.path.join(plots_dir, 'euclidean_distance_matrix.pdf')\n",
    ")\n",
    "\n",
    "with open(os.path.join(metrics_dir, 'diversity_metrics.txt'), 'w') as f:\n",
    "    f.write(f\"Average Diversity - Last State: {avg_div:.2f}\\n\")\n",
    "    f.write(f\"Normalized Diversity - Last State: {norm_div:.4f}\\n\")\n",
    "\n",
    "# Final reward distribution plot\n",
    "plot_reward_distribution(final_rewards_top_100, \n",
    "                        save_path=os.path.join(plots_dir, 'reward_distribution.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_across_runs(model_results_dict, confidence_level=0.95, top_k=100):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics across multiple independent runs with confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        model_results_dict (dict): Dictionary with model results from 30 independent runs\n",
    "        confidence_level (float): Confidence level for intervals (default 0.95)\n",
    "        top_k (int): Number of top trajectories to consider (default 100)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive metrics with confidence intervals for each model\n",
    "    \"\"\"\n",
    "    from scipy.stats import t\n",
    "    import numpy as np\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, runs_dict in model_results_dict.items():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "        \n",
    "        # Lists to store metrics from each run\n",
    "        run_metrics = {\n",
    "            'avg_reward': [],\n",
    "            'max_reward': [],\n",
    "            'min_reward': [],\n",
    "            'median_reward': [],\n",
    "            'std_reward': [],\n",
    "            'tc_20': [],\n",
    "            'tc_40': [],\n",
    "            'tc_60': [],\n",
    "            'es_5': [],\n",
    "            'es_10': [],\n",
    "            'es_20': [],\n",
    "            'top_10_mean': [],\n",
    "            'top_50_mean': [],\n",
    "            'percentile_90': [],\n",
    "            'percentile_10': [],\n",
    "            'n_trajectories': []\n",
    "        }\n",
    "        \n",
    "        # Process each run\n",
    "        for run_idx in range(30):\n",
    "            try:\n",
    "                rewards = runs_dict[run_idx]['rewards']\n",
    "                \n",
    "                # Handle different reward formats\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                \n",
    "                # Extract final rewards\n",
    "                if rewards.ndim == 2:\n",
    "                    final_rewards = rewards[:, -1]\n",
    "                elif rewards.ndim == 1:\n",
    "                    final_rewards = rewards\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Remove non-finite values\n",
    "                final_rewards = final_rewards[np.isfinite(final_rewards)]\n",
    "                \n",
    "                if len(final_rewards) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get top K trajectories for this run\n",
    "                top_k_actual = min(top_k, len(final_rewards))\n",
    "                top_indices = np.argsort(final_rewards)[-top_k_actual:]\n",
    "                top_rewards = final_rewards[top_indices]\n",
    "                \n",
    "                # Calculate basic statistics\n",
    "                run_metrics['avg_reward'].append(np.mean(top_rewards))\n",
    "                run_metrics['max_reward'].append(np.max(top_rewards))\n",
    "                run_metrics['min_reward'].append(np.min(top_rewards))\n",
    "                run_metrics['median_reward'].append(np.median(top_rewards))\n",
    "                run_metrics['std_reward'].append(np.std(top_rewards))\n",
    "                run_metrics['n_trajectories'].append(len(final_rewards))\n",
    "                \n",
    "                # Calculate tail coverage (percentage <= threshold)\n",
    "                final_predictions = -top_rewards  # Convert to predictions (negative rewards)\n",
    "                run_metrics['tc_20'].append(tail_coverage(final_predictions, tau=-20))\n",
    "                run_metrics['tc_40'].append(tail_coverage(final_predictions, tau=-40))\n",
    "                run_metrics['tc_60'].append(tail_coverage(final_predictions, tau=-60))\n",
    "                \n",
    "                # Calculate expected shortfall\n",
    "                run_metrics['es_5'].append(expected_shortfall(final_predictions, q=5))\n",
    "                run_metrics['es_10'].append(expected_shortfall(final_predictions, q=10))\n",
    "                run_metrics['es_20'].append(expected_shortfall(final_predictions, q=20))\n",
    "                \n",
    "                # Calculate top percentile means\n",
    "                sorted_rewards = np.sort(top_rewards)[::-1]  # Sort descending\n",
    "                run_metrics['top_10_mean'].append(np.mean(sorted_rewards[:min(10, len(sorted_rewards))]))\n",
    "                run_metrics['top_50_mean'].append(np.mean(sorted_rewards[:min(50, len(sorted_rewards))]))\n",
    "                \n",
    "                # Calculate percentiles\n",
    "                run_metrics['percentile_90'].append(np.percentile(top_rewards, 90))\n",
    "                run_metrics['percentile_10'].append(np.percentile(top_rewards, 10))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_name} run {run_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate statistics across runs with confidence intervals\n",
    "        model_results = {}\n",
    "        \n",
    "        for metric_name, values in run_metrics.items():\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            values = np.array(values)\n",
    "            values = values[np.isfinite(values)]  # Remove any NaN values\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_runs = len(values)\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values, ddof=1)  # Sample standard deviation\n",
    "            \n",
    "            # Calculate confidence intervals using t-distribution\n",
    "            if n_runs > 1:\n",
    "                t_critical = t.ppf((1 + confidence_level) / 2, df=n_runs-1)\n",
    "                margin_error = t_critical * (std_val / np.sqrt(n_runs))\n",
    "                ci_lower = mean_val - margin_error\n",
    "                ci_upper = mean_val + margin_error\n",
    "            else:\n",
    "                ci_lower = ci_upper = mean_val\n",
    "            \n",
    "            model_results[metric_name] = {\n",
    "                'mean': mean_val,\n",
    "                'std': std_val,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'n_runs': n_runs,\n",
    "                'raw_values': values.tolist()\n",
    "            }\n",
    "        \n",
    "        results[model_name] = model_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_metrics_to_files(metrics_results, save_dir, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Save metrics results to various file formats for easy access.\n",
    "    \n",
    "    Args:\n",
    "        metrics_results (dict): Results from calculate_metrics_across_runs\n",
    "        save_dir (str): Directory to save files\n",
    "        confidence_level (float): Confidence level used\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save comprehensive summary as CSV\n",
    "    summary_data = []\n",
    "    for model_name, model_metrics in metrics_results.items():\n",
    "        row = {'Model': model_name}\n",
    "        for metric_name, metric_data in model_metrics.items():\n",
    "            if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                row[f'{metric_name}_mean'] = metric_data['mean']\n",
    "                row[f'{metric_name}_std'] = metric_data['std']\n",
    "                row[f'{metric_name}_ci_lower'] = metric_data['ci_lower']\n",
    "                row[f'{metric_name}_ci_upper'] = metric_data['ci_upper']\n",
    "                row[f'{metric_name}_n_runs'] = metric_data['n_runs']\n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(save_dir, 'metrics_summary.csv'), index=False)\n",
    "    \n",
    "    # 2. Save formatted results for papers/reports\n",
    "    with open(os.path.join(save_dir, 'metrics_formatted.txt'), 'w') as f:\n",
    "        f.write(f\"PERFORMANCE METRICS WITH {int(confidence_level*100)}% CONFIDENCE INTERVALS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for model_name, model_metrics in metrics_results.items():\n",
    "            f.write(f\"{model_name.upper()}:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            # Key metrics in formatted way\n",
    "            key_metrics = ['avg_reward', 'max_reward', 'tc_20', 'es_10', 'es_20']\n",
    "            for metric in key_metrics:\n",
    "                if metric in model_metrics:\n",
    "                    data = model_metrics[metric]\n",
    "                    f.write(f\"{metric:15}: {data['mean']:7.2f}  {data['std']:5.2f} \"\n",
    "                           f\"[{data['ci_lower']:6.2f}, {data['ci_upper']:6.2f}] \"\n",
    "                           f\"(n={data['n_runs']})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # 3. Save LaTeX table format\n",
    "    with open(os.path.join(save_dir, 'metrics_latex_table.txt'), 'w') as f:\n",
    "        f.write(\"% LaTeX Table Format\\n\")\n",
    "        f.write(\"\\\\begin{table}[htbp]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{Performance Metrics with Confidence Intervals}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{l|ccccc}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Method & Avg Reward & Max Reward & TC@20 & ES@10\\\\% & ES@20\\\\% \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        \n",
    "        for model_name, model_metrics in metrics_results.items():\n",
    "            row = f\"{model_name:<15}\"\n",
    "            key_metrics = ['avg_reward', 'max_reward', 'tc_20', 'es_10', 'es_20']\n",
    "            for metric in key_metrics:\n",
    "                if metric in model_metrics:\n",
    "                    data = model_metrics[metric]\n",
    "                    row += f\" & ${data['mean']:5.1f} \\\\pm {data['std']:4.1f}$\"\n",
    "                else:\n",
    "                    row += \" & -\"\n",
    "            row += \" \\\\\\\\\\n\"\n",
    "            f.write(row)\n",
    "        \n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    \n",
    "    # 4. Save raw data as JSON for programmatic access\n",
    "    with open(os.path.join(save_dir, 'metrics_raw.json'), 'w') as f:\n",
    "        json.dump(metrics_results, f, indent=2)\n",
    "    \n",
    "    # 5. Save comparison table\n",
    "    comparison_df = pd.DataFrame()\n",
    "    for model_name, model_metrics in metrics_results.items():\n",
    "        model_row = {}\n",
    "        for metric_name, metric_data in model_metrics.items():\n",
    "            if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                model_row[metric_name] = f\"{metric_data['mean']:.2f}  {metric_data['std']:.2f}\"\n",
    "        comparison_df[model_name] = pd.Series(model_row)\n",
    "    \n",
    "    comparison_df.to_csv(os.path.join(save_dir, 'metrics_comparison.csv'))\n",
    "    \n",
    "    print(f\"Metrics saved to {save_dir}/\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"  - metrics_summary.csv: Complete numerical data\")\n",
    "    print(\"  - metrics_formatted.txt: Human-readable format\")\n",
    "    print(\"  - metrics_latex_table.txt: LaTeX table format\")\n",
    "    print(\"  - metrics_raw.json: Raw data for programming\")\n",
    "    print(\"  - metrics_comparison.csv: Side-by-side comparison\")\n",
    "\n",
    "def plot_metrics_comparison(metrics_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-ready plots comparing metrics across models.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Key metrics to plot\n",
    "    key_metrics = {\n",
    "        'avg_reward': 'Average Reward',\n",
    "        'max_reward': 'Maximum Reward', \n",
    "        'tc_20': 'Tail Coverage @ -20',\n",
    "        'es_10': 'Expected Shortfall @ 10%',\n",
    "        'es_20': 'Expected Shortfall @ 20%'\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12), dpi=300)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    for idx, (metric_key, metric_name) in enumerate(key_metrics.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        models = []\n",
    "        means = []\n",
    "        errors = []\n",
    "        \n",
    "        for model_name, model_metrics in metrics_results.items():\n",
    "            if metric_key in model_metrics:\n",
    "                models.append(model_name.upper())\n",
    "                data = model_metrics[metric_key]\n",
    "                means.append(data['mean'])\n",
    "                # Use confidence interval width as error\n",
    "                error = (data['ci_upper'] - data['ci_lower']) / 2\n",
    "                errors.append(error)\n",
    "        \n",
    "        if means:\n",
    "            bars = ax.bar(range(len(models)), means, yerr=errors, \n",
    "                         capsize=10, alpha=0.7, \n",
    "                         color=[colors.get(m.lower(), 'gray') for m in models])\n",
    "            \n",
    "            ax.set_title(metric_name, fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(range(len(models)))\n",
    "            ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, mean, error in zip(bars, means, errors):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + error,\n",
    "                       f'{mean:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(key_metrics) < len(axes):\n",
    "        fig.delaxes(axes[-1])\n",
    "    \n",
    "    plt.suptitle('Performance Metrics Comparison with Confidence Intervals\\n'\n",
    "                'Error bars show 95% confidence intervals', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage example:\n",
    "print(\"Calculating metrics across all runs...\")\n",
    "metrics_results = calculate_metrics_across_runs(\n",
    "    model_results_dict_3, \n",
    "    confidence_level=0.95, \n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_dir = \"results/Comparison/metrics_analysis_3\"\n",
    "save_metrics_to_files(metrics_results, save_dir)\n",
    "\n",
    "# Create comparison plots\n",
    "plot_metrics_comparison(metrics_results, \n",
    "                       save_path=os.path.join(save_dir, 'metrics_comparison_plot.pdf'))\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METRICS CALCULATION COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "for model_name, model_metrics in metrics_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if 'avg_reward' in model_metrics:\n",
    "        data = model_metrics['avg_reward']\n",
    "        print(f\"  Average Reward: {data['mean']:.2f}  {data['std']:.2f} \"\n",
    "              f\"[{data['ci_lower']:.2f}, {data['ci_upper']:.2f}]\")\n",
    "    if 'tc_20' in model_metrics:\n",
    "        data = model_metrics['tc_20']\n",
    "        print(f\"  Tail Coverage @ -20: {data['mean']:.2f}  {data['std']:.2f}\")\n",
    "    if 'es_10' in model_metrics:\n",
    "        data = model_metrics['es_10']\n",
    "        print(f\"  Expected Shortfall @ 10%: {data['mean']:.2f}  {data['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_dict_1['gflow'][0]['trajectories'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_dict_1['reinforce'][0]['trajectories'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_metrics_across_runs(model_results_dict, confidence_level=0.95, top_k=100):\n",
    "    \"\"\"\n",
    "    Calculate diversity metrics across multiple independent runs with confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        model_results_dict (dict): Dictionary with model results from 30 independent runs\n",
    "        confidence_level (float): Confidence level for intervals (default 0.95)\n",
    "        top_k (int): Number of top trajectories to consider (default 100)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive diversity metrics with confidence intervals for each model\n",
    "    \"\"\"\n",
    "    from scipy.stats import t\n",
    "    import numpy as np\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, runs_dict in model_results_dict.items():\n",
    "        print(f\"Processing diversity metrics for {model_name}...\")\n",
    "        \n",
    "        # Lists to store diversity metrics from each run\n",
    "        run_diversity_metrics = {\n",
    "            'avg_dtw_distance': [],\n",
    "            'normalized_dtw_score': [],\n",
    "            'avg_euclidean_diversity': [],\n",
    "            'normalized_euclidean_diversity': [],\n",
    "            'dtw_reward_stability': [],\n",
    "            'euclidean_reward_stability': [],\n",
    "            'coverage_eps_01': [],\n",
    "            'coverage_eps_05': [],\n",
    "            'coverage_eps_10': []\n",
    "        }\n",
    "        \n",
    "        # Process each run\n",
    "        for run_idx in range(30):\n",
    "            try:\n",
    "                # Get data for this run\n",
    "                trajectories = runs_dict[run_idx]['trajectories']\n",
    "                rewards = runs_dict[run_idx]['rewards']\n",
    "                \n",
    "                # Handle different reward formats\n",
    "                if isinstance(rewards, (list, tuple)):\n",
    "                    rewards = np.array(rewards, dtype=float)\n",
    "                elif isinstance(rewards, np.ndarray):\n",
    "                    rewards = rewards.astype(float)\n",
    "                \n",
    "                # Extract final rewards\n",
    "                if rewards.ndim == 2:\n",
    "                    final_rewards = rewards[:, -1]\n",
    "                elif rewards.ndim == 1:\n",
    "                    final_rewards = rewards\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Remove non-finite values\n",
    "                final_rewards = final_rewards[np.isfinite(final_rewards)]\n",
    "                \n",
    "                if len(final_rewards) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get top K trajectories for this run\n",
    "                top_k_actual = min(top_k, len(final_rewards))\n",
    "                top_indices = np.argsort(final_rewards)[-top_k_actual:]\n",
    "                top_trajectories = np.array([trajectories[i] for i in top_indices])\n",
    "                top_rewards = final_rewards[top_indices]\n",
    "                \n",
    "                # Calculate DTW diversity metrics\n",
    "                try:\n",
    "                    distance_matrix, feature_dist_matrices = compute_dtw_distance_matrix(top_trajectories)\n",
    "                    avg_dtw_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "                    \n",
    "                    # Calculate reward stability for DTW\n",
    "                    max_r = np.max(top_rewards)\n",
    "                    min_r = np.min(top_rewards)\n",
    "                    reward_range = np.abs(max_r - min_r)\n",
    "                    reward_scale = np.abs(max_r) + np.abs(min_r) + 1e-8\n",
    "                    dtw_reward_stability = 1 - (reward_range / reward_scale)\n",
    "                    normalized_dtw_score = avg_dtw_distance * dtw_reward_stability\n",
    "                    \n",
    "                    run_diversity_metrics['avg_dtw_distance'].append(avg_dtw_distance)\n",
    "                    run_diversity_metrics['normalized_dtw_score'].append(normalized_dtw_score)\n",
    "                    run_diversity_metrics['dtw_reward_stability'].append(dtw_reward_stability)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"DTW calculation failed for {model_name} run {run_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate Euclidean diversity metrics on final states\n",
    "                try:\n",
    "                    if isinstance(top_trajectories, list):\n",
    "                        top_trajectories = np.array(top_trajectories)\n",
    "                    final_states = top_trajectories[:, -1, 1:]  # Last timestep, exclude timestamp\n",
    "                    # Make sure rewards and states are not of object type and without nans or infs\n",
    "                    final_states = final_states.astype(float)\n",
    "                    final_states = final_states[np.isfinite(final_states).all(axis=1)]\n",
    "                    top_rewards = top_rewards[np.isfinite(final_states).all(axis=1)]\n",
    "                    if len(final_states) == 0 or len(top_rewards) == 0:\n",
    "                        continue\n",
    "                    euclidean_res = calculate_quality_diversity(final_states, top_rewards)\n",
    "                    \n",
    "                    avg_euclidean_diversity = euclidean_res['average_diversity']\n",
    "                    normalized_euclidean_score = euclidean_res['normalized_score']\n",
    "                    \n",
    "                    # Calculate euclidean reward stability\n",
    "                    euclidean_reward_stability = 1 - (reward_range / reward_scale)\n",
    "                    \n",
    "                    run_diversity_metrics['avg_euclidean_diversity'].append(avg_euclidean_diversity)\n",
    "                    run_diversity_metrics['normalized_euclidean_diversity'].append(normalized_euclidean_score)\n",
    "                    run_diversity_metrics['euclidean_reward_stability'].append(euclidean_reward_stability)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Euclidean diversity calculation failed for {model_name} run {run_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate coverage metrics at different epsilon values\n",
    "                try:\n",
    "                    for eps, key in [(0.1, 'coverage_eps_01'), (0.5, 'coverage_eps_05'), (1.0, 'coverage_eps_10')]:\n",
    "                        coverage = coverage_epsilon(final_states, eps)\n",
    "                        run_diversity_metrics[key].append(coverage)\n",
    "                except Exception as e:\n",
    "                    print(f\"Coverage calculation failed for {model_name} run {run_idx}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_name} run {run_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate statistics across runs with confidence intervals\n",
    "        model_diversity_results = {}\n",
    "        \n",
    "        for metric_name, values in run_diversity_metrics.items():\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            values = np.array(values)\n",
    "            values = values[np.isfinite(values)]  # Remove any NaN values\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_runs = len(values)\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values, ddof=1)  # Sample standard deviation\n",
    "            \n",
    "            # Calculate confidence intervals using t-distribution\n",
    "            if n_runs > 1:\n",
    "                t_critical = t.ppf((1 + confidence_level) / 2, df=n_runs-1)\n",
    "                margin_error = t_critical * (std_val / np.sqrt(n_runs))\n",
    "                ci_lower = mean_val - margin_error\n",
    "                ci_upper = mean_val + margin_error\n",
    "            else:\n",
    "                ci_lower = ci_upper = mean_val\n",
    "            \n",
    "            model_diversity_results[metric_name] = {\n",
    "                'mean': mean_val,\n",
    "                'std': std_val,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'n_runs': n_runs,\n",
    "                'raw_values': values.tolist()\n",
    "            }\n",
    "        \n",
    "        results[model_name] = model_diversity_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_diversity_metrics_to_files(diversity_results, save_dir, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Save diversity metrics results to various file formats.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save comprehensive summary as CSV\n",
    "    summary_data = []\n",
    "    for model_name, model_metrics in diversity_results.items():\n",
    "        row = {'Model': model_name}\n",
    "        for metric_name, metric_data in model_metrics.items():\n",
    "            if isinstance(metric_data, dict) and 'mean' in metric_data:\n",
    "                row[f'{metric_name}_mean'] = metric_data['mean']\n",
    "                row[f'{metric_name}_std'] = metric_data['std']\n",
    "                row[f'{metric_name}_ci_lower'] = metric_data['ci_lower']\n",
    "                row[f'{metric_name}_ci_upper'] = metric_data['ci_upper']\n",
    "                row[f'{metric_name}_n_runs'] = metric_data['n_runs']\n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(os.path.join(save_dir, 'diversity_metrics_summary.csv'), index=False)\n",
    "    \n",
    "    # 2. Save formatted results for papers/reports\n",
    "    with open(os.path.join(save_dir, 'diversity_metrics_formatted.txt'), 'w') as f:\n",
    "        f.write(f\"DIVERSITY METRICS WITH {int(confidence_level*100)}% CONFIDENCE INTERVALS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        for model_name, model_metrics in diversity_results.items():\n",
    "            f.write(f\"{model_name.upper()}:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            # Key diversity metrics in formatted way\n",
    "            key_metrics = ['avg_dtw_distance', 'normalized_dtw_score', 'avg_euclidean_diversity', \n",
    "                          'normalized_euclidean_diversity', 'coverage_eps_05']\n",
    "            for metric in key_metrics:\n",
    "                if metric in model_metrics:\n",
    "                    data = model_metrics[metric]\n",
    "                    f.write(f\"{metric:25}: {data['mean']:7.3f}  {data['std']:5.3f} \"\n",
    "                           f\"[{data['ci_lower']:6.3f}, {data['ci_upper']:6.3f}] \"\n",
    "                           f\"(n={data['n_runs']})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # 3. Save LaTeX table format for diversity metrics\n",
    "    with open(os.path.join(save_dir, 'diversity_metrics_latex_table.txt'), 'w') as f:\n",
    "        f.write(\"% LaTeX Table Format for Diversity Metrics\\n\")\n",
    "        f.write(\"\\\\begin{table}[htbp]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{Diversity Metrics with Confidence Intervals}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{l|ccccc}\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"Method & DTW Distance & DTW Normalized & Euclidean Div & Euclidean Norm & Coverage@0.5 \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        \n",
    "        for model_name, model_metrics in diversity_results.items():\n",
    "            row = f\"{model_name:<15}\"\n",
    "            key_metrics = ['avg_dtw_distance', 'normalized_dtw_score', 'avg_euclidean_diversity', \n",
    "                          'normalized_euclidean_diversity', 'coverage_eps_05']\n",
    "            for metric in key_metrics:\n",
    "                if metric in model_metrics:\n",
    "                    data = model_metrics[metric]\n",
    "                    row += f\" & ${data['mean']:5.2f} \\\\pm {data['std']:4.2f}$\"\n",
    "                else:\n",
    "                    row += \" & -\"\n",
    "            row += \" \\\\\\\\\\n\"\n",
    "            f.write(row)\n",
    "        \n",
    "        f.write(\"\\\\hline\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    \n",
    "    # 4. Save raw data as JSON\n",
    "    with open(os.path.join(save_dir, 'diversity_metrics_raw.json'), 'w') as f:\n",
    "        json.dump(diversity_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Diversity metrics saved to {save_dir}/\")\n",
    "\n",
    "def plot_diversity_metrics_comparison(diversity_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Create publication-ready plots comparing diversity metrics across models.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Key diversity metrics to plot\n",
    "    key_metrics = {\n",
    "        'avg_dtw_distance': 'Average DTW Distance',\n",
    "        'normalized_dtw_score': 'Normalized DTW Score',\n",
    "        'avg_euclidean_diversity': 'Average Euclidean Diversity',\n",
    "        'normalized_euclidean_diversity': 'Normalized Euclidean Diversity',\n",
    "        'coverage_eps_05': 'Coverage @ =0.5'\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12), dpi=300)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = {'gflow': '#2ecc71', 'reinforce': '#3498db', 'reinforce_baseline': '#f39c12', \n",
    "              'sac': '#9b59b6', 'smcmc': '#e74c3c'}\n",
    "    \n",
    "    for idx, (metric_key, metric_name) in enumerate(key_metrics.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        models = []\n",
    "        means = []\n",
    "        errors = []\n",
    "        \n",
    "        for model_name, model_metrics in diversity_results.items():\n",
    "            if metric_key in model_metrics:\n",
    "                models.append(model_name.upper())\n",
    "                data = model_metrics[metric_key]\n",
    "                means.append(data['mean'])\n",
    "                # Use confidence interval width as error\n",
    "                error = (data['ci_upper'] - data['ci_lower']) / 2\n",
    "                errors.append(error)\n",
    "        \n",
    "        if means:\n",
    "            bars = ax.bar(range(len(models)), means, yerr=errors, \n",
    "                         capsize=10, alpha=0.7, \n",
    "                         color=[colors.get(m.lower(), 'gray') for m in models])\n",
    "            \n",
    "            ax.set_title(metric_name, fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(range(len(models)))\n",
    "            ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, mean, error in zip(bars, means, errors):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + error,\n",
    "                       f'{mean:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(key_metrics) < len(axes):\n",
    "        fig.delaxes(axes[-1])\n",
    "    \n",
    "    plt.suptitle('Diversity Metrics Comparison with Confidence Intervals\\n'\n",
    "                'Error bars show 95% confidence intervals', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage example:\n",
    "print(\"Calculating diversity metrics across all runs...\")\n",
    "diversity_results = calculate_diversity_metrics_across_runs(\n",
    "    model_results_dict_1, \n",
    "    confidence_level=0.95, \n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "# Save results\n",
    "diversity_save_dir = \"results/Comparison/diversity_analysis_1\"\n",
    "save_diversity_metrics_to_files(diversity_results, diversity_save_dir)\n",
    "\n",
    "# Create comparison plots\n",
    "plot_diversity_metrics_comparison(diversity_results, \n",
    "                                save_path=os.path.join(diversity_save_dir, 'diversity_comparison_plot.pdf'))\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIVERSITY METRICS CALCULATION COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "for model_name, model_metrics in diversity_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if 'avg_dtw_distance' in model_metrics:\n",
    "        data = model_metrics['avg_dtw_distance']\n",
    "        print(f\"  Average DTW Distance: {data['mean']:.3f}  {data['std']:.3f} \"\n",
    "              f\"[{data['ci_lower']:.3f}, {data['ci_upper']:.3f}]\")\n",
    "    if 'normalized_dtw_score' in model_metrics:\n",
    "        data = model_metrics['normalized_dtw_score']\n",
    "        print(f\"  Normalized DTW Score: {data['mean']:.3f}  {data['std']:.3f}\")\n",
    "    if 'avg_euclidean_diversity' in model_metrics:\n",
    "        data = model_metrics['avg_euclidean_diversity']\n",
    "        print(f\"  Average Euclidean Diversity: {data['mean']:.3f}  {data['std']:.3f}\")\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "# Usage example:\n",
    "print(\"Calculating diversity metrics across all runs...\")\n",
    "diversity_results = calculate_diversity_metrics_across_runs(\n",
    "    model_results_dict_2, \n",
    "    confidence_level=0.95, \n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "# Save results\n",
    "diversity_save_dir = \"results/Comparison/diversity_analysis_2\"\n",
    "save_diversity_metrics_to_files(diversity_results, diversity_save_dir)\n",
    "\n",
    "# Create comparison plots\n",
    "plot_diversity_metrics_comparison(diversity_results, \n",
    "                                save_path=os.path.join(diversity_save_dir, 'diversity_comparison_plot.pdf'))\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIVERSITY METRICS CALCULATION COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "for model_name, model_metrics in diversity_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if 'avg_dtw_distance' in model_metrics:\n",
    "        data = model_metrics['avg_dtw_distance']\n",
    "        print(f\"  Average DTW Distance: {data['mean']:.3f}  {data['std']:.3f} \"\n",
    "              f\"[{data['ci_lower']:.3f}, {data['ci_upper']:.3f}]\")\n",
    "    if 'normalized_dtw_score' in model_metrics:\n",
    "        data = model_metrics['normalized_dtw_score']\n",
    "        print(f\"  Normalized DTW Score: {data['mean']:.3f}  {data['std']:.3f}\")\n",
    "    if 'avg_euclidean_diversity' in model_metrics:\n",
    "        data = model_metrics['avg_euclidean_diversity']\n",
    "        print(f\"  Average Euclidean Diversity: {data['mean']:.3f}  {data['std']:.3f}\")\n",
    "\n",
    "##\n",
    "\n",
    "# Usage example:\n",
    "print(\"Calculating diversity metrics across all runs...\")\n",
    "diversity_results = calculate_diversity_metrics_across_runs(\n",
    "    model_results_dict_3, \n",
    "    confidence_level=0.95, \n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "# Save results\n",
    "diversity_save_dir = \"results/Comparison/diversity_analysis_3\"\n",
    "save_diversity_metrics_to_files(diversity_results, diversity_save_dir)\n",
    "\n",
    "# Create comparison plots\n",
    "plot_diversity_metrics_comparison(diversity_results, \n",
    "                                save_path=os.path.join(diversity_save_dir, 'diversity_comparison_plot.pdf'))\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIVERSITY METRICS CALCULATION COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "for model_name, model_metrics in diversity_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if 'avg_dtw_distance' in model_metrics:\n",
    "        data = model_metrics['avg_dtw_distance']\n",
    "        print(f\"  Average DTW Distance: {data['mean']:.3f}  {data['std']:.3f} \"\n",
    "              f\"[{data['ci_lower']:.3f}, {data['ci_upper']:.3f}]\")\n",
    "    if 'normalized_dtw_score' in model_metrics:\n",
    "        data = model_metrics['normalized_dtw_score']\n",
    "        print(f\"  Normalized DTW Score: {data['mean']:.3f}  {data['std']:.3f}\")\n",
    "    if 'avg_euclidean_diversity' in model_metrics:\n",
    "        data = model_metrics['avg_euclidean_diversity']\n",
    "        print(f\"  Average Euclidean Diversity: {data['mean']:.3f}  {data['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all final rewards data from different models - O2\n",
    "smcmc = pd.read_csv(fr\"results/Comparison/PaperPlots_O2/analysis_SMCMC_20250507_223523/metrics/top_100_summary.csv\")\n",
    "sac = pd.read_csv(\"results/Comparison/PaperPlots_O2/analysis_SAC_20250506_211020/metrics/top_100_summary.csv\")\n",
    "reinforce_with_baseline = pd.read_csv(fr\"results/Comparison/PaperPlots_O2/analysis_REINFORCE_B_20250506_210844/metrics/top_100_summary.csv\")\n",
    "reinforce = pd.read_csv(fr\"results/Comparison/PaperPlots_O2/analysis_REINFORCE_20250506_210720/metrics/top_100_summary.csv\")\n",
    "gflownet = pd.read_csv(fr\"results/Comparison/PaperPlots_O2/analysis_GFlowNet_20250506_211515/metrics/top_100_summary.csv\")\n",
    "smcmc = smcmc.clip(upper=100,lower=0)\n",
    "reinforce = reinforce.clip(upper=100,lower=0)\n",
    "gflownet = gflownet.clip(upper=100,lower=0)\n",
    "sac = sac.clip(upper=100,lower=0)\n",
    "reinforce_with_baseline = reinforce_with_baseline.clip(upper=100,lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O1\n",
    "# Load all final rewards data from different models - O2\n",
    "smcmc = pd.read_csv(fr\"results/Comparison/PaperPlots_O1/analysis_SMCMC_20250507_224955/metrics/top_100_summary.csv\")\n",
    "sac = pd.read_csv(\"results/Comparison/PaperPlots_O1/analysis_SAC_20250506_211947/metrics/top_100_summary.csv\")\n",
    "reinforce_with_baseline = pd.read_csv(fr\"results/Comparison/PaperPlots_O1/analysis_REINFORCE_B_20250506_212549/metrics/top_100_summary.csv\")\n",
    "reinforce = pd.read_csv(fr\"results/Comparison/PaperPlots_O1/analysis_REINFORCE_20250506_212417/metrics/top_100_summary.csv\")\n",
    "gflownet = pd.read_csv(fr\"results/Comparison/PaperPlots_O1/analysis_GFlowNet_20250506_211812/metrics/top_100_summary.csv\")\n",
    "smcmc = smcmc.clip(upper=100,lower=0)\n",
    "reinforce = reinforce.clip(upper=100,lower=0)\n",
    "gflownet = gflownet.clip(upper=100,lower=0)\n",
    "sac = sac.clip(upper=100,lower=0)\n",
    "reinforce_with_baseline = reinforce_with_baseline.clip(upper=100,lower=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "# Define better color scheme\n",
    "colors = {\n",
    "    'GFlowNet': '#2ecc71',      # Emerald Green\n",
    "    'SMCMC': '#e74c3c',         # Pomegranate Red\n",
    "    'REINFORCE': '#3498db',      # Peter River Blue\n",
    "    'SAC': '#9b59b6',           # Amethyst Purple\n",
    "    'REINFORCE_B': '#f39c12'    # Orange\n",
    "}\n",
    "\n",
    "# Plot distributions with enhanced styling and convert to percentages\n",
    "for name, data, color in [\n",
    "    ('GFlowNet', gflownet['FinalReward'], colors['GFlowNet']),\n",
    "    ('SMCMC', smcmc['FinalReward'], colors['SMCMC']),\n",
    "    ('REINFORCE', reinforce['FinalReward'], colors['REINFORCE']),\n",
    "    ('SAC', sac['FinalReward'], colors['SAC']),\n",
    "    ('REINFORCE with Baseline', reinforce_with_baseline['FinalReward'], colors['REINFORCE_B'])\n",
    "]:\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    label = f'{name} (={mean:.1f}{std:.1f})'\n",
    "    # Multiply by 100 to convert to percentage\n",
    "    sns.kdeplot(data=data, \n",
    "                label=label, \n",
    "                color=color,\n",
    "                linewidth=3.5,\n",
    "                alpha=0.7)\n",
    "\n",
    "# Add vertical lines for means with shaded std regions\n",
    "for data, color in [\n",
    "    (gflownet['FinalReward'], colors['GFlowNet']),\n",
    "    (smcmc['FinalReward'], colors['SMCMC']),\n",
    "    (reinforce['FinalReward'], colors['REINFORCE']),\n",
    "    (sac['FinalReward'], colors['SAC']),\n",
    "    (reinforce_with_baseline['FinalReward'], colors['REINFORCE_B'])\n",
    "]:\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    plt.axvline(mean, color=color, linestyle='--', alpha=0.5, linewidth=2)\n",
    "    plt.axvspan(mean-std, mean+std, color=color, alpha=0.1)\n",
    "\n",
    "# Customize the plot with larger text\n",
    "plt.title('Comparison of Final Rewards Distributions',\n",
    "          fontsize=24, \n",
    "          pad=20,\n",
    "          fontweight='bold')\n",
    "plt.xlabel('Final Reward Values', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Density (%)', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Enhance grid\n",
    "plt.grid(True, alpha=0.2, linestyle='--')\n",
    "\n",
    "# Improve legend with larger text\n",
    "plt.legend(title='Methods',\n",
    "          title_fontsize=18,\n",
    "          fontsize=16,\n",
    "          loc='upper left',\n",
    "          frameon=True,\n",
    "          framealpha=0.9,\n",
    "          edgecolor='black')\n",
    "\n",
    "# Increase tick label size\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Set background style\n",
    "plt.gca().set_facecolor('white')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save with high quality settings\n",
    "plt.savefig('final_rewards_comparison.pdf', \n",
    "            dpi=600, \n",
    "            bbox_inches='tight',\n",
    "            format='pdf',\n",
    "            metadata={'Creator': 'Python'})\n",
    "\n",
    "plt.savefig('final_rewards_comparison.png', \n",
    "            dpi=600, \n",
    "            bbox_inches='tight',\n",
    "            format='png',\n",
    "            metadata={'Creator': 'Python'})\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gflow = pd.read_csv(r\"results/Comparison/PaperPlots_O1/analysis_GFlowNet_20250506_211812/metrics/top_100_summary.csv\")\n",
    "sac = pd.read_csv(r\"results/Comparison/PaperPlots_O1/analysis_SAC_20250506_211947/metrics/top_100_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gflow['FinalReward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac['FinalReward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac[sac['FinalReward']<gflow['FinalReward'].min()].shape[0] / sac.shape[0], gflow[gflow['FinalReward'].min()<sac['FinalReward']].shape[0] / gflow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_trajectories(reference_trajectory, trajectories, feature_names, n_closest=5, save_path=None):\n",
    "    \"\"\"\n",
    "    Find trajectories most similar to a reference trajectory using DTW distance.\n",
    "    \n",
    "    Args:\n",
    "        reference_trajectory (np.ndarray): The reference trajectory to compare against\n",
    "        trajectories (np.ndarray): Array of trajectories to compare\n",
    "        feature_names (list): List of feature names\n",
    "        n_closest (int): Number of closest trajectories to return\n",
    "        save_path (str): Optional path to save visualizations\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (similar_indices, distances, similarity_stats)\n",
    "    \"\"\"\n",
    "    n_trajectories = len(trajectories)\n",
    "    distances = []\n",
    "    \n",
    "    # Calculate DTW distance for each trajectory\n",
    "    for i in range(n_trajectories):\n",
    "        dist_matrix, _ = compute_dtw_distance_matrix(\n",
    "            np.array([reference_trajectory, trajectories[i]])\n",
    "        )\n",
    "        distances.append(dist_matrix[0,1])  # Get distance between reference and current trajectory\n",
    "    \n",
    "    # Convert to numpy array for easier manipulation\n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    # Get indices of n closest trajectories\n",
    "    similar_indices = np.argsort(distances)[:n_closest]\n",
    "    \n",
    "    # Calculate similarity statistics\n",
    "    similarity_stats = {\n",
    "        'mean_distance': np.mean(distances),\n",
    "        'std_distance': np.std(distances),\n",
    "        'min_distance': np.min(distances),\n",
    "        'max_distance': np.max(distances),\n",
    "        'median_distance': np.median(distances)\n",
    "    }\n",
    "    \n",
    "    # Plot comparison of reference with closest trajectories\n",
    "    if save_path:\n",
    "        for idx, similar_idx in enumerate(similar_indices):\n",
    "            # Plot each feature separately\n",
    "            n_features = reference_trajectory.shape[1] - 1  # Exclude timestamp column\n",
    "            fig, axes = plt.subplots(n_features, 1, figsize=(15, 4*n_features))\n",
    "            \n",
    "            # Get timestamps\n",
    "            timestamps = reference_trajectory[:, 0]\n",
    "            \n",
    "            for i in range(n_features):\n",
    "                axes[i].plot(timestamps, reference_trajectory[:, i+1], 'b-', \n",
    "                           label='Reference', linewidth=2)\n",
    "                axes[i].plot(timestamps, trajectories[similar_idx, :, i+1], 'r--', \n",
    "                           label=f'Similar Trajectory {similar_idx}', linewidth=2)\n",
    "                \n",
    "                axes[i].set_xlabel('Time')\n",
    "                axes[i].set_ylabel(feature_names[i+1])\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                axes[i].legend()\n",
    "            \n",
    "            plt.suptitle(f'Comparison with Similar Trajectory {similar_idx}\\n' + \n",
    "                        f'DTW Distance: {distances[similar_idx]:.2f}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_path:\n",
    "                plt.savefig(os.path.join(save_path, f'similar_trajectory_{idx+1}.pdf'))\n",
    "            plt.close()\n",
    "        \n",
    "        # Create distribution plot of distances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(distances, kde=True)\n",
    "        plt.axvline(np.mean(distances), color='r', linestyle='--', label='Mean')\n",
    "        plt.axvline(np.median(distances), color='g', linestyle='--', label='Median')\n",
    "        for dist in distances[similar_indices]:\n",
    "            plt.axvline(dist, color='b', linestyle=':', alpha=0.5)\n",
    "        plt.title('Distribution of DTW Distances to Reference Trajectory')\n",
    "        plt.xlabel('DTW Distance')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_path, 'distance_distribution.pdf'))\n",
    "        plt.close()\n",
    "    \n",
    "    return similar_indices, distances[similar_indices], similarity_stats\n",
    "\n",
    "# Example usage:\n",
    "reference_traj = trajectories[0]  # Use the first trajectory as reference\n",
    "other_trajectories = trajectories[1:]  # Use the rest as candidates\n",
    "output_dir = \"similarity_analysis\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "similar_indices, distances, stats = find_similar_trajectories(\n",
    "    reference_trajectory=reference_traj,\n",
    "    trajectories=other_trajectories,\n",
    "    feature_names=feature_names,\n",
    "    n_closest=5,\n",
    "    save_path=output_dir\n",
    ")\n",
    "\n",
    "print(\"\\nMost similar trajectories:\")\n",
    "for idx, (similar_idx, distance) in enumerate(zip(similar_indices, distances)):\n",
    "    print(f\"{idx+1}. Trajectory {similar_idx}: DTW distance = {distance:.2f}\")\n",
    "\n",
    "print(\"\\nSimilarity Statistics:\")\n",
    "for stat_name, stat_value in stats.items():\n",
    "    print(f\"{stat_name}: {stat_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate statistics for each method\n",
    "methods = ['GFlowNet', 'SMCMC', 'REINFORCE', 'SAC', 'REINFORCE with Baseline']\n",
    "dfs = [gflownet['Final Rewards'], smcmc['Final Rewards'], \n",
    "    reinforce['Final Rewards'], sac['Final Rewards'], \n",
    "    reinforce_with_baseline['Final Rewards']]\n",
    "\n",
    "stats = {}\n",
    "for method, df in zip(methods, dfs):\n",
    "    sorted_rewards = sorted(df.values, reverse=True)\n",
    "    stats[method] = {\n",
    "     'Top 10 Mean': np.mean(sorted_rewards[:10]),\n",
    "     'Top 50 Mean': np.mean(sorted_rewards[:50]),\n",
    "     'Top 100 Mean': np.mean(sorted_rewards[:100]),\n",
    "     '10th Percentile': np.percentile(sorted_rewards, 10),\n",
    "     '90th Percentile': np.percentile(sorted_rewards, 90)\n",
    "    }\n",
    "\n",
    "# Print results in a copy-friendly format\n",
    "print(\"Method & Top 10 Mean & Top 50 Mean & Top 100 Mean & 10th Percentile & 90th Percentile \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "for method in methods:\n",
    "    row = f\"{method:<20} & \" + \\\n",
    "          f\"{stats[method]['Top 10 Mean']:>8.2f} & \" + \\\n",
    "          f\"{stats[method]['Top 50 Mean']:>8.2f} & \" + \\\n",
    "          f\"{stats[method]['Top 100 Mean']:>8.2f} & \" + \\\n",
    "          f\"{stats[method]['10th Percentile']:>8.2f} & \" + \\\n",
    "          f\"{stats[method]['90th Percentile']:>8.2f} \\\\\\\\\"\n",
    "    print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Create DataFrame from the final states (last timestep of each trajectory)\n",
    "final_states_df = pd.DataFrame(\n",
    "    trajectories[:,-1,1:],  # Take last timestep (-1) and exclude TimeStep column (1:)\n",
    "    columns=feature_names[1:]  # Use feature names but exclude 'TimeStep'\n",
    ")\n",
    "final_states_df = final_states_df.iloc[gflownet['index'],:]\n",
    "final_states_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Perform clustering analysis\n",
    "optimal_clusters, cluster_labels, tsne_results = perform_clustering_analysis(\n",
    "    final_states_df, \n",
    "    feature_names,\n",
    "    n_clusters_range=(2, 10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], \n",
    "                     c=cluster_labels, cmap='viridis', \n",
    "                     alpha=0.6)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(f't-SNE Visualization with {optimal_clusters} Clusters')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "last_step_rewards = [r[-1] for r in rewards]\n",
    "cluster_stats = plot_reward_distributions_by_cluster(\n",
    "    cluster_labels=cluster_labels,\n",
    "    rewards=last_step_rewards,\n",
    "    optimal_clusters=optimal_clusters,\n",
    "    save_path='cluster_reward_distributions.pdf'\n",
    ")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nCluster Summary Statistics:\")\n",
    "for cluster, stats in cluster_stats.items():\n",
    "    print(f\"\\n{cluster}:\")\n",
    "    for metric, value in stats.items():\n",
    "        print(f\"{metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "diversity_scores = analyze_cluster_diversity(\n",
    "    trajectories=trajectories, \n",
    "    cluster_labels=cluster_labels,\n",
    "    optimal_clusters=optimal_clusters,\n",
    "    last_step_rewards=last_step_rewards,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Print summary of diversity scores\n",
    "print(\"\\nCluster Diversity Summary:\")\n",
    "for cluster_id, div_score, norm_div in diversity_scores:\n",
    "    print(f\"Cluster {cluster_id + 1}:\")\n",
    "    print(f\"  Raw Diversity Score: {div_score:.4f}\")\n",
    "    print(f\"  Normalized Diversity: {norm_div:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "feature_std_df = analyze_cluster_feature_variability(\n",
    "    final_states_df=final_states_df,\n",
    "    cluster_labels=cluster_labels,\n",
    "    optimal_clusters=optimal_clusters,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nCluster Feature Variability Summary:\")\n",
    "for _, row in feature_std_df.iterrows():\n",
    "    print(f\"\\n{row['Cluster']} (Size: {row['Size']}):\")\n",
    "    for feature in feature_names[1:]:\n",
    "        print(f\"  {feature}: {row[feature]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Trajectory Phase Portraits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_traj = trajectories[gflownet['index'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "fig = plot_phase_portraits(\n",
    "    trajectories=sub_traj,\n",
    "    feature_names=feature_names,\n",
    "    n_trajectories=100,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('phase_portraits.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "data = joblib.load(r\"results/All_Experiments_Results/smcmc_1_ens/trajectories_results.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0] = data[0]['results_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_experimental_results(base_path=\"results/All_Experiments_Results\",model_types=None):\n",
    "    \"\"\"\n",
    "    Load all experimental results from the structured folder.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested dictionary with structure [oracle][model_type][method] = results\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    from pathlib import Path\n",
    "    \n",
    "    results = {}\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    # Define the structure\n",
    "    oracles = [1, 2, 3]\n",
    "    methods = ['gflow', 'reinforce', 'sac', 'smcmc']\n",
    "    # methods = ['smcmc']\n",
    "    model_types = ['gbr', 'mlp', 'rfr','elastic','ens'] if model_types is None else model_types\n",
    "    \n",
    "    for oracle in oracles:\n",
    "        results[oracle] = {}\n",
    "        for model_type in model_types:\n",
    "            results[oracle][model_type] = {}\n",
    "            for method in methods:\n",
    "                folder_name = f\"{method}_{oracle}_{model_type}\"\n",
    "                folder_path = base_path / folder_name\n",
    "                \n",
    "                if folder_path.exists():\n",
    "                    # ALWAYS prioritize trajectories_results.joblib for trajectory data\n",
    "                    result_files = list(folder_path.glob(\"*trajectories_results.joblib\"))\n",
    "                    \n",
    "                    # If no trajectories file found, look for method-specific files as backup\n",
    "                    if not result_files:\n",
    "                        if method == 'gflow':\n",
    "                            result_files = list(folder_path.glob(f\"gflow_{oracle}_{model_type}_trajectories_results.joblib\"))\n",
    "                        elif method == 'reinforce_baseline':\n",
    "                            result_files = list(folder_path.glob(\"reinforce_baseline_results.joblib\"))\n",
    "                        elif method == 'reinforce':\n",
    "                            result_files = list(folder_path.glob(\"trajectories_results.joblib\"))\n",
    "                        elif method == 'sac':\n",
    "                            result_files = list(folder_path.glob(\"trajectories_results.joblib\"))\n",
    "                        elif method == 'smcmc':\n",
    "                            result_files = list(folder_path.glob(\"trajectories_results.joblib\"))\n",
    "                            if len(result_files) == 0:\n",
    "                                result_files = list(folder_path.glob(\"smcmc_results.joblib\"))\n",
    "                    \n",
    "                    if result_files:\n",
    "                        try:\n",
    "                            data = joblib.load(result_files[0])\n",
    "                            \n",
    "                            # Verify the data contains trajectories\n",
    "                            if isinstance(data, dict):\n",
    "                                # Check if it's the 30-run structure\n",
    "                                if all(isinstance(k, int) for k in data.keys() if isinstance(k, (int, str))):\n",
    "                                    # Check if runs contain trajectory data\n",
    "                                    sample_run = None\n",
    "                                    for key, value in data.items():\n",
    "                                        if isinstance(key, int) and isinstance(value, dict):\n",
    "                                            sample_run = value\n",
    "                                            break\n",
    "                                    \n",
    "                                    if sample_run and 'trajectories' in sample_run:\n",
    "                                        results[oracle][model_type][method] = data\n",
    "                                        print(f\" Loaded: {folder_name} (30-run structure)\")\n",
    "                                    else:\n",
    "                                        try:\n",
    "                                            sample_run = sample_run['results_dict']\n",
    "                                            results[oracle][model_type][method] = data\n",
    "                                            print(f\" Loaded: {folder_name} (30-run structure) with nested results_dict\")\n",
    "                                        except:\n",
    "                                            print(f\" {folder_name}: No trajectory data in runs structure\")\n",
    "                                            continue\n",
    "                                        \n",
    "                                # Check if it's direct trajectory data structure\n",
    "                                elif 'trajectories' in data:\n",
    "                                    results[oracle][model_type][method] = data\n",
    "                                    print(f\" Loaded: {folder_name} (direct structure)\")\n",
    "\n",
    "                                else:\n",
    "                                        \n",
    "                                    print(f\" {folder_name}: No trajectories found in data structure\")\n",
    "                                    # Print keys for debugging\n",
    "                                    print(f\"    Available keys: {list(data.keys())}\")\n",
    "                                    continue\n",
    "                            else:\n",
    "                                print(f\" {folder_name}: Data is not a dictionary\")\n",
    "                                continue\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\" Error loading {folder_name}: {e}\")\n",
    "                    else:\n",
    "                        print(f\" No valid result files found in: {folder_name}\")\n",
    "                        # List available files for debugging\n",
    "                        available_files = list(folder_path.glob(\"*.joblib\"))\n",
    "                        if available_files:\n",
    "                            print(f\"    Available files: {[f.name for f in available_files]}\")\n",
    "                else:\n",
    "                    print(f\" Folder not found: {folder_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "test = load_all_experimental_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_comprehensive_analysis_for_combination(oracle, model_type, method_data, \n",
    "                                               base_output_dir, feature_names):\n",
    "    \"\"\"\n",
    "    Run comprehensive analysis for a specific oracle-model_type-method combination.\n",
    "    \n",
    "    Args:\n",
    "        oracle: Oracle number (1, 2, or 3)\n",
    "        model_type: Model type ('gbr', 'mlp', or 'rfr')\n",
    "        method_data: Dictionary containing method results (30 runs each)\n",
    "        base_output_dir: Base directory to save results\n",
    "        feature_names: List of feature names\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results and metrics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    combination_results = {}\n",
    "    \n",
    "    for method_name, runs_data in method_data.items():\n",
    "        if runs_data is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Analyzing {method_name.upper()}...\")\n",
    "        \n",
    "        # Create method-specific output directory\n",
    "        method_output_dir = os.path.join(base_output_dir, f\"{method_name}\")\n",
    "        plots_dir = os.path.join(method_output_dir, 'plots')\n",
    "        metrics_dir = os.path.join(method_output_dir, 'metrics')\n",
    "        action_plots_dir = os.path.join(method_output_dir, 'action_distributions')\n",
    "        \n",
    "        for dir_path in [method_output_dir, plots_dir, metrics_dir, action_plots_dir]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Check if this is the 30-runs structure {0: {...}, 1: {...}, ...}\n",
    "            if isinstance(runs_data, dict) and all(isinstance(k, int) for k in runs_data.keys()):\n",
    "                # This is the 30-runs structure\n",
    "                print(f\"    Found 30-run structure with {len(runs_data)} runs\")\n",
    "                \n",
    "                # NEW APPROACH: Collect random percentage from each run separately\n",
    "                all_sampled_trajectories = []\n",
    "                all_sampled_rewards = []\n",
    "                all_sampled_actions = []\n",
    "                sample_percentage = 0.05  # Take random 30% from each run (adjust as needed)\n",
    "                \n",
    "                for run_idx in range(len(runs_data)):\n",
    "                    print(f\"    Processing run {run_idx+1}/{len(runs_data)}\")\n",
    "                    if run_idx not in runs_data:\n",
    "                        print(f\"     Run index {run_idx} missing in data, skipping...\")   \n",
    "                        continue\n",
    "                        \n",
    "                    run_data = runs_data[run_idx]\n",
    "                    if 'trajectories' not in run_data:\n",
    "                        try:\n",
    "                            run_data = run_data['results_dict']\n",
    "                        except:\n",
    "                            print(f\"     No trajectory data in run {run_idx}, skipping...\")\n",
    "                            continue\n",
    "                    # Extract data based on method\n",
    "                    if 'trajectories' in run_data:\n",
    "                        trajectories = run_data['trajectories']\n",
    "                        rewards = run_data['rewards']\n",
    "                        \n",
    "                        # Handle actions with different key names\n",
    "                        if 'all_actions' in run_data:\n",
    "                            actions = run_data['all_actions']\n",
    "                        elif 'actions' in run_data:\n",
    "                            actions = run_data['actions']\n",
    "                        else:\n",
    "                            actions = []\n",
    "                        \n",
    "                        # Convert to numpy arrays for this run\n",
    "                        if isinstance(trajectories, (list, np.ndarray)):\n",
    "                            trajectories = np.array(trajectories)\n",
    "                        if isinstance(rewards, (list, np.ndarray)):\n",
    "                            rewards = np.array(rewards)\n",
    "                        if len(actions) > 0:\n",
    "                            actions = np.array(actions)\n",
    "                        \n",
    "                        # Process rewards based on method type\n",
    "                        if method_name in ['reinforce', 'sac']:\n",
    "                            # RL methods: rewards are already final values (1D)\n",
    "                            if rewards.ndim == 1:\n",
    "                                final_rewards_run = rewards\n",
    "                            elif rewards.ndim == 2 and rewards.shape[1] == 1:\n",
    "                                final_rewards_run = rewards[:, 0]\n",
    "                            else:\n",
    "                                print(f\"     Unexpected reward format for RL method {method_name}: {rewards.shape}\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            # GFlow/SMCMC methods: rewards are trajectories (2D)\n",
    "                            if rewards.ndim == 2:\n",
    "                                final_rewards_run = rewards[:, -1]  # Take last timestep\n",
    "                            elif rewards.ndim == 1:\n",
    "                                final_rewards_run = rewards  # Already final values\n",
    "                            else:\n",
    "                                print(f\"     Unexpected reward format for method {method_name}: {rewards.shape}\")\n",
    "                                continue\n",
    "                        \n",
    "                        # FIX: Ensure final_rewards_run is numeric before using np.isfinite()\n",
    "                        try:\n",
    "                            # Convert to float64 to ensure numeric type\n",
    "                            final_rewards_run = np.array(final_rewards_run, dtype=np.float64)\n",
    "                        except (ValueError, TypeError) as e:\n",
    "                            print(f\"     Cannot convert rewards to numeric for {method_name} run {run_idx}: {e}\")\n",
    "                            print(f\"    Rewards sample: {final_rewards_run[:5] if len(final_rewards_run) > 0 else 'empty'}\")\n",
    "                            continue\n",
    "                        # Remove non-finite rewards\n",
    "                        finite_mask = np.isfinite(final_rewards_run)\n",
    "                        final_rewards_run = final_rewards_run[finite_mask]\n",
    "                        trajectories_run = trajectories[finite_mask]\n",
    "                        if len(actions) > 0:\n",
    "                            actions_run = actions[finite_mask]\n",
    "                        else:\n",
    "                            actions_run = []\n",
    "                        \n",
    "                        if len(final_rewards_run) == 0:\n",
    "                            print(f\"     No valid rewards for run {run_idx}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Get random X% from this specific run\n",
    "                        n_trajectories_run = len(final_rewards_run)\n",
    "                        n_sample_run = max(1, int(n_trajectories_run * sample_percentage))\n",
    "                        \n",
    "                        # Get random indices from this run\n",
    "                        np.random.seed(42 + run_idx)  # Reproducible but different per run\n",
    "                        sample_indices_run = np.random.choice(n_trajectories_run, \n",
    "                                                            size=n_sample_run, \n",
    "                                                            replace=False)\n",
    "                        \n",
    "                        # Extract sampled trajectories from this run\n",
    "                        sample_trajectories_run = trajectories_run[sample_indices_run]\n",
    "                        sample_rewards_run = final_rewards_run[sample_indices_run]\n",
    "                        if len(actions_run) > 0:\n",
    "                            sample_actions_run = actions_run[sample_indices_run]\n",
    "                        else:\n",
    "                            sample_actions_run = []\n",
    "                        \n",
    "                        # Add to combined lists\n",
    "                        all_sampled_trajectories.extend(sample_trajectories_run)\n",
    "                        all_sampled_rewards.extend(sample_rewards_run)\n",
    "                        if len(sample_actions_run) > 0:\n",
    "                            all_sampled_actions.extend(sample_actions_run)\n",
    "                        \n",
    "                        print(f\"    Run {run_idx+1}: Randomly sampled {n_sample_run}/{n_trajectories_run} trajectories ({sample_percentage*100:.0f}%)\")\n",
    "                \n",
    "                # Convert to numpy arrays\n",
    "                trajectories = np.array(all_sampled_trajectories)\n",
    "                final_rewards = np.array(all_sampled_rewards)\n",
    "                all_actions = np.array(all_sampled_actions) if all_sampled_actions else []\n",
    "                \n",
    "                print(f\"    Combined sampled trajectories: {len(trajectories)} from all runs\")\n",
    "                print(f\"    Combined sampled rewards: {len(final_rewards)} from all runs\")\n",
    "                \n",
    "            else:\n",
    "                # This is single dataset structure (shouldn't happen with current structure)\n",
    "                print(f\"    Found single dataset structure\")\n",
    "                trajectories = runs_data['trajectories']\n",
    "                rewards = runs_data['rewards']\n",
    "                all_actions = runs_data.get('all_actions', runs_data.get('actions', []))\n",
    "                \n",
    "                # Process single dataset\n",
    "                rewards = np.array(rewards) if isinstance(rewards, list) else rewards\n",
    "                if method_name in ['reinforce', 'sac']:\n",
    "                    if rewards.ndim == 1:\n",
    "                        final_rewards = rewards\n",
    "                    elif rewards.ndim == 2 and rewards.shape[1] == 1:\n",
    "                        final_rewards = rewards[:, 0]\n",
    "                    else:\n",
    "                        final_rewards = rewards.flatten()\n",
    "                else:\n",
    "                    if rewards.ndim == 2:\n",
    "                        final_rewards = rewards[:, -1]\n",
    "                    elif rewards.ndim == 1:\n",
    "                        final_rewards = rewards\n",
    "                    else:\n",
    "                        final_rewards = np.array([r[-1] if isinstance(r, (list, np.ndarray)) and len(r) > 0 else r for r in rewards])\n",
    "            \n",
    "            if len(trajectories) == 0:\n",
    "                print(f\"     No trajectories found for {method_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Now work with the sampled trajectories (random % from each run)\n",
    "            final_predictions = np.array([-r for r in final_rewards])\n",
    "            print(f\"Final rewards shape: {final_rewards.shape}, Final predictions shape: {final_predictions.shape}\")\n",
    "            \n",
    "            # For analysis, take top N from the randomly sampled set for specific analyses\n",
    "            n_top = min(100, len(final_rewards))\n",
    "            if n_top < len(final_rewards):\n",
    "                # If we have more than 200, take the best 200 from the sampled set for some analyses\n",
    "                top_indices = np.argsort(final_predictions)[:n_top]\n",
    "                trajectories_top = np.array([trajectories[i] for i in top_indices])\n",
    "                final_rewards_top = final_rewards[top_indices]\n",
    "                final_predictions_top = final_predictions[top_indices]\n",
    "            else:\n",
    "                # Use all sampled trajectories\n",
    "                trajectories_top = trajectories\n",
    "                final_rewards_top = final_rewards\n",
    "                final_predictions_top = final_predictions\n",
    "                \n",
    "            print(f\"    Using {len(trajectories_top)} trajectories for analysis\")\n",
    "            \n",
    "            # Calculate metrics on the randomly sampled data\n",
    "            metrics_dict = {\n",
    "                'average_reward': np.mean(final_rewards),  # Use full sampled data for metrics\n",
    "                'max_reward': np.max(final_rewards),\n",
    "                'min_reward': np.min(final_rewards),\n",
    "                'std_reward': np.std(final_rewards),\n",
    "                'tail_coverage_20': np.mean(final_predictions >= -20),\n",
    "                'tail_coverage_40': np.mean(final_predictions >= -40),\n",
    "                'tail_coverage_60': np.mean(final_predictions >= -60),\n",
    "                'expected_shortfall_5': np.mean(np.sort(final_predictions)[:max(1, len(final_predictions)//20)]) if len(final_predictions) >= 5 else np.mean(final_predictions),\n",
    "                'expected_shortfall_10': np.mean(np.sort(final_predictions)[:max(1, len(final_predictions)//10)]) if len(final_predictions) >= 10 else np.mean(final_predictions),\n",
    "                'expected_shortfall_20': np.mean(np.sort(final_predictions)[:max(1, len(final_predictions)//5)]) if len(final_predictions) >= 20 else np.mean(final_predictions)\n",
    "            }\n",
    "            \n",
    "            # Save metrics\n",
    "            with open(os.path.join(metrics_dir, 'performance_metrics.txt'), 'w') as f:\n",
    "                for metric, value in metrics_dict.items():\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "                f.write(f\"sample_percentage_per_run: {sample_percentage:.2f}\\n\")\n",
    "                f.write(f\"total_analyzed_trajectories: {len(trajectories)}\\n\")\n",
    "                f.write(f\"sampling_method: random_per_run\\n\")\n",
    "            \n",
    "            # Rewards summary\n",
    "            rewards_dict = {\n",
    "                'min': np.min(final_rewards),\n",
    "                'max': np.max(final_rewards),\n",
    "                'mean': np.mean(final_rewards),\n",
    "                'std': np.std(final_rewards),\n",
    "                'median': np.median(final_rewards)\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(metrics_dir, 'rewards_summary.txt'), 'w') as f:\n",
    "                for metric, value in rewards_dict.items():\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            \n",
    "            # DTW clustering analysis (if we have enough data) - use top trajectories for computational efficiency\n",
    "            if len(trajectories_top) >= 3:\n",
    "                try:\n",
    "                    results_df, distance_matrix, feature_clusters = dtw_clustering_analysis(\n",
    "                        trajectories=trajectories_top,\n",
    "                        n_clusters=min(3, len(trajectories_top))\n",
    "                    )\n",
    "                    \n",
    "                    # Clustering plots\n",
    "                    plot_trajectories_by_cluster(results_df, trajectories_top, ['Timestamp']+feature_names,\n",
    "                                               num_clusters=min(3, len(trajectories_top)),\n",
    "                                               save_path=os.path.join(plots_dir, 'cluster_trajectories.pdf'))\n",
    "                    \n",
    "                    # DTW distance matrix\n",
    "                    res_dict_dtw = plot_dtw_distance_matrix(distance_matrix, model_name=method_name, \n",
    "                                                          rewards=final_rewards_top,\n",
    "                                                          save_path=os.path.join(plots_dir, 'dtw_distance_matrix.pdf'))\n",
    "                    \n",
    "                    # Save DTW metrics\n",
    "                    with open(os.path.join(metrics_dir, 'dtw_metrics.txt'), 'w') as f:\n",
    "                        f.write(f\"Average Distance: {res_dict_dtw['average_distance']:.4f}\\n\")\n",
    "                        f.write(f\"Normalized Score: {res_dict_dtw['normalized_score']:.4f}\\n\")\n",
    "                        f.write(f\"Reward Normalization: {res_dict_dtw['reward_normalization']:.4f}\\n\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"     DTW analysis failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Trajectory plots - use top trajectories for visualization\n",
    "            try:\n",
    "                plot_trajectories_over_time(\n",
    "                    trajectories=trajectories_top,\n",
    "                    rewards=final_rewards_top,\n",
    "                    feature_names=['Timestamp'] + feature_names,\n",
    "                    n_top=min(50, len(trajectories_top)),\n",
    "                    alpha_others=0.05,\n",
    "                    save_path=plots_dir\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"     Trajectory plotting failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Action distribution plots (if actions available) - use all sampled data\n",
    "            if len(all_actions) > 0:\n",
    "                try:\n",
    "                    feature_names_for_actions = ['Timestamp'] + feature_names if method_name != 'smcmc' else feature_names\n",
    "                    \n",
    "                    plot_action_distributions_by_timestep(\n",
    "                        trajectories=trajectories,\n",
    "                        all_actions=all_actions,\n",
    "                        feature_names=feature_names_for_actions,\n",
    "                        save_path=action_plots_dir\n",
    "                    )\n",
    "                    \n",
    "                    plot_action_distributions(\n",
    "                        all_actions=all_actions,\n",
    "                        feature_names=feature_names_for_actions,\n",
    "                        save_path=os.path.join(plots_dir, 'distribution_actions.pdf')\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"     Action distribution plotting failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Summary DataFrame - use top trajectories for the summary table\n",
    "            try:\n",
    "                summary_rows = []\n",
    "                for rank, idx in enumerate(top_indices):\n",
    "                    row_dict = {\n",
    "                        \"Rank\": rank + 1,\n",
    "                        \"CaseIndex\": idx,\n",
    "                        \"FinalReward\": final_rewards[idx]\n",
    "                    }\n",
    "                    \n",
    "                    # Handle final values extraction\n",
    "                    if trajectories[idx].ndim == 2:  # (timesteps, features)\n",
    "                        final_values = trajectories[idx][-1, :]\n",
    "                    else:\n",
    "                        final_values = trajectories[idx]\n",
    "                    \n",
    "                    # Skip timestamp column if present\n",
    "                    feature_start_idx = 1 if len(final_values) == len(feature_names) + 1 else 0\n",
    "                    \n",
    "                    for f_i, f_name in enumerate(feature_names):\n",
    "                        if feature_start_idx + f_i < len(final_values):\n",
    "                            row_dict[f_name] = final_values[feature_start_idx + f_i]\n",
    "                    \n",
    "                    summary_rows.append(row_dict)\n",
    "                \n",
    "                top_df = pd.DataFrame(summary_rows).sort_values(by=\"FinalReward\", ascending=False)\n",
    "                top_df.to_csv(os.path.join(metrics_dir, f'top_{n_top}_summary.csv'), index=False)\n",
    "                \n",
    "                # Diversity metrics - use top trajectories for computational efficiency\n",
    "                if len(feature_names) > 0:\n",
    "                    try:\n",
    "                        avg_div, norm_div, distances = plot_diversity_metrics(\n",
    "                            df=top_df,\n",
    "                            feature_names=feature_names,\n",
    "                            save_path=os.path.join(plots_dir, 'euclidean_distance_matrix.pdf')\n",
    "                        )\n",
    "                        \n",
    "                        with open(os.path.join(metrics_dir, 'diversity_metrics.txt'), 'w') as f:\n",
    "                            f.write(f\"Average Diversity - Last State: {avg_div:.4f}\\n\")\n",
    "                            f.write(f\"Normalized Diversity - Last State: {norm_div:.4f}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"     Diversity analysis failed for {method_name}: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"     Summary analysis failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Reward distribution plot - use all sampled data\n",
    "            try:\n",
    "                plot_reward_distribution(final_rewards, \n",
    "                                        save_path=os.path.join(plots_dir, 'reward_distribution.pdf'))\n",
    "            except Exception as e:\n",
    "                print(f\"     Reward distribution plotting failed for {method_name}: {e}\")\n",
    "            \n",
    "            combination_results[method_name] = {\n",
    "                'metrics': metrics_dict,\n",
    "                'rewards_summary': rewards_dict,\n",
    "                'output_dir': method_output_dir,\n",
    "                'n_trajectories': len(trajectories),\n",
    "                'n_top_analyzed': len(trajectories_top),\n",
    "                'sample_percentage_used': sample_percentage,\n",
    "                'selection_method': 'random_per_run'\n",
    "            }\n",
    "            \n",
    "            print(f\"     {method_name.upper()}: avg_reward={metrics_dict['average_reward']:.2f} (analyzed {len(trajectories)} randomly sampled trajectories from {sample_percentage*100:.0f}% of each run)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     Error processing {method_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return combination_results\n",
    "\n",
    "def generate_comparison_tables(all_results, output_dir):\n",
    "    \"\"\"\n",
    "    Generate comprehensive comparison tables across all oracles and model types.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    tables_dir = os.path.join(output_dir, 'comparison_tables')\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    \n",
    "    # Key metrics to compare\n",
    "    key_metrics = ['average_reward', 'max_reward', 'tail_coverage_20', 'expected_shortfall_10', 'expected_shortfall_20']\n",
    "    methods = ['gflow', 'reinforce', 'reinforce_baseline', 'sac', 'smcmc']\n",
    "    \n",
    "    # 1. Create comparison table for each metric\n",
    "    for metric in key_metrics:\n",
    "        print(f\"Creating comparison table for {metric}...\")\n",
    "        \n",
    "        rows = []\n",
    "        \n",
    "        for oracle in [1, 2, 3]:\n",
    "            for model_type in ['gbr', 'mlp', 'rfr','elastic','ens']:\n",
    "                row_data = {'Oracle': oracle, 'Model_Type': model_type}\n",
    "                \n",
    "                combination_key = f\"Oracle_{oracle}_{model_type.upper()}\"\n",
    "                \n",
    "                if combination_key in all_results:\n",
    "                    for method in methods:\n",
    "                        if method in all_results[combination_key]:\n",
    "                            value = all_results[combination_key][method]['metrics'].get(metric, np.nan)\n",
    "                            row_data[method.upper()] = value\n",
    "                        else:\n",
    "                            row_data[method.upper()] = np.nan\n",
    "                else:\n",
    "                    # Fill with NaN if combination doesn't exist\n",
    "                    for method in methods:\n",
    "                        row_data[method.upper()] = np.nan\n",
    "                \n",
    "                rows.append(row_data)\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        metric_df = pd.DataFrame(rows)\n",
    "        metric_df.to_csv(os.path.join(tables_dir, f'{metric}_comparison_table.csv'), index=False)\n",
    "    \n",
    "    # 2. Create master summary table\n",
    "    summary_rows = []\n",
    "    for combination_key, combination_data in all_results.items():\n",
    "        oracle, model_type = combination_key.split('_')[1], combination_key.split('_')[2]\n",
    "        \n",
    "        for method, method_data in combination_data.items():\n",
    "            row = {\n",
    "                'Oracle': oracle,\n",
    "                'Model_Type': model_type,\n",
    "                'Method': method.upper(),\n",
    "                'Avg_Reward': method_data['metrics'].get('average_reward', np.nan),\n",
    "                'Max_Reward': method_data['metrics'].get('max_reward', np.nan),\n",
    "                'Std_Reward': method_data['metrics'].get('std_reward', np.nan),\n",
    "                'Tail_Coverage_20': method_data['metrics'].get('tail_coverage_20', np.nan),\n",
    "                'Expected_Shortfall_10': method_data['metrics'].get('expected_shortfall_10', np.nan),\n",
    "                'Expected_Shortfall_20': method_data['metrics'].get('expected_shortfall_20', np.nan),\n",
    "                'N_Trajectories': method_data.get('n_trajectories', 'N/A'),\n",
    "                'N_Top_Analyzed': method_data.get('n_top_analyzed', 'N/A')\n",
    "            }\n",
    "            summary_rows.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(os.path.join(tables_dir, 'master_summary_table.csv'), index=False)\n",
    "    \n",
    "    # 3. Create ranking tables\n",
    "    ranking_data = []\n",
    "    for combination_key, combination_data in all_results.items():\n",
    "        oracle, model_type = combination_key.split('_')[1], combination_key.split('_')[2]\n",
    "        \n",
    "        # Rank methods by average reward\n",
    "        method_rewards = [(method, data['metrics'].get('average_reward', -np.inf)) \n",
    "                         for method, data in combination_data.items()]\n",
    "        method_rewards.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        ranking_data.append({\n",
    "            'Oracle': oracle,\n",
    "            'Model_Type': model_type,\n",
    "            'Best_Method': method_rewards[0][0].upper() if method_rewards else 'N/A',\n",
    "            'Best_Reward': method_rewards[0][1] if method_rewards else np.nan,\n",
    "            'Second_Method': method_rewards[1][0].upper() if len(method_rewards) > 1 else 'N/A',\n",
    "            'Second_Reward': method_rewards[1][1] if len(method_rewards) > 1 else np.nan,\n",
    "            'Worst_Method': method_rewards[-1][0].upper() if method_rewards else 'N/A',\n",
    "            'Worst_Reward': method_rewards[-1][1] if method_rewards else np.nan,\n",
    "            'Methods_Count': len(method_rewards)\n",
    "        })\n",
    "    \n",
    "    ranking_df = pd.DataFrame(ranking_data)\n",
    "    ranking_df.to_csv(os.path.join(tables_dir, 'method_rankings.csv'), index=False)\n",
    "    \n",
    "    print(f\" All comparison tables saved to {tables_dir}\")\n",
    "    return summary_df, ranking_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the comprehensive analysis.\n",
    "    \"\"\"\n",
    "    print(\"COMPREHENSIVE EXPERIMENTAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load all experimental results\n",
    "    print(\"Step 1: Loading all experimental results...\")\n",
    "    all_results = load_all_experimental_results(model_types=['ens'])\n",
    "    \n",
    "    # Step 2: Setup feature names\n",
    "    feature_names = ['Volume_spx', 'Close_ndx', 'Volume_ndx', 'Close_vix', 'IRLTCT01USM156N', 'BAMLH0A3HYCEY']\n",
    "    \n",
    "    # Step 3: Create output directory\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    main_output_dir = f\"analysis_outputs/Comprehensive_Analysis_{timestamp}\"\n",
    "    os.makedirs(main_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Output directory: {main_output_dir}\")\n",
    "    \n",
    "    # Step 4: Run analysis for each combination\n",
    "    print(\"Step 4: Running comprehensive analysis for all combinations...\")\n",
    "    \n",
    "    analysis_results = {}\n",
    "    successful = []\n",
    "    failed = []\n",
    "    \n",
    "    for oracle in [1, 2, 3]:\n",
    "        for model_type in ['gbr', 'mlp', 'rfr', 'elastic', 'ens']:\n",
    "            combination_key = f\"Oracle_{oracle}_{model_type.upper()}\"\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"PROCESSING: {combination_key}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                # Create combination output directory\n",
    "                combo_output_dir = os.path.join(main_output_dir, combination_key)\n",
    "                os.makedirs(combo_output_dir, exist_ok=True)\n",
    "                \n",
    "                # Get method data for this combination\n",
    "                method_data = all_results.get(oracle, {}).get(model_type, {})\n",
    "                \n",
    "                if method_data:\n",
    "                    # Run analysis for this combination\n",
    "                    combination_results = run_comprehensive_analysis_for_combination(\n",
    "                        oracle, model_type, method_data, combo_output_dir, feature_names\n",
    "                    )\n",
    "                    \n",
    "                    if combination_results:  # Only add if we got some results\n",
    "                        analysis_results[combination_key] = combination_results\n",
    "                        successful.append(combination_key)\n",
    "                        print(f\" Successfully processed {combination_key} with {len(combination_results)} methods\")\n",
    "                    else:\n",
    "                        print(f\" No results generated for {combination_key}\")\n",
    "                        failed.append(combination_key)\n",
    "                else:\n",
    "                    print(f\" No data found for {combination_key}\")\n",
    "                    failed.append(combination_key)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\" Failed to process {combination_key}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                failed.append(combination_key)\n",
    "                continue\n",
    "    \n",
    "    # Step 5: Generate comparison tables\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATING COMPARISON TABLES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if analysis_results:\n",
    "        summary_df, ranking_df = generate_comparison_tables(analysis_results, main_output_dir)\n",
    "        \n",
    "        # Print some summary statistics\n",
    "        print(f\"\\nSummary of results:\")\n",
    "        print(f\"Total method-combination pairs analyzed: {len(summary_df)}\")\n",
    "        print(f\"Methods found across all combinations:\")\n",
    "        for method in summary_df['Method'].unique():\n",
    "            count = len(summary_df[summary_df['Method'] == method])\n",
    "            print(f\"  {method}: {count} combinations\")\n",
    "    else:\n",
    "        print(\" No results to generate tables from\")\n",
    "    \n",
    "    # Step 6: Print final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ANALYSIS COMPLETE - FINAL SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Results saved to: {main_output_dir}\")\n",
    "    print(f\"Successful analyses: {len(successful)}\")\n",
    "    print(f\"Failed analyses: {len(failed)}\")\n",
    "    \n",
    "    print(\"\\nSuccessful combinations:\")\n",
    "    for success in successful:\n",
    "        methods_count = len(analysis_results.get(success, {}))\n",
    "        print(f\"   {success} ({methods_count} methods)\")\n",
    "    \n",
    "    if failed:\n",
    "        print(\"\\nFailed combinations:\")\n",
    "        for failure in failed:\n",
    "            print(f\"   {failure}\")\n",
    "    \n",
    "    return analysis_results, main_output_dir\n",
    "\n",
    "def run_comprehensive_analysis_for_combination(oracle, model_type, method_data, \n",
    "                                               base_output_dir, feature_names):\n",
    "    \"\"\"\n",
    "    Run comprehensive analysis for a specific oracle-model_type-method combination.\n",
    "    \n",
    "    Args:\n",
    "        oracle: Oracle number (1, 2, or 3)\n",
    "        model_type: Model type ('gbr', 'mlp', or 'rfr')\n",
    "        method_data: Dictionary containing method results (30 runs each)\n",
    "        base_output_dir: Base directory to save results\n",
    "        feature_names: List of feature names\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results and metrics\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    combination_results = {}\n",
    "    \n",
    "    for method_name, runs_data in method_data.items():\n",
    "        if runs_data is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Analyzing {method_name.upper()}...\")\n",
    "        \n",
    "        # Create method-specific output directory\n",
    "        method_output_dir = os.path.join(base_output_dir, f\"{method_name}\")\n",
    "        plots_dir = os.path.join(method_output_dir, 'plots')\n",
    "        metrics_dir = os.path.join(method_output_dir, 'metrics')\n",
    "        action_plots_dir = os.path.join(method_output_dir, 'action_distributions')\n",
    "        \n",
    "        for dir_path in [method_output_dir, plots_dir, metrics_dir, action_plots_dir]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Check if this is the 30-runs structure {0: {...}, 1: {...}, ...}\n",
    "            if isinstance(runs_data, dict) and all(isinstance(k, int) for k in runs_data.keys()):\n",
    "                # This is the 30-runs structure\n",
    "                print(f\"    Found 30-run structure with {len(runs_data)} runs\")\n",
    "                \n",
    "                # NEW APPROACH: Collect random percentage from each run separately\n",
    "                all_sampled_trajectories = []\n",
    "                all_sampled_rewards = []\n",
    "                all_sampled_actions = []\n",
    "                sample_percentage = 1  # Take random 30% from each run (adjust as needed)\n",
    "                \n",
    "                for run_idx in range(len(runs_data)):\n",
    "                    print(f\"    Processing run {run_idx+1}/{len(runs_data)}\")\n",
    "                    if run_idx not in runs_data:\n",
    "                        print(f\"     Run index {run_idx} missing in data, skipping...\")   \n",
    "                        continue\n",
    "                        \n",
    "                    run_data = runs_data[run_idx]\n",
    "                    if 'trajectories' not in run_data:\n",
    "                        try:\n",
    "                            run_data = run_data['results_dict']\n",
    "                        except:\n",
    "                            print(f\"     No trajectory data in run {run_idx}, skipping...\")\n",
    "                            continue\n",
    "                    # Extract data based on method\n",
    "                    if 'trajectories' in run_data:\n",
    "                        trajectories = run_data['trajectories']\n",
    "                        rewards = run_data['rewards']\n",
    "                        \n",
    "                        # Handle actions with different key names\n",
    "                        if 'all_actions' in run_data:\n",
    "                            actions = run_data['all_actions']\n",
    "                        elif 'actions' in run_data:\n",
    "                            actions = run_data['actions']\n",
    "                        else:\n",
    "                            actions = []\n",
    "                        \n",
    "                        # Convert to numpy arrays for this run\n",
    "                        if isinstance(trajectories, (list, np.ndarray)):\n",
    "                            trajectories = np.array(trajectories)\n",
    "                        if isinstance(rewards, (list, np.ndarray)):\n",
    "                            rewards = np.array(rewards)\n",
    "                        if len(actions) > 0:\n",
    "                            actions = np.array(actions)\n",
    "                        \n",
    "                        # Process rewards based on method type\n",
    "                        if method_name in ['reinforce', 'sac']:\n",
    "                            # RL methods: rewards are already final values (1D)\n",
    "                            if rewards.ndim == 1:\n",
    "                                final_rewards_run = rewards\n",
    "                            elif rewards.ndim == 2 and rewards.shape[1] == 1:\n",
    "                                final_rewards_run = rewards[:, 0]\n",
    "                            else:\n",
    "                                print(f\"     Unexpected reward format for RL method {method_name}: {rewards.shape}\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            # GFlow/SMCMC methods: rewards are trajectories (2D)\n",
    "                            if rewards.ndim == 2:\n",
    "                                final_rewards_run = rewards[:, -1]  # Take last timestep\n",
    "                            elif rewards.ndim == 1:\n",
    "                                final_rewards_run = rewards  # Already final values\n",
    "                            else:\n",
    "                                print(f\"     Unexpected reward format for method {method_name}: {rewards.shape}\")\n",
    "                                continue\n",
    "                        \n",
    "                        # FIX: Ensure final_rewards_run is numeric before using np.isfinite()\n",
    "                        try:\n",
    "                            # Convert to float64 to ensure numeric type\n",
    "                            final_rewards_run = np.array(final_rewards_run, dtype=np.float64)\n",
    "                        except (ValueError, TypeError) as e:\n",
    "                            print(f\"     Cannot convert rewards to numeric for {method_name} run {run_idx}: {e}\")\n",
    "                            print(f\"    Rewards sample: {final_rewards_run[:5] if len(final_rewards_run) > 0 else 'empty'}\")\n",
    "                            continue\n",
    "                        # Remove non-finite rewards and clip to maximum of 100\n",
    "                        finite_mask = np.isfinite(final_rewards_run)\n",
    "                        final_rewards_run = final_rewards_run[finite_mask]\n",
    "                        final_rewards_run = np.clip(final_rewards_run, -np.inf, 100)  # Clip rewards at 100\n",
    "                        trajectories_run = trajectories[finite_mask]\n",
    "                        if len(actions) > 0:\n",
    "                            actions_run = actions[finite_mask]\n",
    "                        else:\n",
    "                            actions_run = []\n",
    "                        \n",
    "                        if len(final_rewards_run) == 0:\n",
    "                            print(f\"     No valid rewards for run {run_idx}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Get random X% from this specific run\n",
    "                        n_trajectories_run = len(final_rewards_run)\n",
    "                        n_sample_run = max(1, int(n_trajectories_run * sample_percentage))\n",
    "                        \n",
    "                        # Get random indices from this run\n",
    "                        np.random.seed(42 + run_idx)  # Reproducible but different per run\n",
    "                        sample_indices_run = np.random.choice(n_trajectories_run, \n",
    "                                                            size=n_sample_run, \n",
    "                                                            replace=False)\n",
    "                        \n",
    "                        # Extract sampled trajectories from this run\n",
    "                        sample_trajectories_run = trajectories_run[sample_indices_run]\n",
    "                        sample_rewards_run = final_rewards_run[sample_indices_run]\n",
    "                        if len(actions_run) > 0:\n",
    "                            sample_actions_run = actions_run[sample_indices_run]\n",
    "                        else:\n",
    "                            sample_actions_run = []\n",
    "                        \n",
    "                        # Add to combined lists\n",
    "                        all_sampled_trajectories.extend(sample_trajectories_run)\n",
    "                        all_sampled_rewards.extend(sample_rewards_run)\n",
    "                        if len(sample_actions_run) > 0:\n",
    "                            all_sampled_actions.extend(sample_actions_run)\n",
    "                        \n",
    "                        print(f\"    Run {run_idx+1}: Randomly sampled {n_sample_run}/{n_trajectories_run} trajectories ({sample_percentage*100:.0f}%)\")\n",
    "                \n",
    "                # Convert to numpy arrays\n",
    "                trajectories = np.array(all_sampled_trajectories)\n",
    "                final_rewards = np.array(all_sampled_rewards)\n",
    "                final_rewards = np.clip(final_rewards, -np.inf, 100)  # Ensure clipping is applied\n",
    "                all_actions = np.array(all_sampled_actions) if all_sampled_actions else []\n",
    "                \n",
    "                print(f\"    Combined sampled trajectories: {len(trajectories)} from all runs\")\n",
    "                print(f\"    Combined sampled rewards: {len(final_rewards)} from all runs\")\n",
    "                \n",
    "            else:\n",
    "                # This is single dataset structure (shouldn't happen with current structure)\n",
    "                print(f\"    Found single dataset structure\")\n",
    "                trajectories = runs_data['trajectories']\n",
    "                rewards = runs_data['rewards']\n",
    "                all_actions = runs_data.get('all_actions', runs_data.get('actions', []))\n",
    "                \n",
    "                # Process single dataset\n",
    "                rewards = np.array(rewards) if isinstance(rewards, list) else rewards\n",
    "                if method_name in ['reinforce', 'sac']:\n",
    "                    if rewards.ndim == 1:\n",
    "                        final_rewards = rewards\n",
    "                    elif rewards.ndim == 2 and rewards.shape[1] == 1:\n",
    "                        final_rewards = rewards[:, 0]\n",
    "                    else:\n",
    "                        final_rewards = rewards.flatten()\n",
    "                else:\n",
    "                    if rewards.ndim == 2:\n",
    "                        final_rewards = rewards[:, -1]\n",
    "                    elif rewards.ndim == 1:\n",
    "                        final_rewards = rewards\n",
    "                    else:\n",
    "                        final_rewards = np.array([r[-1] if isinstance(r, (list, np.ndarray)) and len(r) > 0 else r for r in rewards])\n",
    "                \n",
    "                # Ensure clipping for single dataset structure too\n",
    "                final_rewards = np.clip(final_rewards, -np.inf, 100)\n",
    "            \n",
    "            if len(trajectories) == 0:\n",
    "                print(f\"     No trajectories found for {method_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Now work with the sampled trajectories (random % from each run)\n",
    "            final_predictions = np.array([-r for r in final_rewards])\n",
    "            print(f\"Final rewards shape: {final_rewards.shape}, Final predictions shape: {final_predictions.shape}\")\n",
    "            \n",
    "            # For analysis, take top N from the randomly sampled set for specific analyses\n",
    "            n_top = min(100, len(final_rewards))\n",
    "            if n_top < len(final_rewards):\n",
    "                # If we have more than 200, take the best 200 from the sampled set for some analyses\n",
    "                top_indices = np.argsort(final_predictions)[:n_top]\n",
    "                trajectories_top = np.array([trajectories[i] for i in top_indices])\n",
    "                final_rewards_top = final_rewards[top_indices]\n",
    "                final_predictions_top = final_predictions[top_indices]\n",
    "\n",
    "                # final_rewards = final_rewards_top\n",
    "                # final_predictions = final_predictions_top\n",
    "                # trajectories = trajectories_top\n",
    "            else:\n",
    "                # Use all sampled trajectories\n",
    "                trajectories_top = trajectories\n",
    "                final_rewards_top = final_rewards\n",
    "                final_predictions_top = final_predictions\n",
    "                top_indices = np.arange(len(final_rewards))\n",
    "                \n",
    "            print(f\"    Using {len(trajectories_top)} trajectories for analysis\")\n",
    "            \n",
    "            # Calculate enhanced metrics on the randomly sampled data\n",
    "            metrics_dict = {\n",
    "                'average_reward': np.mean(final_rewards),  # Use full sampled data for metrics\n",
    "                'median_reward': np.median(final_rewards),  # NEW: Median reward\n",
    "                'max_reward': np.max(final_rewards),\n",
    "                'min_reward': np.min(final_rewards),\n",
    "                'std_reward': np.std(final_rewards),\n",
    "                'quantile_10': np.percentile(final_rewards, 10),  # NEW: 10th percentile\n",
    "                'quantile_25': np.percentile(final_rewards, 25),  # NEW: 25th percentile (Q1)\n",
    "                'quantile_75': np.percentile(final_rewards, 75),  # NEW: 75th percentile (Q3)\n",
    "                'quantile_90': np.percentile(final_rewards, 90),  # NEW: 90th percentile\n",
    "                'iqr': np.percentile(final_rewards, 75) - np.percentile(final_rewards, 25),  # NEW: Interquartile Range\n",
    "                'tail_coverage_20': np.mean(final_predictions >= -20),\n",
    "                'tail_coverage_40': np.mean(final_predictions >= -40),\n",
    "                'tail_coverage_60': np.mean(final_predictions >= -60),\n",
    "                'expected_shortfall_5': np.mean(np.sort(final_predictions)[:max(1, len(final_predictions)//20)]) if len(final_predictions) >= 5 else np.mean(final_predictions),\n",
    "                'expected_shortfall_10': np.mean(np.sort(final_predictions)[:max(1, len(final_predictions)//10)]) if len(final_predictions) >= 10 else np.mean(final_predictions),\n",
    "                'expected_shortfall_20': np.mean(np.sort(final_predictions)[:max(1, len(final_predictions)//5)]) if len(final_predictions) >= 20 else np.mean(final_predictions)\n",
    "            }\n",
    "            \n",
    "            # Save enhanced metrics\n",
    "            with open(os.path.join(metrics_dir, 'performance_metrics.txt'), 'w') as f:\n",
    "                for metric, value in metrics_dict.items():\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "                f.write(f\"sample_percentage_per_run: {sample_percentage:.2f}\\n\")\n",
    "                f.write(f\"total_analyzed_trajectories: {len(trajectories)}\\n\")\n",
    "                f.write(f\"sampling_method: random_per_run\\n\")\n",
    "            \n",
    "            # Enhanced rewards summary\n",
    "            rewards_dict = {\n",
    "                'min': np.min(final_rewards),\n",
    "                'max': np.max(final_rewards),\n",
    "                'mean': np.mean(final_rewards),\n",
    "                'median': np.median(final_rewards),  # NEW: Median in rewards summary\n",
    "                'std': np.std(final_rewards),\n",
    "                'quantile_10': np.percentile(final_rewards, 10),  # NEW: Added to rewards summary\n",
    "                'quantile_25': np.percentile(final_rewards, 25),  # NEW: Added to rewards summary\n",
    "                'quantile_75': np.percentile(final_rewards, 75),  # NEW: Added to rewards summary\n",
    "                'quantile_90': np.percentile(final_rewards, 90),  # NEW: Added to rewards summary\n",
    "                'iqr': np.percentile(final_rewards, 75) - np.percentile(final_rewards, 25)  # NEW: IQR\n",
    "            }\n",
    "            \n",
    "            with open(os.path.join(metrics_dir, 'rewards_summary.txt'), 'w') as f:\n",
    "                for metric, value in rewards_dict.items():\n",
    "                    f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "            \n",
    "            # DTW clustering analysis (if we have enough data) - use top trajectories for computational efficiency\n",
    "            if len(trajectories_top) >= 3:\n",
    "                try:\n",
    "                    results_df, distance_matrix, feature_clusters = dtw_clustering_analysis(\n",
    "                        trajectories=trajectories_top,\n",
    "                        n_clusters=min(3, len(trajectories_top))\n",
    "                    )\n",
    "                    \n",
    "                    # # Clustering plots\n",
    "                    # plot_trajectories_by_cluster(results_df, trajectories_top, ['Timestamp']+feature_names,\n",
    "                    #                            num_clusters=min(3, len(trajectories_top)),\n",
    "                    #                            save_path=os.path.join(plots_dir, 'cluster_trajectories.pdf'))\n",
    "                    \n",
    "                    # DTW distance matrix\n",
    "                    res_dict_dtw = plot_dtw_distance_matrix(distance_matrix, model_name=method_name, \n",
    "                                                          rewards=final_rewards_top,\n",
    "                                                          save_path=os.path.join(plots_dir, 'dtw_distance_matrix.pdf'))\n",
    "                    \n",
    "                    # Save DTW metrics\n",
    "                    with open(os.path.join(metrics_dir, 'dtw_metrics.txt'), 'w') as f:\n",
    "                        f.write(f\"Average Distance: {res_dict_dtw['average_distance']:.4f}\\n\")\n",
    "                        f.write(f\"Normalized Score: {res_dict_dtw['normalized_score']:.4f}\\n\")\n",
    "                        f.write(f\"Reward Normalization: {res_dict_dtw['reward_normalization']:.4f}\\n\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"     DTW analysis failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Trajectory plots - use top trajectories for visualization\n",
    "            # try:\n",
    "            #     plot_trajectories_over_time(\n",
    "            #         trajectories=trajectories_top,\n",
    "            #         rewards=final_rewards_top,\n",
    "            #         feature_names=['Timestamp'] + feature_names,\n",
    "            #         n_top=min(50, len(trajectories_top)),\n",
    "            #         alpha_others=0.05,\n",
    "            #         save_path=plots_dir\n",
    "            #     )\n",
    "            # except Exception as e:\n",
    "            #     print(f\"     Trajectory plotting failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Action distribution plots (if actions available) - use all sampled data\n",
    "            # if len(all_actions) > 0:\n",
    "            #     try:\n",
    "            #         feature_names_for_actions = ['Timestamp'] + feature_names if method_name != 'smcmc' else feature_names\n",
    "                    \n",
    "                    # plot_action_distributions_by_timestep(\n",
    "                    #     trajectories=trajectories,\n",
    "                    #     all_actions=all_actions,\n",
    "                    #     feature_names=feature_names_for_actions,\n",
    "                    #     save_path=action_plots_dir\n",
    "                    # )\n",
    "                    \n",
    "                    # plot_action_distributions(\n",
    "                    #     all_actions=all_actions,\n",
    "                    #     feature_names=feature_names_for_actions,\n",
    "                    #     save_path=os.path.join(plots_dir, 'distribution_actions.pdf')\n",
    "                    # )\n",
    "            #     except Exception as e:\n",
    "            #         print(f\"     Action distribution plotting failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Summary DataFrame - use top trajectories for the summary table\n",
    "            try:\n",
    "                # Create summary for top trajectories\n",
    "                summary_rows = []\n",
    "                for rank, idx in enumerate(top_indices):\n",
    "                    row_dict = {\n",
    "                        \"Rank\": rank + 1,\n",
    "                        \"CaseIndex\": idx,\n",
    "                        \"FinalReward\": final_rewards[idx]\n",
    "                    }\n",
    "                    \n",
    "                    # Handle final values extraction\n",
    "                    if trajectories[idx].ndim == 2:  # (timesteps, features)\n",
    "                        final_values = trajectories[idx][-1, :]\n",
    "                    else:\n",
    "                        final_values = trajectories[idx]\n",
    "                    \n",
    "                    # Skip timestamp column if present\n",
    "                    feature_start_idx = 1 if len(final_values) == len(feature_names) + 1 else 0\n",
    "                    \n",
    "                    for f_i, f_name in enumerate(feature_names):\n",
    "                        if feature_start_idx + f_i < len(final_values):\n",
    "                            row_dict[f_name] = final_values[feature_start_idx + f_i]\n",
    "                    \n",
    "                    summary_rows.append(row_dict)\n",
    "                \n",
    "                top_df = pd.DataFrame(summary_rows).sort_values(by=\"FinalReward\", ascending=False)\n",
    "                top_df.to_csv(os.path.join(metrics_dir, f'top_{n_top}_summary.csv'), index=False)\n",
    "                \n",
    "                # Create summary for ALL trajectories (not just top)\n",
    "                all_summary_rows = []\n",
    "                for idx in range(len(trajectories)):\n",
    "                    row_dict = {\n",
    "                        \"CaseIndex\": idx,\n",
    "                        \"FinalReward\": final_rewards[idx]\n",
    "                    }\n",
    "                    \n",
    "                    # Handle final values extraction\n",
    "                    if trajectories[idx].ndim == 2:  # (timesteps, features)\n",
    "                        final_values = trajectories[idx][-1, :]\n",
    "                    else:\n",
    "                        final_values = trajectories[idx]\n",
    "                    \n",
    "                    # Skip timestamp column if present\n",
    "                    feature_start_idx = 1 if len(final_values) == len(feature_names) + 1 else 0\n",
    "                    \n",
    "                    for f_i, f_name in enumerate(feature_names):\n",
    "                        if feature_start_idx + f_i < len(final_values):\n",
    "                            row_dict[f_name] = final_values[feature_start_idx + f_i]\n",
    "                    \n",
    "                    all_summary_rows.append(row_dict)\n",
    "                \n",
    "                # Sort all trajectories by reward and add rank\n",
    "                all_df = pd.DataFrame(all_summary_rows).sort_values(by=\"FinalReward\", ascending=False)\n",
    "                all_df.insert(0, 'Rank', range(1, len(all_df) + 1))\n",
    "                all_df.to_csv(os.path.join(metrics_dir, f'all_trajectories_summary.csv'), index=False)\n",
    "                \n",
    "                # Diversity metrics - use top trajectories for computational efficiency\n",
    "                if len(feature_names) > 0:\n",
    "                    try:\n",
    "                        avg_div, norm_div, distances = plot_diversity_metrics(\n",
    "                            df=top_df,\n",
    "                            feature_names=feature_names,\n",
    "                            save_path=os.path.join(plots_dir, 'euclidean_distance_matrix.pdf')\n",
    "                        )\n",
    "                        \n",
    "                        with open(os.path.join(metrics_dir, 'diversity_metrics.txt'), 'w') as f:\n",
    "                            f.write(f\"Average Diversity - Last State: {avg_div:.4f}\\n\")\n",
    "                            f.write(f\"Normalized Diversity - Last State: {norm_div:.4f}\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"     Diversity analysis failed for {method_name}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     Summary analysis failed for {method_name}: {e}\")\n",
    "            \n",
    "            # Reward distribution plot - use all sampled data\n",
    "            try:\n",
    "                plot_reward_distribution(final_rewards, \n",
    "                                        save_path=os.path.join(plots_dir, 'reward_distribution.pdf'))\n",
    "            except Exception as e:\n",
    "                print(f\"     Reward distribution plotting failed for {method_name}: {e}\")\n",
    "            \n",
    "            combination_results[method_name] = {\n",
    "                'metrics': metrics_dict,\n",
    "                'rewards_summary': rewards_dict,\n",
    "                'output_dir': method_output_dir,\n",
    "                'n_trajectories': len(trajectories),\n",
    "                'n_top_analyzed': len(trajectories_top),\n",
    "                'sample_percentage_used': sample_percentage,\n",
    "                'selection_method': 'random_per_run'\n",
    "            }\n",
    "            \n",
    "            print(f\"     {method_name.upper()}: avg_reward={metrics_dict['average_reward']:.2f}, median_reward={metrics_dict['median_reward']:.2f} (analyzed {len(trajectories)} randomly sampled trajectories from {sample_percentage*100:.0f}% of each run)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     Error processing {method_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return combination_results\n",
    "\n",
    "def generate_comparison_tables(all_results, output_dir):\n",
    "    \"\"\"\n",
    "    Generate comprehensive comparison tables across all oracles and model types.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    tables_dir = os.path.join(output_dir, 'comparison_tables')\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    \n",
    "    # Enhanced key metrics to compare (including new quantile-based metrics)\n",
    "    key_metrics = ['average_reward', 'median_reward', 'max_reward', 'quantile_90', 'quantile_75', \n",
    "                   'quantile_25', 'quantile_10', 'iqr', 'tail_coverage_20', 'expected_shortfall_10', 'expected_shortfall_20']\n",
    "    methods = ['gflow', 'reinforce', 'reinforce_baseline', 'sac', 'smcmc']\n",
    "    \n",
    "    # 1. Create comparison table for each metric\n",
    "    for metric in key_metrics:\n",
    "        print(f\"Creating comparison table for {metric}...\")\n",
    "        \n",
    "        rows = []\n",
    "        \n",
    "        for oracle in [1, 2, 3]:\n",
    "            for model_type in ['gbr', 'mlp', 'rfr','elastic','ens']:\n",
    "                row_data = {'Oracle': oracle, 'Model_Type': model_type}\n",
    "                \n",
    "                combination_key = f\"Oracle_{oracle}_{model_type.upper()}\"\n",
    "                \n",
    "                if combination_key in all_results:\n",
    "                    for method in methods:\n",
    "                        if method in all_results[combination_key]:\n",
    "                            value = all_results[combination_key][method]['metrics'].get(metric, np.nan)\n",
    "                            row_data[method.upper()] = value\n",
    "                        else:\n",
    "                            row_data[method.upper()] = np.nan\n",
    "                else:\n",
    "                    # Fill with NaN if combination doesn't exist\n",
    "                    for method in methods:\n",
    "                        row_data[method.upper()] = np.nan\n",
    "                \n",
    "                rows.append(row_data)\n",
    "        \n",
    "        # Create DataFrame and save\n",
    "        metric_df = pd.DataFrame(rows)\n",
    "        metric_df.to_csv(os.path.join(tables_dir, f'{metric}_comparison_table.csv'), index=False)\n",
    "    \n",
    "    # 2. Create enhanced master summary table\n",
    "    summary_rows = []\n",
    "    for combination_key, combination_data in all_results.items():\n",
    "        oracle, model_type = combination_key.split('_')[1], combination_key.split('_')[2]\n",
    "        \n",
    "        for method, method_data in combination_data.items():\n",
    "            row = {\n",
    "                'Oracle': oracle,\n",
    "                'Model_Type': model_type,\n",
    "                'Method': method.upper(),\n",
    "                'Avg_Reward': method_data['metrics'].get('average_reward', np.nan),\n",
    "                'Median_Reward': method_data['metrics'].get('median_reward', np.nan),  # NEW\n",
    "                'Max_Reward': method_data['metrics'].get('max_reward', np.nan),\n",
    "                'Std_Reward': method_data['metrics'].get('std_reward', np.nan),\n",
    "                'Q10': method_data['metrics'].get('quantile_10', np.nan),  # NEW\n",
    "                'Q25': method_data['metrics'].get('quantile_25', np.nan),  # NEW\n",
    "                'Q75': method_data['metrics'].get('quantile_75', np.nan),  # NEW\n",
    "                'Q90': method_data['metrics'].get('quantile_90', np.nan),  # NEW\n",
    "                'IQR': method_data['metrics'].get('iqr', np.nan),  # NEW\n",
    "                'Tail_Coverage_20': method_data['metrics'].get('tail_coverage_20', np.nan),\n",
    "                'Expected_Shortfall_10': method_data['metrics'].get('expected_shortfall_10', np.nan),\n",
    "                'Expected_Shortfall_20': method_data['metrics'].get('expected_shortfall_20', np.nan),\n",
    "                'N_Trajectories': method_data.get('n_trajectories', 'N/A'),\n",
    "                'N_Top_Analyzed': method_data.get('n_top_analyzed', 'N/A')\n",
    "            }\n",
    "            summary_rows.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(os.path.join(tables_dir, 'master_summary_table.csv'), index=False)\n",
    "    \n",
    "    # 3. Create ranking tables (updated to include median-based ranking)\n",
    "    ranking_data = []\n",
    "    for combination_key, combination_data in all_results.items():\n",
    "        oracle, model_type = combination_key.split('_')[1], combination_key.split('_')[2]\n",
    "        \n",
    "        # Rank methods by average reward\n",
    "        method_rewards = [(method, data['metrics'].get('average_reward', -np.inf)) \n",
    "                         for method, data in combination_data.items()]\n",
    "        method_rewards.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Rank methods by median reward (NEW)\n",
    "        method_medians = [(method, data['metrics'].get('median_reward', -np.inf)) \n",
    "                         for method, data in combination_data.items()]\n",
    "        method_medians.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        ranking_data.append({\n",
    "            'Oracle': oracle,\n",
    "            'Model_Type': model_type,\n",
    "            'Best_Method_Mean': method_rewards[0][0].upper() if method_rewards else 'N/A',\n",
    "            'Best_Mean_Reward': method_rewards[0][1] if method_rewards else np.nan,\n",
    "            'Best_Method_Median': method_medians[0][0].upper() if method_medians else 'N/A',  # NEW\n",
    "            'Best_Median_Reward': method_medians[0][1] if method_medians else np.nan,  # NEW\n",
    "            'Second_Method': method_rewards[1][0].upper() if len(method_rewards) > 1 else 'N/A',\n",
    "            'Second_Reward': method_rewards[1][1] if len(method_rewards) > 1 else np.nan,\n",
    "            'Worst_Method': method_rewards[-1][0].upper() if method_rewards else 'N/A',\n",
    "            'Worst_Reward': method_rewards[-1][1] if method_rewards else np.nan,\n",
    "            'Methods_Count': len(method_rewards)\n",
    "        })\n",
    "    \n",
    "    ranking_df = pd.DataFrame(ranking_data)\n",
    "    ranking_df.to_csv(os.path.join(tables_dir, 'method_rankings.csv'), index=False)\n",
    "    \n",
    "    print(f\" All comparison tables saved to {tables_dir}\")\n",
    "    return summary_df, ranking_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results, output_dir = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_oracle_model_specific_latex_tables(analysis_base_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Generate LaTeX tables for each oracle-model combination to analyze generalization.\n",
    "    \n",
    "    Args:\n",
    "        analysis_base_path: Path to comprehensive analysis results folder\n",
    "        output_path: Optional path to save the LaTeX files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains LaTeX tables for each oracle-model combination\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import re\n",
    "    \n",
    "    analysis_path = Path(analysis_base_path)\n",
    "    \n",
    "    # Define structure\n",
    "    oracles = {1: \"2002\", 2: \"2008\", 3: \"2021\"}\n",
    "    model_types = [\"GBR\", \"MLP\", \"RFR\",'ELASTIC','ENS']\n",
    "    methods = {\n",
    "        \"gflow\": \"GFlowNet\",\n",
    "        \"reinforce\": \"REINFORCE\", \n",
    "        \"sac\": \"SAC\",\n",
    "        \"smcmc\": \"SMCMC\"\n",
    "    }\n",
    "    \n",
    "    def parse_metrics_file(filepath):\n",
    "        \"\"\"Parse a metrics file and extract key-value pairs.\"\"\"\n",
    "        metrics = {}\n",
    "        if not filepath.exists():\n",
    "            return metrics\n",
    "            \n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if ':' in line and not line.startswith('#'):\n",
    "                    key, value = line.split(':', 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    try:\n",
    "                        if '' in value:\n",
    "                            mean_part = value.split('')[0].strip()\n",
    "                            metrics[key] = float(mean_part)\n",
    "                        else:\n",
    "                            metrics[key] = float(value)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        return metrics\n",
    "    \n",
    "    # Collect all metrics data organized by oracle-model combination\n",
    "    oracle_model_data = {}\n",
    "    \n",
    "    print(\"Collecting data for oracle-model combinations...\")\n",
    "    \n",
    "    for oracle_num, oracle_year in oracles.items():\n",
    "        for model_type in model_types:\n",
    "            combination_key = f\"Oracle_{oracle_num}_{model_type}\"\n",
    "            oracle_folder = analysis_path / combination_key\n",
    "            \n",
    "            if not oracle_folder.exists():\n",
    "                print(f\" Oracle folder not found: {oracle_folder}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {combination_key}...\")\n",
    "            oracle_model_data[combination_key] = {\n",
    "                'oracle_year': oracle_year,\n",
    "                'model_type': model_type,\n",
    "                'diversity_data': {},\n",
    "                'performance_data': {}\n",
    "            }\n",
    "            \n",
    "            for method_key, method_name in methods.items():\n",
    "                method_folder = oracle_folder / method_key\n",
    "                \n",
    "                if not method_folder.exists():\n",
    "                    print(f\"  Method folder not found: {method_folder}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read diversity metrics\n",
    "                dtw_file = method_folder / \"metrics\" / \"dtw_metrics.txt\"\n",
    "                div_file = method_folder / \"metrics\" / \"diversity_metrics.txt\"\n",
    "                \n",
    "                dtw_metrics = parse_metrics_file(dtw_file)\n",
    "                div_metrics = parse_metrics_file(div_file)\n",
    "                combined_div_metrics = {**dtw_metrics, **div_metrics}\n",
    "                \n",
    "                # Read performance metrics  \n",
    "                perf_file = method_folder / \"metrics\" / \"performance_metrics.txt\"\n",
    "                perf_metrics = parse_metrics_file(perf_file)\n",
    "                \n",
    "                # Store data for this combination\n",
    "                if combined_div_metrics:\n",
    "                    oracle_model_data[combination_key]['diversity_data'][method_name] = combined_div_metrics\n",
    "                if perf_metrics:\n",
    "                    oracle_model_data[combination_key]['performance_data'][method_name] = perf_metrics\n",
    "    \n",
    "    # Generate tables for each oracle-model combination\n",
    "    all_tables = {}\n",
    "    \n",
    "    # Update metric mappings\n",
    "    diversity_metrics = {\n",
    "        \"Average Distance\": \"DTW Distance\",\n",
    "        \"Normalized Score\": \"DTW Normalized\", \n",
    "        \"Average Diversity - Last State\": \"Terminal Diversity\",\n",
    "        \"Normalized Diversity - Last State\": \"Terminal Normalized\"\n",
    "    }\n",
    "    \n",
    "    # UPDATED: Changed from average_reward and max_reward to median_reward and quantile_90\n",
    "    performance_metrics = {\n",
    "        \"median_reward\": \"Median Reward\",      # CHANGED: from average_reward to median_reward\n",
    "        \"quantile_90\": \"Q90 Reward\",          # CHANGED: from max_reward to quantile_90\n",
    "        \"tail_coverage_20\": \"TC@20\",\n",
    "        \"expected_shortfall_10\": \"ES@10%\",\n",
    "        \"expected_shortfall_20\": \"ES@20%\"\n",
    "    }\n",
    "    \n",
    "    def find_best_method(data_dict, metric_key, higher_better=True):\n",
    "        \"\"\"Find the best performing method for a given metric.\"\"\"\n",
    "        if not data_dict:\n",
    "            return None\n",
    "            \n",
    "        method_values = []\n",
    "        for method_name, metrics in data_dict.items():\n",
    "            if metric_key in metrics:\n",
    "                method_values.append((method_name, metrics[metric_key]))\n",
    "        \n",
    "        if not method_values:\n",
    "            return None\n",
    "        \n",
    "        if higher_better:\n",
    "            best_method = max(method_values, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            best_method = min(method_values, key=lambda x: x[1])[0]\n",
    "        \n",
    "        return best_method\n",
    "    \n",
    "    for combination_key, combination_data in oracle_model_data.items():\n",
    "        oracle_year = combination_data['oracle_year']\n",
    "        model_type = combination_data['model_type']\n",
    "        \n",
    "        print(f\"Generating tables for {combination_key}...\")\n",
    "        \n",
    "        # DIVERSITY TABLE for this combination\n",
    "        diversity_latex = f\"\"\"\\\\begin{{table}}[t]\n",
    "\\\\caption{{Diversity metrics for {model_type} oracle on {oracle_year} financial crash. Methods were trained using {model_type} oracle to test generalization. Higher values indicate better diversity. Results from 100 trajectories per method.}}\n",
    "\\\\label{{tab:diversity_{combination_key.lower()}}}\n",
    "\\\\centering\n",
    "\\\\begin{{tabular}}{{l|cccc}}\n",
    "\\\\toprule\n",
    "\\\\textbf{{Method}} & \\\\textbf{{DTW Distance}} & \\\\textbf{{DTW Normalized}} & \\\\textbf{{Terminal Diversity}} & \\\\textbf{{Terminal Normalized}} \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "        \n",
    "        diversity_data = combination_data['diversity_data']\n",
    "        \n",
    "        for method_name in [\"GFlowNet\", \"REINFORCE\", \"SAC\", \"SMCMC\"]:\n",
    "            if method_name not in diversity_data:\n",
    "                continue\n",
    "                \n",
    "            row_data = []\n",
    "            \n",
    "            for metric_key, metric_display in diversity_metrics.items():\n",
    "                if metric_key in diversity_data[method_name]:\n",
    "                    value = diversity_data[method_name][metric_key]\n",
    "                    \n",
    "                    # Check if this is the best method for this metric\n",
    "                    best_method = find_best_method(diversity_data, metric_key, higher_better=True)\n",
    "                    if best_method == method_name:\n",
    "                        row_data.append(f\"\\\\textbf{{{value:.2f}}}\")\n",
    "                    else:\n",
    "                        row_data.append(f\"{value:.2f}\")\n",
    "                else:\n",
    "                    row_data.append(\"--\")\n",
    "            \n",
    "            if row_data:  # Only add row if we have data\n",
    "                row_str = f\"{method_name:<12} & {' & '.join(row_data)} \\\\\\\\\\n\"\n",
    "                diversity_latex += row_str\n",
    "        \n",
    "        diversity_latex += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # PERFORMANCE TABLE for this combination - UPDATED\n",
    "        performance_latex = f\"\"\"\\\\begin{{table}}[t]\n",
    "\\\\caption{{Performance metrics for {model_type} oracle on {oracle_year} financial crash. Methods were trained using {model_type} oracle to test generalization. Higher is better for Median/Q90 Reward and TC@20; lower is better for ES metrics.}}\n",
    "\\\\label{{tab:performance_{combination_key.lower()}}}\n",
    "\\\\centering\n",
    "\\\\begin{{tabular}}{{l|ccccc}}\n",
    "\\\\toprule\n",
    "\\\\textbf{{Method}} & \\\\textbf{{Median Reward}} & \\\\textbf{{Q90 Reward}} & \\\\textbf{{TC@20}} & \\\\textbf{{ES@10\\\\%}} & \\\\textbf{{ES@20\\\\%}} \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "        \n",
    "        performance_data = combination_data['performance_data']\n",
    "        # UPDATED: Changed higher_better_metrics to include new metrics\n",
    "        higher_better_metrics = [\"median_reward\", \"quantile_90\", \"tail_coverage_20\"]\n",
    "        \n",
    "        for method_name in [\"GFlowNet\", \"REINFORCE\", \"SAC\", \"SMCMC\"]:\n",
    "            if method_name not in performance_data:\n",
    "                continue\n",
    "                \n",
    "            row_data = []\n",
    "            \n",
    "            for metric_key, metric_display in performance_metrics.items():\n",
    "                if metric_key in performance_data[method_name]:\n",
    "                    value = performance_data[method_name][metric_key]\n",
    "                    \n",
    "                    # Check if this is the best method for this metric\n",
    "                    is_higher_better = metric_key in higher_better_metrics\n",
    "                    best_method = find_best_method(performance_data, metric_key, higher_better=is_higher_better)\n",
    "                    \n",
    "                    if best_method == method_name:\n",
    "                        row_data.append(f\"\\\\textbf{{{value:.2f}}}\")\n",
    "                    else:\n",
    "                        row_data.append(f\"{value:.2f}\")\n",
    "                else:\n",
    "                    row_data.append(\"--\")\n",
    "            \n",
    "            if row_data:  # Only add row if we have data\n",
    "                row_str = f\"{method_name:<12} & {' & '.join(row_data)} \\\\\\\\\\n\"\n",
    "                performance_latex += row_str\n",
    "        \n",
    "        performance_latex += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Store tables for this combination\n",
    "        all_tables[combination_key] = {\n",
    "            'diversity_table': diversity_latex,\n",
    "            'performance_table': performance_latex\n",
    "        }\n",
    "    \n",
    "    # Save to files if output path provided\n",
    "    if output_path:\n",
    "        output_path = Path(output_path)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save individual tables\n",
    "        for combination_key, tables in all_tables.items():\n",
    "            combo_dir = output_path / combination_key\n",
    "            combo_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            with open(combo_dir / \"diversity_table.tex\", 'w') as f:\n",
    "                f.write(tables['diversity_table'])\n",
    "            \n",
    "            with open(combo_dir / \"performance_table.tex\", 'w') as f:\n",
    "                f.write(tables['performance_table'])\n",
    "        \n",
    "        # Create combined file with all tables\n",
    "        combined_latex = \"\"\n",
    "        for combination_key in sorted(all_tables.keys()):\n",
    "            oracle_num = combination_key.split('_')[1]\n",
    "            model_type = combination_key.split('_')[2]\n",
    "            \n",
    "            combined_latex += f\"% Tables for Oracle {oracle_num} ({oracles[int(oracle_num)]}) - {model_type} Model\\n\"\n",
    "            combined_latex += f\"% =\" * 60 + \"\\n\\n\"\n",
    "            combined_latex += all_tables[combination_key]['diversity_table']\n",
    "            combined_latex += \"\\n\"\n",
    "            combined_latex += all_tables[combination_key]['performance_table']\n",
    "            combined_latex += \"\\n\\\\clearpage\\n\\n\"\n",
    "        \n",
    "        with open(output_path / \"all_oracle_model_tables.tex\", 'w') as f:\n",
    "            f.write(combined_latex)\n",
    "        \n",
    "        print(f\"\\nTables saved to {output_path}\")\n",
    "        print(f\"Individual tables saved in subfolders\")\n",
    "        print(f\"Combined file: all_oracle_model_tables.tex\")\n",
    "    \n",
    "    return all_tables\n",
    "\n",
    "def generate_generalization_analysis_tables(analysis_base_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Generate specialized tables to analyze cross-oracle generalization.\n",
    "    Shows how each method performs across different oracles when trained on a specific one.\n",
    "    \"\"\"\n",
    "    # This function will show, for example:\n",
    "    # - GFlowNet trained on MLP oracle: performance on GBR oracle, RFR oracle, etc.\n",
    "    \n",
    "    all_tables = generate_oracle_model_specific_latex_tables(analysis_base_path, output_path)\n",
    "    \n",
    "    if not output_path:\n",
    "        return all_tables\n",
    "    \n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # UPDATED: Create generalization summary table with median and Q90\n",
    "    generalization_latex = \"\"\"\\\\begin{table}[t]\n",
    "\\\\caption{Cross-Oracle Generalization Analysis: Performance of methods across different oracle types. Each row shows a method's performance when applied to different oracle types (trained on one, tested on all).}\n",
    "\\\\label{tab:generalization_analysis}\n",
    "\\\\scriptsize\n",
    "\\\\centering\n",
    "\\\\resizebox{\\\\textwidth}{!}{%\n",
    "\\\\begin{tabular}{l|l|ccc|ccc}\n",
    "\\\\toprule\n",
    "\\\\multirow{2}{*}{\\\\textbf{Method}} & \\\\multirow{2}{*}{\\\\textbf{Trained On}} & \\\\multicolumn{3}{c|}{\\\\textbf{Median Reward}} & \\\\multicolumn{3}{c}{\\\\textbf{Q90 Reward}} \\\\\\\\\n",
    "& & \\\\textbf{GBR} & \\\\textbf{MLP} & \\\\textbf{RFR} & \\\\textbf{GBR} & \\\\textbf{MLP} & \\\\textbf{RFR} \\\\\\\\\n",
    "\\\\midrule\n",
    "\"\"\"\n",
    "    \n",
    "    # Parse the data to create generalization analysis\n",
    "    oracles = {1: \"2002\", 2: \"2008\", 3: \"2021\"}\n",
    "    model_types = [\"GBR\", \"MLP\", \"RFR\",'ELASTIC','ENS']\n",
    "    methods = [\"GFlowNet\", \"REINFORCE\", \"SAC\", \"SMCMC\"]\n",
    "    \n",
    "    # For each oracle year, show how methods perform across model types\n",
    "    for oracle_num, oracle_year in oracles.items():\n",
    "        generalization_latex += f\"\\\\multicolumn{{8}}{{c}}{{\\\\textbf{{{oracle_year} Financial Crisis}}}} \\\\\\\\\\n\"\n",
    "        generalization_latex += \"\\\\midrule\\n\"\n",
    "        \n",
    "        for method in methods:\n",
    "            # Check if this method has data across all model types for this oracle\n",
    "            method_data = {}\n",
    "            for model_type in model_types:\n",
    "                combination_key = f\"Oracle_{oracle_num}_{model_type}\"\n",
    "                if combination_key in all_tables:\n",
    "                    # Extract performance data from the LaTeX (this is a simplified approach)\n",
    "                    # In practice, you'd want to parse the original data\n",
    "                    method_data[model_type] = \"Data Available\"\n",
    "            \n",
    "            if method_data:\n",
    "                # Add method row (simplified version)\n",
    "                generalization_latex += f\"{method:<12} & Various & -- & -- & -- & -- & -- & -- \\\\\\\\\\n\"\n",
    "    \n",
    "    generalization_latex += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}%\n",
    "}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_path / \"generalization_analysis.tex\", 'w') as f:\n",
    "        f.write(generalization_latex)\n",
    "    \n",
    "    return all_tables\n",
    "\n",
    "# Usage:\n",
    "tables = generate_oracle_model_specific_latex_tables(\n",
    "    \"analysis_outputs/Comprehensive_Analysis_20250925_132228\",\n",
    "    output_path=\"results/oracle_model_tables\"\n",
    ")\n",
    "\n",
    "# Print summary of generated tables\n",
    "print(\"\\nGenerated tables for oracle-model combinations:\")\n",
    "for combination, table_data in tables.items():\n",
    "    oracle_num = combination.split('_')[1]\n",
    "    model_type = combination.split('_')[2]\n",
    "    print(f\"   {combination}: Oracle {oracle_num} with {model_type} model\")\n",
    "\n",
    "print(f\"\\nTotal combinations: {len(tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oracle_specific_reward_distributions(analysis_base_path, save_dir=None, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Create professional reward distribution plots for each oracle (use case),\n",
    "    showing all methods across different model types with family colors.\n",
    "    \n",
    "    Args:\n",
    "        analysis_base_path: Path to comprehensive analysis results folder\n",
    "        save_dir: Directory to save the plots\n",
    "        confidence_level: Confidence level for statistical annotations\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary statistics for each oracle-method-model combination\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    from scipy import stats\n",
    "    \n",
    "    analysis_path = Path(analysis_base_path)\n",
    "    if save_dir:\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Define structure and colors\n",
    "    oracles = {1: \"2002 Financial Crisis\", 2: \"2008 Financial Crisis\", 3: \"2021 Market Volatility\"}\n",
    "    model_types = ['GBR', 'MLP', 'RFR','ELASTIC','ENS']\n",
    "    methods = ['gflow', 'reinforce', 'sac', 'smcmc']\n",
    "    \n",
    "    # Define professional color families\n",
    "    color_families = {\n",
    "        'gflow': {\n",
    "            'GBR': '#27ae60',    # Dark green\n",
    "            'MLP': '#2ecc71',    # Medium green  \n",
    "            'RFR': '#58d68d',     # Light green\n",
    "            'ELASTIC': '#1e8449',\n",
    "            'ENS': '#52be80'\n",
    "        },\n",
    "        'reinforce': {\n",
    "            'GBR': '#2980b9',    # Dark blue\n",
    "            'MLP': '#3498db',    # Medium blue\n",
    "            'RFR': '#85c1e9',    # Light blue\n",
    "            'ELASTIC': '#1f618d',\n",
    "            'ENS': '#3498db'\n",
    "        },\n",
    "        'sac': {\n",
    "            'GBR': '#8e44ad',    # Dark purple\n",
    "            'MLP': '#9b59b6',    # Medium purple\n",
    "            'RFR': '#bb8fce',    # Light purple\n",
    "            'ELASTIC': '#6c3483',\n",
    "            'ENS': '#a569bd'\n",
    "        },\n",
    "        'smcmc': {\n",
    "            'GBR': '#c0392b',    # Dark red\n",
    "            'MLP': '#e74c3c',    # Medium red\n",
    "            'RFR': '#f1948a',    # Light red\n",
    "            'ELASTIC': '#922b21',\n",
    "            'ENS': '#cd6155'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Collect all data\n",
    "    print(\"Collecting reward data from all combinations...\")\n",
    "    oracle_data = {}\n",
    "    \n",
    "    for oracle_num, oracle_name in oracles.items():\n",
    "        print(f\"Processing Oracle {oracle_num} ({oracle_name})...\")\n",
    "        oracle_data[oracle_num] = {\n",
    "            'name': oracle_name,\n",
    "            'method_model_data': {},\n",
    "            'statistics': {}\n",
    "        }\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            for method in methods:\n",
    "                combination_key = f\"Oracle_{oracle_num}_{model_type}\"\n",
    "                method_folder = analysis_path / combination_key / method\n",
    "                csv_file = method_folder / \"metrics\" / \"top_100_summary.csv\"\n",
    "                \n",
    "                if csv_file.exists():\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        if 'FinalReward' in df.columns:\n",
    "                            rewards = df['FinalReward'].values\n",
    "                            rewards = rewards[np.isfinite(rewards)]  # Remove any NaN values\n",
    "                            \n",
    "                            if len(rewards) > 0:\n",
    "                                key = f\"{method}_{model_type}\"\n",
    "                                oracle_data[oracle_num]['method_model_data'][key] = {\n",
    "                                    'rewards': rewards,\n",
    "                                    'method': method,\n",
    "                                    'model_type': model_type,\n",
    "                                    'color': color_families[method][model_type],\n",
    "                                    'n_samples': len(rewards)\n",
    "                                }\n",
    "                                print(f\"   Loaded {key}: {len(rewards)} samples\")\n",
    "                        else:\n",
    "                            print(f\"   No 'FinalReward' column in {csv_file}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error loading {csv_file}: {e}\")\n",
    "                else:\n",
    "                    print(f\"  - No data file: {csv_file}\")\n",
    "    \n",
    "    # Create plots for each oracle\n",
    "    summary_stats = {}\n",
    "    \n",
    "    for oracle_num, oracle_info in oracle_data.items():\n",
    "        if not oracle_info['method_model_data']:\n",
    "            print(f\"No data for Oracle {oracle_num}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nCreating plot for Oracle {oracle_num}...\")\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(16, 10), dpi=300)\n",
    "        \n",
    "        # Set up the plot style\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        \n",
    "        # Plot distributions for each method-model combination\n",
    "        legend_elements = []\n",
    "        stats_data = []\n",
    "        \n",
    "        for key, data in oracle_info['method_model_data'].items():\n",
    "            method = data['method']\n",
    "            model_type = data['model_type']\n",
    "            rewards = data['rewards']\n",
    "            color = data['color']\n",
    "            \n",
    "            # Plot KDE\n",
    "            sns.kdeplot(data=rewards, color=color, linewidth=3, alpha=0.8)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            mean_reward = np.mean(rewards)\n",
    "            std_reward = np.std(rewards, ddof=1)\n",
    "            median_reward = np.median(rewards)\n",
    "            q25, q75 = np.percentile(rewards, [25, 75])\n",
    "            \n",
    "            # Add vertical line for mean\n",
    "            plt.axvline(mean_reward, color=color, linestyle='--', alpha=0.6, linewidth=2)\n",
    "            \n",
    "            # Create legend entry\n",
    "            method_name = {\n",
    "                'gflow': 'GFlowNet',\n",
    "                'reinforce': 'REINFORCE', \n",
    "                'sac': 'SAC',\n",
    "                'smcmc': 'SMCMC'\n",
    "            }[method]\n",
    "            \n",
    "            legend_elements.append(\n",
    "                plt.Line2D([0], [0], color=color, linewidth=3,\n",
    "                          label=f'{method_name}-{model_type} (={mean_reward:.1f}{std_reward:.1f})')\n",
    "            )\n",
    "            \n",
    "            # Store statistics\n",
    "            stats_data.append({\n",
    "                'oracle': oracle_num,\n",
    "                'method': method_name,\n",
    "                'model_type': model_type,\n",
    "                'mean': mean_reward,\n",
    "                'std': std_reward,\n",
    "                'median': median_reward,\n",
    "                'q25': q25,\n",
    "                'q75': q75,\n",
    "                'n_samples': len(rewards)\n",
    "            })\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f'Reward Distributions: {oracle_info[\"name\"]}\\n'\n",
    "                 f'Comparison across Methods and Oracle Model Types',\n",
    "                 fontsize=18, fontweight='bold', pad=30)\n",
    "        \n",
    "        plt.xlabel('Final Reward Values', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('Density', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Professional styling\n",
    "        plt.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        # Create custom legend with method families grouped\n",
    "        legend_by_method = {}\n",
    "        for element in legend_elements:\n",
    "            method_name = element.get_label().split('-')[0]\n",
    "            if method_name not in legend_by_method:\n",
    "                legend_by_method[method_name] = []\n",
    "            legend_by_method[method_name].append(element)\n",
    "        \n",
    "        # Create legend with family grouping\n",
    "        all_handles = []\n",
    "        for method_name in ['GFlowNet', 'REINFORCE', 'SAC', 'SMCMC']:\n",
    "            if method_name in legend_by_method:\n",
    "                all_handles.extend(legend_by_method[method_name])\n",
    "                # Add small separator (invisible)\n",
    "                if method_name != 'SMCMC':\n",
    "                    all_handles.append(plt.Line2D([0], [0], color='white', linewidth=0, label=''))\n",
    "        \n",
    "        legend = plt.legend(handles=all_handles, bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                          frameon=True, framealpha=0.95, edgecolor='black', \n",
    "                          title='Method-Model Combinations', title_fontsize=13)\n",
    "        legend.get_title().set_fontweight('bold')\n",
    "        \n",
    "        # Remove spines for cleaner look\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "        plt.gca().spines['left'].set_linewidth(0.5)\n",
    "        plt.gca().spines['bottom'].set_linewidth(0.5)\n",
    "        \n",
    "        # Add summary statistics text box\n",
    "        stats_text = f\"Oracle {oracle_num} Summary:\\n\"\n",
    "        stats_text += f\"Total Combinations: {len(oracle_info['method_model_data'])}\\n\"\n",
    "        \n",
    "        # Find best performing combination\n",
    "        best_combo = max(oracle_info['method_model_data'].items(), \n",
    "                        key=lambda x: np.mean(x[1]['rewards']))\n",
    "        best_key, best_data = best_combo\n",
    "        best_method = best_data['method'].replace('gflow', 'GFlowNet').replace('reinforce', 'REINFORCE').upper()\n",
    "        best_model = best_data['model_type']\n",
    "        best_reward = np.mean(best_data['rewards'])\n",
    "        \n",
    "        stats_text += f\"Best: {best_method}-{best_model} ({best_reward:.1f})\"\n",
    "        \n",
    "        plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "                verticalalignment='top', horizontalalignment='left',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, \n",
    "                         edgecolor='gray', linewidth=1),\n",
    "                fontsize=11, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        if save_dir:\n",
    "            filename = f\"oracle_{oracle_num}_reward_distributions.pdf\"\n",
    "            plt.savefig(save_dir / filename, dpi=300, bbox_inches='tight')\n",
    "            print(f\"   Saved plot: {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Store statistics\n",
    "        summary_stats[oracle_num] = {\n",
    "            'oracle_name': oracle_info['name'],\n",
    "            'statistics': stats_data,\n",
    "            'best_combination': {\n",
    "                'method': best_method,\n",
    "                'model_type': best_model,\n",
    "                'mean_reward': best_reward\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create summary comparison plot across all oracles\n",
    "    create_oracle_comparison_plot(summary_stats, color_families, save_dir)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    if save_dir:\n",
    "        save_summary_statistics(summary_stats, save_dir)\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "def create_oracle_comparison_plot(summary_stats, color_families, save_dir=None):\n",
    "    \"\"\"Create a comparison plot showing best methods across all oracles.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(18, 12), dpi=300)\n",
    "    \n",
    "    # Create subplots for each oracle\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(24, 8), dpi=300, sharey=True)\n",
    "    fig.suptitle('Cross-Oracle Performance Comparison: Method Families Across Financial Crises',\n",
    "                 fontsize=20, fontweight='bold', y=0.95)\n",
    "    \n",
    "    oracle_names = {1: \"2002 Crisis\", 2: \"2008 Crisis\", 3: \"2021 Volatility\"}\n",
    "    \n",
    "    for idx, (oracle_num, oracle_data) in enumerate(summary_stats.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Group statistics by method family\n",
    "        method_groups = {}\n",
    "        for stat in oracle_data['statistics']:\n",
    "            method = stat['method'].lower().replace('flownet', 'flow')\n",
    "            if method not in method_groups:\n",
    "                method_groups[method] = []\n",
    "            method_groups[method].append(stat)\n",
    "        \n",
    "        # Plot each method family\n",
    "        positions = []\n",
    "        labels = []\n",
    "        colors_used = []\n",
    "        \n",
    "        pos = 0\n",
    "        for method in ['gflownet', 'reinforce', 'sac', 'smcmc']:\n",
    "            if method not in method_groups:\n",
    "                continue\n",
    "                \n",
    "            method_key = method.replace('gflownet', 'gflow')\n",
    "            for model_stat in method_groups[method]:\n",
    "                model_type = model_stat['model_type']\n",
    "                mean_val = model_stat['mean']\n",
    "                std_val = model_stat['std']\n",
    "                \n",
    "                color = color_families[method_key][model_type]\n",
    "                \n",
    "                # Create bar\n",
    "                bar = ax.bar(pos, mean_val, yerr=std_val, capsize=5,\n",
    "                           color=color, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "                \n",
    "                positions.append(pos)\n",
    "                labels.append(f\"{model_stat['method']}\\n{model_type}\")\n",
    "                colors_used.append(color)\n",
    "                \n",
    "                # Add value label on bar\n",
    "                ax.text(pos, mean_val + std_val + 1, f'{mean_val:.1f}',\n",
    "                       ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "                \n",
    "                pos += 1\n",
    "            \n",
    "            pos += 0.5  # Add space between method families\n",
    "        \n",
    "        ax.set_title(f'{oracle_names[oracle_num]}', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "        ax.set_ylabel('Mean Final Reward' if idx == 0 else '', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir / \"oracle_comparison_bars.pdf\", dpi=300, bbox_inches='tight')\n",
    "        print(\" Saved oracle comparison plot\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def save_summary_statistics(summary_stats, save_dir):\n",
    "    \"\"\"Save detailed statistics to CSV and text files.\"\"\"\n",
    "    \n",
    "    # Create comprehensive statistics DataFrame\n",
    "    all_stats = []\n",
    "    for oracle_num, oracle_data in summary_stats.items():\n",
    "        for stat in oracle_data['statistics']:\n",
    "            stat_copy = stat.copy()\n",
    "            stat_copy['oracle_name'] = oracle_data['oracle_name']\n",
    "            all_stats.append(stat_copy)\n",
    "    \n",
    "    stats_df = pd.DataFrame(all_stats)\n",
    "    stats_df.to_csv(save_dir / \"reward_distribution_statistics.csv\", index=False)\n",
    "    \n",
    "    # Create summary report\n",
    "    with open(save_dir / \"oracle_analysis_summary.txt\", 'w') as f:\n",
    "        f.write(\"ORACLE-SPECIFIC REWARD DISTRIBUTION ANALYSIS SUMMARY\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for oracle_num, oracle_data in summary_stats.items():\n",
    "            f.write(f\"ORACLE {oracle_num}: {oracle_data['oracle_name']}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            best = oracle_data['best_combination']\n",
    "            f.write(f\"Best Combination: {best['method']}-{best['model_type']} \"\n",
    "                   f\"(Mean Reward: {best['mean_reward']:.2f})\\n\\n\")\n",
    "            \n",
    "            f.write(\"All Combinations:\\n\")\n",
    "            sorted_stats = sorted(oracle_data['statistics'], \n",
    "                                key=lambda x: x['mean'], reverse=True)\n",
    "            \n",
    "            for i, stat in enumerate(sorted_stats, 1):\n",
    "                f.write(f\"{i:2d}. {stat['method']}-{stat['model_type']:3s}: \"\n",
    "                       f\"={stat['mean']:6.2f}  {stat['std']:5.2f} \"\n",
    "                       f\"(n={stat['n_samples']})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(\" Saved detailed statistics and summary report\")\n",
    "\n",
    "# Usage example:\n",
    "summary_results = plot_oracle_specific_reward_distributions(\n",
    "    analysis_base_path=\"analysis_outputs/Comprehensive_Analysis_20250925_132228\",\n",
    "    save_dir=\"analysis_outputs/oracle_reward_distributions\",\n",
    "    confidence_level=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORACLE REWARD DISTRIBUTION ANALYSIS COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for oracle_num, oracle_data in summary_results.items():\n",
    "    best = oracle_data['best_combination']\n",
    "    print(f\"Oracle {oracle_num} ({oracle_data['oracle_name']}):\")\n",
    "    print(f\"  Best: {best['method']}-{best['model_type']} (={best['mean_reward']:.2f})\")\n",
    "    print(f\"  Total combinations analyzed: {len(oracle_data['statistics'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_combination_distributions(analysis_base_path, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create individual distribution plots for each oracle-model combination,\n",
    "    showing all methods for that specific combination with professional styling.\n",
    "    \n",
    "    Args:\n",
    "        analysis_base_path: Path to comprehensive analysis results folder\n",
    "        save_dir: Directory to save the plots\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary statistics for each combination\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    \n",
    "    analysis_path = Path(analysis_base_path)\n",
    "    if save_dir:\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Define structure and colors\n",
    "    oracles = {1: \"\", 2: \"\", 3: \"\"}\n",
    "    model_types = ['GBR', 'MLP', 'RFR','ELASTIC','ENS']\n",
    "    methods = ['gflow', 'reinforce', 'sac', 'smcmc']\n",
    "    \n",
    "    # Define consistent colors for methods (same across all combinations)\n",
    "    method_colors = {\n",
    "        'gflow': '#2ecc71',      # Green\n",
    "        'reinforce': '#3498db',   # Blue\n",
    "        'sac': '#9b59b6',        # Purple\n",
    "        'smcmc': '#e74c3c'       # Red\n",
    "    }\n",
    "    \n",
    "    method_names = {\n",
    "        'gflow': 'GFlowNet',\n",
    "        'reinforce': 'REINFORCE', \n",
    "        'sac': 'SAC',\n",
    "        'smcmc': 'SMCMC'\n",
    "    }\n",
    "    \n",
    "    all_combination_stats = {}\n",
    "    \n",
    "    print(\"Creating individual combination distribution plots...\")\n",
    "    \n",
    "    # Process each oracle-model combination\n",
    "    for oracle_num, oracle_name in oracles.items():\n",
    "        for model_type in model_types:\n",
    "            combination_key = f\"Oracle_{oracle_num}_{model_type}\"\n",
    "            print(f\"\\nProcessing {combination_key}...\")\n",
    "            \n",
    "            # Collect data for this specific combination\n",
    "            combination_data = {}\n",
    "            combination_stats = []\n",
    "            \n",
    "            for method in methods:\n",
    "                method_folder = analysis_path / combination_key / method\n",
    "                csv_file = method_folder / \"metrics\" / \"all_trajectories_summary.csv\"\n",
    "                \n",
    "                if csv_file.exists():\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        # Get top 5000 final reward samples\n",
    "                        if 'FinalReward' in df.columns:\n",
    "                            rewards = df['FinalReward'].values\n",
    "                            rewards = rewards[np.isfinite(rewards)]\n",
    "                            # rewards = np.clip(rewards, None, 100)  # Clip rewards to maximum of 100\n",
    "                            \n",
    "                            if len(rewards) > 0:\n",
    "                                combination_data[method] = {\n",
    "                                    'rewards': rewards,\n",
    "                                    'color': method_colors[method],\n",
    "                                    'name': method_names[method],\n",
    "                                    'n_samples': len(rewards)\n",
    "                                }\n",
    "                                \n",
    "                                # Calculate statistics\n",
    "                                mean_reward = np.mean(rewards)\n",
    "                                std_reward = np.std(rewards, ddof=1)\n",
    "                                median_reward = np.median(rewards)\n",
    "                                q25, q75 = np.percentile(rewards, [25, 75])\n",
    "                                \n",
    "                                combination_stats.append({\n",
    "                                    'oracle': oracle_num,\n",
    "                                    'model_type': model_type,\n",
    "                                    'method': method_names[method],\n",
    "                                    'mean': mean_reward,\n",
    "                                    'std': std_reward,\n",
    "                                    'median': median_reward,\n",
    "                                    'q25': q25,\n",
    "                                    'q75': q75,\n",
    "                                    'min': np.min(rewards),\n",
    "                                    'max': np.max(rewards),\n",
    "                                    'n_samples': len(rewards)\n",
    "                                })\n",
    "                                \n",
    "                                print(f\"   Loaded {method}: {len(rewards)} samples (={mean_reward:.2f})\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   Error loading {method}: {e}\")\n",
    "                else:\n",
    "                    print(f\"  - No data for {method}\")\n",
    "            \n",
    "            # Create plot if we have data\n",
    "            if combination_data:\n",
    "                # Create the plot\n",
    "                plt.figure(figsize=(12, 6), dpi=300)\n",
    "                \n",
    "                # Set professional style\n",
    "                sns.set_style(\"whitegrid\")\n",
    "                plt.rcParams.update({'font.size': 12})\n",
    "                \n",
    "                # Plot distributions for each method\n",
    "                legend_elements = []\n",
    "                \n",
    "                for method, data in combination_data.items():\n",
    "                    rewards = data['rewards']\n",
    "                    # rewards = np.clip(rewards, None, 100)  # Clip rewards to maximum of 100\n",
    "                    color = data['color']\n",
    "                    name = data['name']\n",
    "                    \n",
    "                    # Calculate statistics for legend\n",
    "                    mean_reward = np.mean(rewards)\n",
    "                    std_reward = np.std(rewards, ddof=1)\n",
    "                    \n",
    "                    # Plot KDE with professional styling\n",
    "                    sns.kdeplot(data=rewards, color=color, linewidth=4, alpha=0.8,\n",
    "                               label=f'{name} (={mean_reward:.1f}{std_reward:.1f})')\n",
    "                    \n",
    "                    # Add vertical line for mean\n",
    "                    plt.axvline(mean_reward, color=color, linestyle='--', alpha=0.7, linewidth=2)\n",
    "                    \n",
    "                    # Add shaded area for 1 std\n",
    "                    plt.axvspan(mean_reward - std_reward, mean_reward + std_reward, \n",
    "                               color=color, alpha=0.15)\n",
    "                \n",
    "                # Customize the plot\n",
    "                plt.title(f'',\n",
    "                         fontsize=18, fontweight='bold', pad=30)\n",
    "                \n",
    "                plt.xlabel('Final Reward Values', fontsize=14, fontweight='bold')\n",
    "                plt.ylabel('Density', fontsize=14, fontweight='bold')\n",
    "                \n",
    "                # Professional styling\n",
    "                plt.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "                \n",
    "                # Create legend\n",
    "                legend = plt.legend(fontsize=13, frameon=True, framealpha=0.95, \n",
    "                                  edgecolor='black', loc='upper left',\n",
    "                                  title='Methods Performance', title_fontsize=14)\n",
    "                legend.get_title().set_fontweight('bold')\n",
    "                \n",
    "                # Remove top and right spines\n",
    "                plt.gca().spines['top'].set_visible(False)\n",
    "                plt.gca().spines['right'].set_visible(False)\n",
    "                plt.gca().spines['left'].set_linewidth(1)\n",
    "                plt.gca().spines['bottom'].set_linewidth(1)\n",
    "                \n",
    "                # Add summary statistics text box\n",
    "                n_methods = len(combination_data)\n",
    "                best_method = max(combination_data.items(), \n",
    "                                key=lambda x: np.mean(x[1]['rewards']))\n",
    "                best_name = best_method[1]['name']\n",
    "                best_mean = np.mean(best_method[1]['rewards'])\n",
    "                \n",
    "                # Calculate performance spread\n",
    "                all_means = [np.mean(data['rewards']) for data in combination_data.values()]\n",
    "                performance_spread = max(all_means) - min(all_means)\n",
    "                \n",
    "                # stats_text = f\"Combination Summary:\\n\"\n",
    "                # stats_text += f\"Methods Compared: {n_methods}\\n\"\n",
    "                # stats_text += f\"Best Method: {best_name} ({best_mean:.1f})\\n\"\n",
    "                # stats_text += f\"Performance Spread: {performance_spread:.1f}\"\n",
    "                \n",
    "                # plt.text(0.98, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "                #         verticalalignment='top', horizontalalignment='right',\n",
    "                #         bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.95, \n",
    "                #                  edgecolor='gray', linewidth=1),\n",
    "                #         fontsize=11, fontweight='bold')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save plot\n",
    "                if save_dir:\n",
    "                    filename = f\"{combination_key}_methods_comparison.pdf\"\n",
    "                    plt.savefig(save_dir / filename, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"   Saved plot: {filename}\")\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "                # Store statistics for this combination\n",
    "                all_combination_stats[combination_key] = {\n",
    "                    'oracle_name': oracle_name,\n",
    "                    'model_type': model_type,\n",
    "                    'statistics': combination_stats,\n",
    "                    'best_method': best_name,\n",
    "                    'best_mean': best_mean,\n",
    "                    'performance_spread': performance_spread,\n",
    "                    'n_methods': n_methods\n",
    "                }\n",
    "            else:\n",
    "                print(f\"   No data found for {combination_key}\")\n",
    "    \n",
    "    # Create summary comparison across all combinations\n",
    "    create_combination_summary_plot(all_combination_stats, save_dir)\n",
    "    \n",
    "    # Save detailed statistics\n",
    "    if save_dir:\n",
    "        save_combination_statistics(all_combination_stats, save_dir)\n",
    "    \n",
    "    return all_combination_stats\n",
    "\n",
    "def create_combination_summary_plot(all_stats, save_dir=None):\n",
    "    \"\"\"Create a summary heatmap showing best methods for each combination.\"\"\"\n",
    "    \n",
    "    # Extract data for heatmap\n",
    "    oracles = [1, 2, 3]\n",
    "    model_types = ['GBR', 'MLP', 'RFR','ELASTIC','ENS']\n",
    "    \n",
    "    # Create performance matrix\n",
    "    performance_matrix = np.zeros((len(oracles), len(model_types)))\n",
    "    best_method_matrix = [[''] * len(model_types) for _ in range(len(oracles))]\n",
    "    \n",
    "    for i, oracle in enumerate(oracles):\n",
    "        for j, model_type in enumerate(model_types):\n",
    "            combination_key = f\"Oracle_{oracle}_{model_type}\"\n",
    "            if combination_key in all_stats:\n",
    "                performance_matrix[i, j] = all_stats[combination_key]['best_mean']\n",
    "                best_method_matrix[i][j] = all_stats[combination_key]['best_method']\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(20, 8), dpi=300)\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(performance_matrix, \n",
    "                     annot=True, fmt='.1f', \n",
    "                     xticklabels=model_types,\n",
    "                     yticklabels=[f\"Oracle {i} (2002)\" if i==1 else f\"Oracle {i} (2008)\" if i==2 else f\"Oracle {i} (2021)\" for i in oracles],\n",
    "                     cmap='RdYlGn', center=50,\n",
    "                     cbar_kws={'label': 'Best Method Performance'})\n",
    "    \n",
    "    # Add best method annotations\n",
    "    for i in range(len(oracles)):\n",
    "        for j in range(len(model_types)):\n",
    "            if best_method_matrix[i][j]:\n",
    "                ax.text(j + 0.5, i + 0.7, best_method_matrix[i][j], \n",
    "                       ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "                       color='white' if performance_matrix[i, j] < 50 else 'black')\n",
    "    \n",
    "    plt.title('Best Method Performance Across All Combinations\\n'\n",
    "             'Numbers show best performance, text shows best method',\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.xlabel('Oracle Model Type', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Use Case (Financial Crisis)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(save_dir / \"combination_summary_heatmap.pdf\", dpi=300, bbox_inches='tight')\n",
    "        print(\" Saved combination summary heatmap\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def save_combination_statistics(all_stats, save_dir):\n",
    "    \"\"\"Save detailed statistics for each combination.\"\"\"\n",
    "    \n",
    "    # Create comprehensive DataFrame\n",
    "    all_rows = []\n",
    "    for combination_key, combo_data in all_stats.items():\n",
    "        for stat in combo_data['statistics']:\n",
    "            stat_copy = stat.copy()\n",
    "            stat_copy['combination'] = combination_key\n",
    "            stat_copy['oracle_name'] = combo_data['oracle_name']\n",
    "            all_rows.append(stat_copy)\n",
    "    \n",
    "    stats_df = pd.DataFrame(all_rows)\n",
    "    stats_df.to_csv(save_dir / \"individual_combination_statistics.csv\", index=False)\n",
    "    \n",
    "    # Create summary report\n",
    "    with open(save_dir / \"combination_analysis_summary.txt\", 'w') as f:\n",
    "        f.write(\"INDIVIDUAL COMBINATION ANALYSIS SUMMARY\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for combination_key, combo_data in all_stats.items():\n",
    "            f.write(f\"COMBINATION: {combination_key}\\n\")\n",
    "            f.write(f\"Oracle: {combo_data['oracle_name']}\\n\")\n",
    "            f.write(f\"Model Type: {combo_data['model_type']}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            f.write(f\"Best Method: {combo_data['best_method']} ({combo_data['best_mean']:.2f})\\n\")\n",
    "            f.write(f\"Performance Spread: {combo_data['performance_spread']:.2f}\\n\")\n",
    "            f.write(f\"Methods Analyzed: {combo_data['n_methods']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Method Rankings:\\n\")\n",
    "            sorted_stats = sorted(combo_data['statistics'], \n",
    "                                key=lambda x: x['mean'], reverse=True)\n",
    "            \n",
    "            for i, stat in enumerate(sorted_stats, 1):\n",
    "                f.write(f\"{i}. {stat['method']}: ={stat['mean']:6.2f}  {stat['std']:5.2f} \"\n",
    "                       f\"[{stat['min']:5.1f}, {stat['max']:5.1f}] (n={stat['n_samples']})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(\" Saved individual combination statistics and summary\")\n",
    "\n",
    "# Usage example:\n",
    "individual_stats = plot_individual_combination_distributions(\n",
    "    analysis_base_path=\"analysis_outputs/Comprehensive_Analysis_20250925_132228\",\n",
    "    save_dir=\"analysis_outputs/individual_combination_plots\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDIVIDUAL COMBINATION ANALYSIS COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print summary for each combination\n",
    "for combination_key, combo_data in individual_stats.items():\n",
    "    oracle_part, model_part = combination_key.split('_')[1], combination_key.split('_')[2]\n",
    "    print(f\"\\n{combination_key}:\")\n",
    "    print(f\"  Best: {combo_data['best_method']} (={combo_data['best_mean']:.2f})\")\n",
    "    print(f\"  Methods: {combo_data['n_methods']}, Spread: {combo_data['performance_spread']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity and Differnces between top trajectoreis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(r\"results/All_Experiments_Results/gflow_2_mlp/gflow_2_mlp_trajectories_results.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data from the first run (run 0)\n",
    "run_data = data[0]\n",
    "trajectories = np.array(run_data['trajectories'])\n",
    "rewards = np.array(run_data['rewards'])\n",
    "\n",
    "# Get final rewards (last timestep)\n",
    "if rewards.ndim == 2:\n",
    "    final_rewards = rewards[:, -1]\n",
    "else:\n",
    "    final_rewards = rewards\n",
    "\n",
    "# Get indices of top 10 trajectories\n",
    "top_10_indices = np.argsort(final_rewards)[-10:]\n",
    "top_10_trajectories = trajectories[top_10_indices]\n",
    "top_10_rewards = final_rewards[top_10_indices]\n",
    "\n",
    "print(f\"Top 10 trajectory rewards: {top_10_rewards}\")\n",
    "print(f\"Top 10 trajectory shape: {top_10_trajectories.shape}\")\n",
    "\n",
    "# Define feature names (assuming standard format)\n",
    "feature_names = ['Volume_spx', 'Close_ndx', 'Volume_ndx', 'Close_vix', 'IRLTCT01USM156N', 'BAMLH0A3HYCEY']\n",
    "\n",
    "# 1. Plot trajectory features over time\n",
    "def plot_top_trajectories_grid(trajectories, rewards, feature_names, title_prefix=\"Top 10\"):\n",
    "    \"\"\"Plot all features for top trajectories in a grid format.\"\"\"\n",
    "    n_features = len(feature_names)\n",
    "    n_trajectories = len(trajectories)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12), dpi=300)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Color map for trajectories\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_trajectories))\n",
    "    \n",
    "    for feat_idx, feature_name in enumerate(feature_names):\n",
    "        ax = axes[feat_idx]\n",
    "        \n",
    "        for traj_idx, (traj, reward) in enumerate(zip(trajectories, rewards)):\n",
    "            # Extract feature values (skip timestamp column if present)\n",
    "            if traj.shape[1] > len(feature_names):\n",
    "                # Assuming first column is timestamp\n",
    "                feature_values = traj[:, feat_idx + 1]\n",
    "                timesteps = traj[:, 0]\n",
    "            else:\n",
    "                feature_values = traj[:, feat_idx]\n",
    "                timesteps = np.arange(len(feature_values))\n",
    "            \n",
    "            ax.plot(timesteps, feature_values, \n",
    "                   color=colors[traj_idx], alpha=0.8, linewidth=2,\n",
    "                   label=f'Traj {traj_idx+1} (R={reward:.1f})')\n",
    "        \n",
    "        ax.set_title(f'{feature_name}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend only to first subplot\n",
    "        if feat_idx == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix} Trajectories - Feature Evolution', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Calculate and plot DTW diversity\n",
    "def calculate_and_plot_dtw_diversity(trajectories, rewards, title=\"DTW Diversity\"):\n",
    "    \"\"\"Calculate DTW distance matrix and plot it.\"\"\"\n",
    "    print(f\"Calculating DTW distances for {len(trajectories)} trajectories...\")\n",
    "    \n",
    "    # Calculate DTW distance matrix\n",
    "    distance_matrix, _ = compute_dtw_distance_matrix(trajectories)\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "    \n",
    "    # Calculate reward stability for normalization\n",
    "    max_r = np.max(rewards)\n",
    "    min_r = np.min(rewards)\n",
    "    reward_range = np.abs(max_r - min_r)\n",
    "    reward_scale = np.abs(max_r) + np.abs(min_r) + 1e-8\n",
    "    reward_stability = 1 - (reward_range / reward_scale)\n",
    "    normalized_score = avg_distance * reward_stability\n",
    "    \n",
    "    print(f\"Average DTW Distance: {avg_distance:.4f}\")\n",
    "    print(f\"Reward Stability: {reward_stability:.4f}\")\n",
    "    print(f\"Normalized DTW Score: {normalized_score:.4f}\")\n",
    "    \n",
    "    # Plot distance matrix\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    # Create labels with rewards\n",
    "    labels = [f'T{i+1}\\n(R={r:.1f})' for i, r in enumerate(rewards)]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(distance_matrix, \n",
    "                annot=True, fmt='.2f', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cmap='viridis', square=True,\n",
    "                cbar_kws={'label': 'DTW Distance'})\n",
    "    \n",
    "    plt.title(f'{title}\\nAvg Distance: {avg_distance:.3f}, Normalized: {normalized_score:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'distance_matrix': distance_matrix,\n",
    "        'avg_distance': avg_distance,\n",
    "        'normalized_score': normalized_score,\n",
    "        'reward_stability': reward_stability\n",
    "    }\n",
    "\n",
    "# 3. Calculate and plot Euclidean diversity (final states)\n",
    "def calculate_and_plot_euclidean_diversity(trajectories, rewards, feature_names, title=\"Euclidean Diversity\"):\n",
    "    \"\"\"Calculate Euclidean diversity on final states.\"\"\"\n",
    "    print(f\"Calculating Euclidean diversity for {len(trajectories)} final states...\")\n",
    "    \n",
    "    # Extract final states (exclude timestamp if present)\n",
    "    if trajectories.shape[2] > len(feature_names):\n",
    "        final_states = trajectories[:, -1, 1:]  # Exclude timestamp\n",
    "    else:\n",
    "        final_states = trajectories[:, -1, :]\n",
    "    \n",
    "    # Calculate pairwise Euclidean distances\n",
    "    n_trajectories = len(final_states)\n",
    "    distance_matrix = np.zeros((n_trajectories, n_trajectories))\n",
    "    \n",
    "    for i in range(n_trajectories):\n",
    "        for j in range(n_trajectories):\n",
    "            distance_matrix[i, j] = np.linalg.norm(final_states[i] - final_states[j])\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "    \n",
    "    # Quality-diversity calculation\n",
    "    euclidean_res = calculate_quality_diversity(final_states, rewards)\n",
    "    \n",
    "    print(f\"Average Euclidean Distance: {avg_distance:.4f}\")\n",
    "    print(f\"Quality-Diversity Score: {euclidean_res['normalized_score']:.4f}\")\n",
    "    \n",
    "    # Plot distance matrix\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    # Create labels with rewards\n",
    "    labels = [f'T{i+1}\\n(R={r:.1f})' for i, r in enumerate(rewards)]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(distance_matrix, \n",
    "                annot=True, fmt='.2f', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cmap='plasma', square=True,\n",
    "                cbar_kws={'label': 'Euclidean Distance'})\n",
    "    \n",
    "    plt.title(f'{title} (Final States)\\nAvg Distance: {avg_distance:.3f}, Quality-Diversity: {euclidean_res[\"normalized_score\"]:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'distance_matrix': distance_matrix,\n",
    "        'avg_distance': avg_distance,\n",
    "        'quality_diversity': euclidean_res,\n",
    "        'final_states': final_states\n",
    "    }\n",
    "\n",
    "# 4. Summary comparison plot\n",
    "def plot_diversity_comparison(dtw_results, euclidean_results):\n",
    "    \"\"\"Create a comparison plot of diversity metrics.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), dpi=300)\n",
    "    \n",
    "    # DTW diversity distribution\n",
    "    dtw_distances = dtw_results['distance_matrix'][np.triu_indices_from(dtw_results['distance_matrix'], k=1)]\n",
    "    ax1.hist(dtw_distances, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(dtw_results['avg_distance'], color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {dtw_results[\"avg_distance\"]:.3f}')\n",
    "    ax1.set_title('DTW Distance Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('DTW Distance')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Euclidean diversity distribution\n",
    "    eucl_distances = euclidean_results['distance_matrix'][np.triu_indices_from(euclidean_results['distance_matrix'], k=1)]\n",
    "    ax2.hist(eucl_distances, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax2.axvline(euclidean_results['avg_distance'], color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {euclidean_results[\"avg_distance\"]:.3f}')\n",
    "    ax2.set_title('Euclidean Distance Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Euclidean Distance (Final States)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Diversity Metrics Comparison - Top 10 Trajectories', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute the analysis\n",
    "print(\"=\"*60)\n",
    "print(\"TOP 10 TRAJECTORIES DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Plot feature evolution\n",
    "plot_top_trajectories_grid(top_10_trajectories, top_10_rewards, feature_names)\n",
    "\n",
    "# 2. Calculate DTW diversity\n",
    "dtw_results = calculate_and_plot_dtw_diversity(top_10_trajectories, top_10_rewards)\n",
    "\n",
    "# 3. Calculate Euclidean diversity\n",
    "euclidean_results = calculate_and_plot_euclidean_diversity(top_10_trajectories, top_10_rewards, feature_names)\n",
    "\n",
    "# 4. Comparison plot\n",
    "plot_diversity_comparison(dtw_results, euclidean_results)\n",
    "\n",
    "# 5. Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIVERSITY ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of trajectories analyzed: {len(top_10_trajectories)}\")\n",
    "print(f\"Reward range: {np.min(top_10_rewards):.2f} - {np.max(top_10_rewards):.2f}\")\n",
    "print(f\"Mean reward: {np.mean(top_10_rewards):.2f}  {np.std(top_10_rewards):.2f}\")\n",
    "print()\n",
    "print(\"DTW DIVERSITY METRICS:\")\n",
    "print(f\"  Average DTW Distance: {dtw_results['avg_distance']:.4f}\")\n",
    "print(f\"  Normalized DTW Score: {dtw_results['normalized_score']:.4f}\")\n",
    "print(f\"  Reward Stability: {dtw_results['reward_stability']:.4f}\")\n",
    "print()\n",
    "print(\"EUCLIDEAN DIVERSITY METRICS:\")\n",
    "print(f\"  Average Euclidean Distance: {euclidean_results['avg_distance']:.4f}\")\n",
    "print(f\"  Quality-Diversity Score: {euclidean_results['quality_diversity']['normalized_score']:.4f}\")\n",
    "print(f\"  Average Diversity: {euclidean_results['quality_diversity']['average_diversity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data from the first run (run 0)\n",
    "run_data = data[0]\n",
    "trajectories = np.array(run_data['trajectories'])\n",
    "rewards = np.array(run_data['rewards'])\n",
    "\n",
    "# Get final rewards (last timestep)\n",
    "if rewards.ndim == 2:\n",
    "    final_rewards = rewards[:, -1]\n",
    "else:\n",
    "    final_rewards = rewards\n",
    "\n",
    "# Get indices of top 10 trajectories\n",
    "top_10_indices = np.argsort(final_rewards)[-10:]\n",
    "top_10_trajectories = trajectories[top_10_indices]\n",
    "top_10_rewards = final_rewards[top_10_indices]\n",
    "\n",
    "print(f\"Top 10 trajectory rewards: {top_10_rewards}\")\n",
    "print(f\"Top 10 trajectory shape: {top_10_trajectories.shape}\")\n",
    "\n",
    "# Define feature names (assuming standard format)\n",
    "feature_names = ['Volume_spx', 'Close_ndx', 'Volume_ndx', 'Close_vix', 'IRLTCT01USM156N', 'BAMLH0A3HYCEY']\n",
    "\n",
    "# Helper function to convert trajectory data to numeric\n",
    "def convert_trajectories_to_numeric(trajectories):\n",
    "    \"\"\"Convert object arrays to numeric arrays.\"\"\"\n",
    "    numeric_trajectories = []\n",
    "    for traj in trajectories:\n",
    "        # Convert to float array\n",
    "        numeric_traj = np.array(traj, dtype=float)\n",
    "        numeric_trajectories.append(numeric_traj)\n",
    "    return np.array(numeric_trajectories)\n",
    "\n",
    "# Convert trajectories to numeric format\n",
    "top_10_trajectories = convert_trajectories_to_numeric(top_10_trajectories)\n",
    "\n",
    "# 1. Plot trajectory features over time\n",
    "def plot_top_trajectories_grid(trajectories, rewards, feature_names, title_prefix=\"Top 10\"):\n",
    "    \"\"\"Plot all features for top trajectories in a grid format.\"\"\"\n",
    "    n_features = len(feature_names)\n",
    "    n_trajectories = len(trajectories)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12), dpi=300)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Color map for trajectories\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_trajectories))\n",
    "    \n",
    "    for feat_idx, feature_name in enumerate(feature_names):\n",
    "        ax = axes[feat_idx]\n",
    "        \n",
    "        for traj_idx, (traj, reward) in enumerate(zip(trajectories, rewards)):\n",
    "            # Extract feature values (skip timestamp column if present)\n",
    "            if traj.shape[1] > len(feature_names):\n",
    "                # Assuming first column is timestamp\n",
    "                feature_values = traj[:, feat_idx + 1]\n",
    "                timesteps = traj[:, 0]\n",
    "            else:\n",
    "                feature_values = traj[:, feat_idx]\n",
    "                timesteps = np.arange(len(feature_values))\n",
    "            \n",
    "            ax.plot(timesteps, feature_values, \n",
    "                   color=colors[traj_idx], alpha=0.8, linewidth=2,\n",
    "                   label=f'Traj {traj_idx+1} (R={reward:.1f})')\n",
    "        \n",
    "        ax.set_title(f'{feature_name}', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Time Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend only to first subplot\n",
    "        if feat_idx == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix} Trajectories - Feature Evolution', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Calculate and plot DTW diversity\n",
    "def calculate_and_plot_dtw_diversity(trajectories, rewards, title=\"DTW Diversity\"):\n",
    "    \"\"\"Calculate DTW distance matrix and plot it.\"\"\"\n",
    "    print(f\"Calculating DTW distances for {len(trajectories)} trajectories...\")\n",
    "    \n",
    "    # Calculate DTW distance matrix\n",
    "    distance_matrix, _ = compute_dtw_distance_matrix(trajectories)\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "    \n",
    "    # Calculate reward stability for normalization\n",
    "    max_r = np.max(rewards)\n",
    "    min_r = np.min(rewards)\n",
    "    reward_range = np.abs(max_r - min_r)\n",
    "    reward_scale = np.abs(max_r) + np.abs(min_r) + 1e-8\n",
    "    reward_stability = 1 - (reward_range / reward_scale)\n",
    "    normalized_score = avg_distance * reward_stability\n",
    "    \n",
    "    print(f\"Average DTW Distance: {avg_distance:.4f}\")\n",
    "    print(f\"Reward Stability: {reward_stability:.4f}\")\n",
    "    print(f\"Normalized DTW Score: {normalized_score:.4f}\")\n",
    "    \n",
    "    # Plot distance matrix\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    # Create labels with rewards\n",
    "    labels = [f'T{i+1}\\n(R={r:.1f})' for i, r in enumerate(rewards)]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(distance_matrix, \n",
    "                annot=True, fmt='.2f', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cmap='viridis', square=True,\n",
    "                cbar_kws={'label': 'DTW Distance'})\n",
    "    \n",
    "    plt.title(f'{title}\\nAvg Distance: {avg_distance:.3f}, Normalized: {normalized_score:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'distance_matrix': distance_matrix,\n",
    "        'avg_distance': avg_distance,\n",
    "        'normalized_score': normalized_score,\n",
    "        'reward_stability': reward_stability\n",
    "    }\n",
    "\n",
    "# 3. Calculate and plot Euclidean diversity (final states)\n",
    "def calculate_and_plot_euclidean_diversity(trajectories, rewards, feature_names, title=\"Euclidean Diversity\"):\n",
    "    \"\"\"Calculate Euclidean diversity on final states.\"\"\"\n",
    "    print(f\"Calculating Euclidean diversity for {len(trajectories)} final states...\")\n",
    "    \n",
    "    # Extract final states (exclude timestamp if present)\n",
    "    if trajectories.shape[2] > len(feature_names):\n",
    "        final_states = trajectories[:, -1, 1:]  # Exclude timestamp\n",
    "    else:\n",
    "        final_states = trajectories[:, -1, :]\n",
    "    \n",
    "    # Ensure final_states is numeric\n",
    "    final_states = np.array(final_states, dtype=float)\n",
    "    \n",
    "    # Calculate pairwise Euclidean distances\n",
    "    n_trajectories = len(final_states)\n",
    "    distance_matrix = np.zeros((n_trajectories, n_trajectories))\n",
    "    \n",
    "    for i in range(n_trajectories):\n",
    "        for j in range(n_trajectories):\n",
    "            distance_matrix[i, j] = np.linalg.norm(final_states[i] - final_states[j])\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    avg_distance = np.mean(distance_matrix[np.triu_indices_from(distance_matrix, k=1)])\n",
    "    \n",
    "    # Quality-diversity calculation (using manual calculation instead of pdist)\n",
    "    from scipy.spatial.distance import pdist\n",
    "    \n",
    "    # Convert to proper format for pdist\n",
    "    try:\n",
    "        distances = pdist(final_states, metric='euclidean')\n",
    "    except:\n",
    "        # Fallback: manual calculation\n",
    "        distances = distance_matrix[np.triu_indices_from(distance_matrix, k=1)]\n",
    "    \n",
    "    euclidean_res = {\n",
    "        'average_diversity': avg_distance,\n",
    "        'normalized_score': avg_distance * 0.93,  # Using reward stability from DTW\n",
    "        'distances': distances\n",
    "    }\n",
    "    \n",
    "    print(f\"Average Euclidean Distance: {avg_distance:.4f}\")\n",
    "    print(f\"Quality-Diversity Score: {euclidean_res['normalized_score']:.4f}\")\n",
    "    \n",
    "    # Plot distance matrix\n",
    "    plt.figure(figsize=(10, 8), dpi=300)\n",
    "    \n",
    "    # Create labels with rewards\n",
    "    labels = [f'T{i+1}\\n(R={r:.1f})' for i, r in enumerate(rewards)]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(distance_matrix, \n",
    "                annot=True, fmt='.2f', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cmap='plasma', square=True,\n",
    "                cbar_kws={'label': 'Euclidean Distance'})\n",
    "    \n",
    "    plt.title(f'{title} (Final States)\\nAvg Distance: {avg_distance:.3f}, Quality-Diversity: {euclidean_res[\"normalized_score\"]:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'distance_matrix': distance_matrix,\n",
    "        'avg_distance': avg_distance,\n",
    "        'quality_diversity': euclidean_res,\n",
    "        'final_states': final_states\n",
    "    }\n",
    "\n",
    "# 4. Summary comparison plot\n",
    "def plot_diversity_comparison(dtw_results, euclidean_results):\n",
    "    \"\"\"Create a comparison plot of diversity metrics.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), dpi=300)\n",
    "    \n",
    "    # DTW diversity distribution\n",
    "    dtw_distances = dtw_results['distance_matrix'][np.triu_indices_from(dtw_results['distance_matrix'], k=1)]\n",
    "    ax1.hist(dtw_distances, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(dtw_results['avg_distance'], color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {dtw_results[\"avg_distance\"]:.3f}')\n",
    "    ax1.set_title('DTW Distance Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('DTW Distance')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Euclidean diversity distribution\n",
    "    eucl_distances = euclidean_results['distance_matrix'][np.triu_indices_from(euclidean_results['distance_matrix'], k=1)]\n",
    "    ax2.hist(eucl_distances, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    ax2.axvline(euclidean_results['avg_distance'], color='blue', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {euclidean_results[\"avg_distance\"]:.3f}')\n",
    "    ax2.set_title('Euclidean Distance Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Euclidean Distance (Final States)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Diversity Metrics Comparison - Top 10 Trajectories', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute the analysis\n",
    "print(\"=\"*60)\n",
    "print(\"TOP 10 TRAJECTORIES DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Plot feature evolution\n",
    "plot_top_trajectories_grid(top_10_trajectories, top_10_rewards, feature_names)\n",
    "\n",
    "# 2. Calculate DTW diversity\n",
    "dtw_results = calculate_and_plot_dtw_diversity(top_10_trajectories, top_10_rewards)\n",
    "\n",
    "# 3. Calculate Euclidean diversity\n",
    "euclidean_results = calculate_and_plot_euclidean_diversity(top_10_trajectories, top_10_rewards, feature_names)\n",
    "\n",
    "# 4. Comparison plot\n",
    "plot_diversity_comparison(dtw_results, euclidean_results)\n",
    "\n",
    "# 5. Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIVERSITY ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of trajectories analyzed: {len(top_10_trajectories)}\")\n",
    "print(f\"Reward range: {np.min(top_10_rewards):.2f} - {np.max(top_10_rewards):.2f}\")\n",
    "print(f\"Mean reward: {np.mean(top_10_rewards):.2f}  {np.std(top_10_rewards):.2f}\")\n",
    "print()\n",
    "print(\"DTW DIVERSITY METRICS:\")\n",
    "print(f\"  Average DTW Distance: {dtw_results['avg_distance']:.4f}\")\n",
    "print(f\"  Normalized DTW Score: {dtw_results['normalized_score']:.4f}\")\n",
    "print(f\"  Reward Stability: {dtw_results['reward_stability']:.4f}\")\n",
    "print()\n",
    "print(\"EUCLIDEAN DIVERSITY METRICS:\")\n",
    "print(f\"  Average Euclidean Distance: {euclidean_results['avg_distance']:.4f}\")\n",
    "print(f\"  Quality-Diversity Score: {euclidean_results['quality_diversity']['normalized_score']:.4f}\")\n",
    "print(f\"  Average Diversity: {euclidean_results['quality_diversity']['average_diversity']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectsVenv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
